---
title: An introduction to PCA
subtitle: Using principal component analysis to reveal latent structure in data
abstract: PCA is often used to reduce the dimensionality of large data while preserving a significant amount of variance. More fundamentally, it is a framework for studying the covariance statistics of data. In this section, we will introduce the concept of PCA with some toy examples.
---

::: {.content-visible when-format="html"}

::: {.callout-tip}
# Run this notebook interactively!

Here's a [link to this notebook](https://colab.research.google.com/github/BonnerLab/ccn-tutorial/blob/main/notebooks/introducing_pca.ipynb) on Google Colab.
:::

:::

## A simple experiment

Let's perform an imaginary neuroscience experiment! We'll record voltages from $P = 2$ neurons in visual cortex while the participant passively views $N = 1000$ dots of different *colors* and *sizes*.

{{< include _code/install_package.qmd >}}

```{python}
# | label: import_libraries
# | code-summary: Import various libraries

from collections.abc import Sequence, Callable
import warnings
from typing import NamedTuple

import numpy as np
import pandas as pd
import xarray as xr
import seaborn as sns
import matplotlib as mpl
from matplotlib import pyplot as plt
from matplotlib.animation import FuncAnimation
from matplotlib_inline.backend_inline import set_matplotlib_formats
import ipywidgets as widgets
from IPython.display import display, HTML

```

{{< include _code/set_visualization_defaults.qmd >}}

{{< include _code/initialize_rng.qmd >}}

### Creating the stimuli

Let's create $N = 1000$ dots of different *colors* and *sizes*. From the scatterplot, we can see that the two latent variables are [uncorrelated](reference.qmd#correlation)).

```{python}
# | label: create_the_stimuli
# | code-summary: Create stimulus dots of various colors and sizes
# | fig-align: center


def create_stimuli(
    *,
    n_stimuli: int,
    rng: np.random.Generator,
) -> pd.DataFrame:
    return pd.DataFrame(
        {
            "color": rng.random(size=(n_stimuli,)),
            "size": rng.random(size=(n_stimuli,)),
        }
    ).set_index(1 + np.arange(n_stimuli))


def view_stimuli(data: pd.DataFrame) -> mpl.figure.Figure:
    fig, ax = plt.subplots()
    sns.scatterplot(
        ax=ax,
        data=data,
        x="color",
        y="size",
        hue="color",
        size="size",
        palette="flare",
        legend=False,
    )
    sns.despine(ax=ax, trim=True)
    fig.tight_layout()
    plt.close(fig)

    return fig


stimuli = create_stimuli(n_stimuli=1_000, rng=rng)

view_stimuli(stimuli)
```

### Simulating neural responses

Now, let's simulate some neural data. We need to decide how the $P = 2$ neurons might respond to these $N = 1000$ stimulus dots. Each neuron could respond to either one or both of the latent features that define these stimuli -- $\text{color}$ and $\text{size}$. The neuron's responses could also be subject to noise. Hence, we model each neuron’s response $r_\text{neuron}$ as a simple linear combination of the two latent features with stimulus-independent Gaussian noise $\epsilon$:

$r_{\text{neuron}} \sim \beta_{\text{color}} \left( \text{color} \right) + \beta_{\text{size}} \left( \text{size} \right) + \epsilon$, where $\epsilon \sim \mathcal{N}(\mu_{\text{neuron}}, \sigma_{\text{neuron}}^2)$

```{python}
# | label: utility_Neuron
# | code-filename: Define the parameters controlling neuron responses
# | code-fold: show
# | add-from: ../src/utilities/toy_example/neurons.py
# | source-lang: python
# | start-line: 8
# | end-line: 14
#
```

```{python}
# | label: utility_simulate_neuron_responses
# | code-filename: (Function) Simulate neuron responses
# | code-fold: show
# | add-from: ../src/utilities/toy_example/neurons.py
# | source-lang: python
# | start-line: 17
# | end-line: 31
#
```

```{python}
# | label: function_simulate_multiple_neuron_responses
# | code-filename: (Function) Simulate multiple neurons' responses
# | add-from: ../src/utilities/toy_example/neurons.py
# | source-lang: python
# | start-line: 34
# | end-line: 68
#
```

This procedure produces a data matrix $X \in \mathbb{R}^{N \times P}$ containing the $P = 2$ neurons' responses to the $N = 1000$ stimuli.

```{python}
# | label: import_utility_functions
# | include: false

from utilities.toy_example.neurons import Neuron, simulate_multiple_neuron_responses
```

```{python}
# | label: simulate_the_neuron_responses
# | code-summary: Simulate the responses of two neurons to the stimuli
# | code-fold: show

neurons = (
    Neuron(beta_color=3, beta_size=-2, std=1, mean=7),
    Neuron(beta_color=-2, beta_size=5, std=3, mean=-6),
)

data = simulate_multiple_neuron_responses(
    stimuli=stimuli,
    neurons=neurons,
    rng=rng,
)

display(data)
```

### Visualizing the neurons

We can visualize the responses of each neuron to each dot. Note that this is a 1-dimensional scatterplot; the spread along the vertical axis is just for visualization purposes.

```{python}
# | label: visualize_neuron_responses
# | code-summary: Visualize the individual neuron responses
# | fig-align: center


def view_individual_scatter(
    data: xr.DataArray,
    *,
    coord: str,
    dim: str,
    template_func: Callable[[int], str]
) -> mpl.figure.Figure:
    rng = np.random.default_rng()
    data_ = data.assign_coords(
        {"arbitrary": ("stimulus", rng.random(data.sizes["stimulus"]))}
    )
    min_, max_ = data_.min(), data_.max()

    n_features = data.sizes[dim]

    fig, axes = plt.subplots(nrows=n_features, figsize=(7, 2 * n_features))

    for index, ax in zip(data[coord].values, axes.flat):
        label = template_func(index)
        sns.scatterplot(
            ax=ax,
            data=(
                data_
                .isel({dim: data[coord].values == index})
                .rename(label)
                .to_dataframe()
            ),
            x=label,
            y="arbitrary",
            hue="color",
            size="size",
            palette="flare",
            legend=False,
        )
        sns.despine(ax=ax, left=True, offset=10)

        ax.set_xlim([min_, max_])
        ax.get_yaxis().set_visible(False)

    fig.tight_layout(h_pad=3)
    plt.close(fig)

    return fig


view_individual_scatter(
    data,
    coord="neuron",
    dim="neuron",
    template_func=lambda x: f"neuron {x} response",
)

```

We can see that each neuron is tuned to both color *and* size. Additionally, note that the neurons' responses have different variances.

```{python}
# | label: variance
# | code-summary: Compute the variance of each neuron's responses

variances = data.var(dim="stimulus", ddof=1).round(3).rename("neuron variances")
for i_neuron in range(variances.sizes["neuron"]):
    print(f"variance of neuron {i_neuron + 1} responses: {variances[i_neuron].values}")
print(f"total variance: {variances.sum().values}")
```

## Understanding the neural code

### Linear encoding models

### Representational similarity analysis

## Studying the latent dimensions

We have a population of neurons that contain information about the stimulus: that is, from the activity pattern we recorded, we expect to be able to reliably decode the color and size of the dot presented. How is this information encoded in the population activity? Is there a neuron that is sensitive to color and another that is sensitive to size? Or are there neural responses more complicated than this? If so, is there another view of the population code that might be more informative?

### Some geometric intuition

Since we only have $P = 2$ neurons, we can visualize these data as a scatterplot, which makes their [covariance](reference.qmd#covariance) apparent.

```{python}
# | label: joint-response
# | code-summary: Visualize both neurons' responses jointly
# | fig-align: center


def view_joint_scatter(
    data: xr.DataArray,
    *,
    coord: str,
    dim: str,
    template_func: Callable[[int], str]
) -> mpl.figure.Figure:
    fig, ax = plt.subplots()

    data_ = pd.DataFrame(
        {coord_: data[coord_].values for coord_ in ("color", "size")}
        | {
            template_func(index): (
                data.isel({dim: index - 1}).to_dataframe()[coord]
            )
            for index in (1, 2)
        }
    )

    sns.scatterplot(
        ax=ax,
        data=data_,
        x=template_func(1),
        y=template_func(2),
        hue="color",
        size="size",
        legend=False,
        palette="flare",
    )
    ax.axhline(0, c="gray", ls="--")
    ax.axvline(0, c="gray", ls="--")

    ax.set_aspect("equal", "box")
    sns.despine(ax=ax, offset=20)
    plt.close(fig)

    return fig


view_joint_scatter(
    data,
    coord="neuron responses",
    dim="neuron",
    template_func=lambda x: f"neuron {x} response",
)
```

The organization of these data in this 2-dimensional space suggests an obvious way to change our viewpoint.

```{python}
# | label: visualize_pca_animation
# | code-summary: Animate the PCA transformation
# | fig-align: center


def animate_pca_transformation(
    data: xr.DataArray,
    *,
    durations: dict[str, int] = {
        "center": 1_000,
        "rotate": 1_000,
        "pause": 500,
    },
    interval: int = 50,
) -> str:
    def _compute_2d_rotation_matrix(theta: float) -> np.ndarray:
        return np.array(
            [
                [np.cos(theta), -np.sin(theta)],
                [np.sin(theta), np.cos(theta)],
            ]
        )

    fig = view_joint_scatter(
        data,
        coord="neuron responses",
        dim="neuron",
        template_func=lambda x: f"neuron {x} response",
    )
    ax = fig.get_axes()[0]
    scatter = ax.get_children()[0]
    title = fig.suptitle("neuron responses")

    n_frames = {key: value // interval + 1 for key, value in durations.items()}

    x_mean, y_mean = data.mean("stimulus").values
    delta = np.array([x_mean, y_mean]) / n_frames["center"]

    _, _, v_h = np.linalg.svd(data - data.mean("stimulus"))
    v = v_h.transpose()
    theta = np.arccos(v[0, 0])
    rotation = _compute_2d_rotation_matrix(-theta / n_frames["rotate"])

    transformed = (data - data.mean("stimulus")).values @ v

    radius = max(np.linalg.norm(transformed, axis=-1))
    limit = max(np.abs(data).max(), np.abs(transformed).max(), radius)
    ax.set_xlim([-limit, limit])
    ax.set_ylim([-limit, limit])
    fig.tight_layout()

    frame_to_retitle_center = 2 * n_frames["pause"]
    frame_to_start_centering = frame_to_retitle_center + n_frames["pause"]
    frame_to_stop_centering = frame_to_start_centering + n_frames["center"]
    frame_to_retitle_rotate = frame_to_stop_centering + n_frames["pause"]
    frame_to_start_rotating = frame_to_retitle_rotate + n_frames["pause"]
    frame_to_stop_rotating = frame_to_start_rotating + n_frames["rotate"]
    frame_to_retitle_transformed = frame_to_stop_rotating + n_frames["pause"]
    frame_to_end = frame_to_retitle_transformed + 2 * n_frames["pause"]

    def _update(frame: int) -> None:
        if frame < frame_to_retitle_center:
            return
        elif frame == frame_to_retitle_center:
            title.set_text("step 1 of 2: center the data")
            ax.set_xlabel("")
            ax.set_ylabel("")
        elif frame < frame_to_start_centering:
            return
        elif frame <= frame_to_stop_centering:
            scatter.set_offsets(scatter.get_offsets() - delta)
        elif frame == frame_to_retitle_rotate:
            title.set_text("step 2 of 2: rotate the data")
        elif frame < frame_to_start_rotating:
            return
        elif frame <= frame_to_stop_rotating:
            scatter.set_offsets(scatter.get_offsets().data @ rotation)
        elif frame < frame_to_retitle_transformed:
            return
        elif frame == frame_to_retitle_transformed:
            title.set_text("principal components")
            ax.set_xlabel("principal component 1")
            ax.set_ylabel("principal component 2")
        elif frame <= frame_to_end:
            return

    animation = FuncAnimation(
        fig=fig,
        func=_update,
        frames=frame_to_end,
        interval=interval,
        repeat=True,
        repeat_delay=2_000,
    )
    plt.close(fig)
    return animation.to_html5_video()


display(HTML(animate_pca_transformation(data)))

```

### The mathematical definition

Given a data matrix $X \in \mathbb{R}^{N \times P}$, we need to compute the [eigendecomposition](reference.qmd#eigendecomposition)[^definition-eigendecomposition] of its [auto-covariance](reference.qmd#auto-covariance)[^definition-autocovariance]:

$$
\begin{align}
    \text{cov}(X)
    &= \left(\dfrac{1}{n - 1}\right) (X - \overline{X})^\top (X - \overline{X})\\
    &= V \Lambda V^\top
\end{align}
$$

![Computing the auto-covariance, where $X$ is centered (i.e. $X - \overline{X}$)](assets/autocovariance.svg){height=50mm}

![Computing the eigendecomposition of the auto-covariance](assets/eigendecomposition.svg){height=50mm}

The columns of $V$ are *eigenvectors* that specify the directions of variance while the corresponding diagonal elements of $\Lambda$ are *eigenvalues* that specify the amount of variance along the eigenvector[^proof-eigenvalue-covariance].

The original data matrix can be transformed by projecting it onto the eigenvectors: $\widetilde{X} = \left(X - \overline{X}\right) V$.

![Transforming the original data matrix](assets/projection.svg){height=50mm}

#### Some terminology

Ambient space
: The neural representation of each stimulus $i \in {1, \dots, N}$ can be captured by a vector of activations $X_i \in \mathbb{R}^P$. This vector space is typically called the *ambient space* -- the space where the raw data lives.

Loadings
: The eigenvectors of the covariance matrix are often called *loadings*

Scores
:

::: {.callout-note collapse="true"}
# Viewing PCA as an optimization

PCA can be used to project data into a lower-dimensional space (i.e. $p \le f$) in a way that best preserves the geometry of the data. Specifically, computing a PCA decomposition of $X$ yields a matrix $V \in \mathbb{R}^{f \times p}$ such that $V = \argmin_{V \in \mathbb{U_{f \times p}}} \sum_{i=1}^n \left|| x_i - VV^\top x_i \right||_2$, where $||\cdot||_2$ denotes the $L_2$-norm and $\mathbb{U_{f \times p}}$ denotes the set of orthonormal matrices with shape $f \times p$.
:::

### A computational recipe

```{python}
# | label: recipe-pca
# | code-summary: A computational recipe for PCA
# | code-fold: show


class PCA:
    def __init__(self) -> None:
        self.mean: np.ndarray
        self.eigenvectors: np.ndarray
        self.eigenvalues: np.ndarray

    def fit(self, /, data: np.ndarray) -> None:
        self.mean = data.mean(axis=-2)

        data_centered = data - self.mean  # <1>
        _, s, v_t = np.linalg.svd(data_centered)  # <2>

        n_stimuli = data.shape[-2]

        self.eigenvectors = np.swapaxes(v_t, -1, -2)  # <3>
        self.eigenvalues = s**2 / (n_stimuli - 1)  # <4>

    def transform(self, /, data: np.ndarray) -> np.ndarray:
        return (data - self.mean) @ self.eigenvectors  # <5>
```
1. Center the data matrix.
2. Compute its singular value decomposition[^definition-singular-value-decomposition].
3. The right singular vectors $V$ of the data matrix are the eigenvectors of its auto-covariance.
4. The singular values $\Sigma$ of the data matrix are related to the eigenvalues $\Lambda$ of its auto-covariance as $\Lambda = \Sigma^2 / (N - 1)$
5. To project data from the ambient space to the latent space, we must subtract the mean computed in Step 1, and multiply the data by the eigenvectors.

::: {.callout-tip collapse="true"}
### Why do we compute PCA this way instead of $\text{eig}(\text{cov}(X))$?

To apply PCA to a data matrix, we might be tempted to use the definition and naively compute its [auto-covariance](reference.qmd#auto-covariance) followed by an [eigendecomposition](reference.qmd#eigendecomposition). However, when the number of neurons $P$ is large, this approach is memory-intensive and prone to numerical errors.

Instead, we can use the [singular value decomposition](reference.qmd#singular-value-decomposition) (SVD) of $X$ to efficiently compute its PCA transformation. Specifically, $X = U \Sigma V^\top$ is a singular value decomposition, where $U$ and $V$ are orthonormal and $\Sigma$ is diagonal.

The auto-covariance matrix reduces to $X^\top X / (n - 1) = V \left(\frac{\Sigma^2}{n - 1} \right) V^\top$, which is exactly the eigendecomposition required.

Specifically, the eigenvalues $\lambda_i$ of the auto-covariance matrix are related to the singular values $\sigma_i$ of the data matrix as $\lambda_i = \sigma_i^2 / (N - 1)$, while the eigenvectors of the auto-covariance matrix are exactly the right singular vectors $V$ of the data matrix $X$.
:::

::: {.callout-tip collapse="true"}
# Only need the first few PCs?

Check out [truncated SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)!
:::

### Inspecting the eigenvectors

```{python}
# | label: pca-visualize
# | code-summary: Visualize the transformed data
# | fig-align: center

def compute_pca(data: xr.DataArray) -> xr.Dataset:
    pca = PCA()
    pca.fit(data.values)

    data_transformed = pca.transform(data.values)

    return xr.Dataset(
        data_vars={
            "score": xr.DataArray(
                data=data_transformed,
                dims=("stimulus", "component"),
            ),
            "eigenvector": xr.DataArray(
                data=pca.eigenvectors,
                dims=("component", "neuron"),
            ),
        },
        coords={
            "rank": ("component", 1 + np.arange(data_transformed.shape[-1])),
            "eigenvalue": ("component", pca.eigenvalues),
        }
        | {coord: (data[coord].dims[0], data[coord].values) for coord in data.coords},
    )


pca = compute_pca(data)
display(pca["score"])
view_joint_scatter(
    pca["score"],
    coord="score",
    dim="component",
    template_func=lambda x: f"principal component {x}",
)
```

### Interpreting the transformed data

Let's view the data projected onto each of the principal components.

```{python}
# | label: view_principal_components
# | code-summary: Visualize principal component scores
# | fig-align: center

view_individual_scatter(
    pca["score"],
    coord="rank",
    dim="component",
    template_func=lambda x: f"principal component {x}",
)
```

We can observe that:

- the *first* principal component is largely driven by the *size* of the stimulus
- the *second* principal component is largely driven by the *color* of the stimulus

::: {.callout-important}
However, note that these components do not directly correspond to either of the latent variables; rather, each is a mixture of stimulus-dependent signal and noise.
:::

### Analyzing the covariance statistics

We can compute the variance along each eigenvector. The total variance along all eigenvectors is the same as the total variance of the original data.

```{python}
# | label: eigenvalues
# | code-summary: Display eigenvalues

eigenvalues = pca["eigenvalue"].round(3)
for i_neuron in range(eigenvalues.sizes["component"]):
    print(
        f"variance along eigenvector {i_neuron + 1} (eigenvalue {i_neuron + 1}):"
        f" {eigenvalues[i_neuron].values}"
    )
print(f"total variance: {eigenvalues.sum().values}")
```

```{python}
# | label: visualize_eigenspectrum
# | code-summary: View eigenspectrum
# | fig-align: center

def view_eigenspectrum(pca: xr.DataArray) -> mpl.figure.Figure:
    fig, ax = plt.subplots(figsize=(pca.sizes["component"], 5))
    sns.lineplot(
        ax=ax,
        data=pca["component"].to_dataframe(),
        x="rank",
        y="eigenvalue",
        marker="s",
    )
    ax.set_xticks(pca["rank"].values)
    ax.set_ylim(bottom=0)
    sns.despine(ax=ax, offset=20)
    plt.close(fig)

    return fig


view_eigenspectrum(pca)
```

## More neurons!

Let’s record from more neurons (say $P = 10$)! Here, the *ambient dimensionality* of the feature space is $P = 10$.

```{python}
# | label: simulate_more_neurons
# | code-summary: Simulate responses from more neurons
# | fig-align: center

def _simulate_random_neuron(rng: np.random.Generator) -> Neuron:
    return Neuron(
        beta_color=rng.integers(-10, 11),
        beta_size=rng.integers(-10, 11),
        std=rng.integers(-10, 11),
        mean=rng.integers(-10, 11),
    )

neurons = tuple([_simulate_random_neuron(rng) for _ in range(10)])

big_data = simulate_multiple_neuron_responses(
    stimuli=stimuli,
    neurons=neurons,
    rng=rng,
)

display(big_data)
```

```{python}
# | label: view_principal_components_big
# | code-summary: Visualize principal component scores
# | fig-align: center

big_pca = compute_pca(big_data)

view_individual_scatter(
    big_pca["score"],
    coord="rank",
    dim="component",
    template_func=lambda x: f"principal component {x}",
)
```

```{python}
# | label: visualize_eigenspectrum_big
# | code-summary: View eigenspectrum
# | fig-align: center

view_eigenspectrum(big_pca)
```

However, we know that this data was generated from exactly *2* latent variables -- *color* and *size*. We refer to these as the *effective dimensions*. In this example, the remaining dimensions, which correspond to noise, have relatively low variance, and we can see a signficant drop in the eigenspectrum after the second eigenvector.

### Quantifying dimensionality

Based on the spectrum, several approaches are used to quantify the latent dimensionality of a dataset:

#### Rank of the covariance matrix

The *rank* of the covariance matrix -- equal to the number of *nonzero* eigenvalues -- would be the latent dimensionality in the ideal setting where the data has zero noise. In real data, the rank is typically equal to the ambient dimensionality, since there is typically some variance along every dimension.

#### Setting an arbitrary variance threshold

Though not typically used today, another approach is to set an arbitrary threshold on the variance (historically recommended as $1$ for normalized data); only dimensions with variance above that threshold are considered useful.

#### Setting an arbitrary *cumulative* variance threshold

A very commonly used method is to set a threshold based on the cumulative variance of the data: the number of dimensions required to exceed, say $80\%$ of the variance, is taken as the latent dimensionality.

#### Eyeballing the "knee" of the spectrum

When the number of latent dimensions is low, eigenspectra often have a sharp discontinuity (the "knee"), where a small number of dimensions have high-variance and the remainder have much have lower variance. The latent dimensionality is then taken to be the number of dimensions above the threshold, determined by eye.

#### Computing a summary statistic over the entire spectrum

A metric such as *effective dimensionality* summarizes the spectrum using an entropy-like measure, taking into account variances along all the dimensions:

$$\text{effective dimensionality}(\lambda_1, \dots \lambda_n) = \dfrac{\left( \sum_{i=1}^n \lambda_i \right)^2}{\sum_{i=1}^n \lambda_i^2}$$

However, keep in mind that this is a toy example with idealized data. As we will see, when using standard PCA on real data it may be impossible to identify a clear distinction between meaningful dimensions and noise.

## Preprocessing your data

Before PCA, it's often recommended to preprocess your data by Z-scoring each of your input features $X$ -- ensuring that they have zero mean and unit variance:

$$Z = \dfrac{X - \mu}{\sigma}$$

::: {.callout-important collapse="false"}
# When should you standardize your data?

Often, PCA is applied to data where the features are fundamentally different from each other. For example, we might have a dataset where the features of interest are the prices of cars (in dollars) and their masses (in kilograms). Since these two features have different units, the variances of the features are not directly comparable -- there's no obvious way to numerically compare a variance of ($20,000)^2^ in price and a variance of (1,000 kg)^2^ in mass. Even if the features being compared are all the same, if they are in different units -- say euros, dollars, and cents -- the raw variances of the data matrix are meaningless.

Since PCA implicitly assumes that the variances along each dimension are comparable, we can Z-score each of our features before applying PCA to ensure that they are on a common scale.

Note, however, that this transformation reduces the information in the system -- it is possible that the variances of the features are informative.
:::

## What if the stimulus features covary?

[^definition-eigendecomposition]: The eigendecomposition of a symmetric matrix $X \in \mathbb{R}^{n \times n}$ involves rewriting it as the product of three matrices $X = V \Lambda V^\top$, where $V \in \mathbb{n \times n}$ is orthonormal and $\Lambda \in \mathbb{n \times n}$ is diagonal with non-negative entries.

[^definition-autocovariance]: Given a data matrix $X \in \mathbb{R}^{n \times f}$ containing neural responses to $n$ stimuli from $f$ neurons, the *auto-covariance* of $X$ (or simply its *covariance*) is defined as:

    $\text{cov}(X) = \left(\dfrac{1}{n - 1}\right) (X - \overline{X})^\top (X - \overline{X})$

    This is an $f \times f$ matrix where the $(i, j)$-th element measures how much neuron $i$ covaries with neuron $j$. If the covariance is positive, they tend to have similar activation: a stimulus that activates one neuron will tend to activate the other. If the covariance is negative, the neurons will have dissimilar activation: a stimulus that activates one neuron will likely not activate the other.

[^proof-eigenvalue-covariance]: Let's compute the auto-covariance of the projected data $\widetilde{X}$:

    $$
    \begin{align}
        \text{cov}(\widetilde{X})
        &= \left(\dfrac{1}{n - 1}\right) \widetilde{X}^\top \widetilde{X}\\
        &= \left(\dfrac{1}{n - 1}\right) \left((X - \overline{X})V\right)^\top \left((X - \overline{X})V\right)\\
        &= \left(\dfrac{1}{n - 1}\right) V^\top (X - \overline{X})^\top (X - \overline{X})V\\
        &= V^\top \left(\dfrac{1}{n - 1}\right) (X - \overline{X})^\top (X - \overline{X})V\\
        &= V^\top \left( V \Lambda V^\top \right) V\\
        &= I \Lambda I\\
        &= \Lambda
    \end{align}
    $$

[^definition-singular-value-decomposition]: The singular value decomposition (SVD) involves rewriting a matrix $X \in \mathbb{R}^{m \times n}$ as the product of three matrices $X = U \Sigma V^\top$, where $U \in \mathbb{R}^{m \times m}$ and $V \in \mathbb{R}^{n \times n}$ are orthonormal matrices and $\Sigma \in \mathbb{R}^{m \times n}$ is zero everywhere except potentially on its leading diagonal