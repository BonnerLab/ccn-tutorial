---
title: Dealing with noisy data
subtitle: Cross-validated methods to separate high-dimensional signal from noise
abstract: TODO
---

::: {.content-visible when-format="html"}

::: {.callout-tip}
# Run this notebook interactively!

Here's a [link to this notebook](https://colab.research.google.com/github/BonnerLab/ccn-tutorial/blob/main/notebooks/dealing_with_noise.ipynb) on Google Colab.
:::

:::

{{< include _code/install_package.qmd >}}

```{python}
# | label: imports
# | code-summary: Import various libraries

from collections.abc import Sequence
import functools
import warnings
from typing import NamedTuple

import numpy as np
import pandas as pd
import xarray as xr
import seaborn as sns
import matplotlib as mpl
from matplotlib import pyplot as plt
from matplotlib_inline.backend_inline import set_matplotlib_formats
import ipywidgets as widgets
from IPython.display import display

from utilities.brain import load_dataset

```

{{< include _code/set_visualization_defaults.qmd >}}

{{< include _code/initialize_rng.qmd >}}

## Neural data is noisy

Experimental data is noisy -- often fundamentally. When we measure neural activity in response to a stimulus, our recordings comprise several sources of variation:

- intrinsic stochasticity in the system -- e.g. the stochastic spiking of neurons
- measurement noise
  - e.g.scanner drift in fMRI
  - e.g. poor electrode scalp contact in EEG
- cross-trial variation -- i.e. recording responses to the same stimuli from the same animal on multiple days
  - e.g. when your participant has coffee in the morning before a long scanning session
  - e.g. in brain-computer intefaces, where representational shifts require the system to be recalibrated frequently
- cross-individual differences -- i.e. recording from different participants in an experiment
- stimulus-dependent signal
  - e.g. face-selective neurons in the fusiform face area responding more to faces than houses

Of these sources of variation, when investigating a sensory system, we are typically interested in the last one -- how the neural response varies with the stimulus.

### Truncating spectra might ignore low-variance signal!

In the last section, we observed that neural responses have high-dimensional structure, as evidenced by the covariance spectrum obtained from principal component analysis. How much of this variance is driven by stimulus-dependent signal -- and how much is noise? The typical assumption when applying principal component analysis as a dimensionality reduction tool is that high-variance dimensions correspond to *signal* in the system, while low-variance dimensions represent *noise*. However, there is no fundamental reason this has to be the case!

For example, consider a *very realistic* scenario where a participant gets bored performing our experiment inside an fMRI scanner and starts yodeling to entertain themselves. The highest variance components in the measured neural response would likely be motion artifacts and not whatever signal we were interested in measuring. A high-quality data preprocessing pipeline would help mitigate such extreme components of variance -- but cannot remove them entirely!

In fact, as we saw when we inspected the first couple of latent dimensions in our toy example, the principal components didn't correspond directly to the latent variables generating the data; rather, they were a mixture of stimulus-dependent variance and nuisance variance.

This suggests that using principal component analysis as a dimensionality reduction tool by setting an arbitrary variance threshold is likely too stringent a criterion: there is possible low-variance signal along the many neglected dimensions in the tail!

Clearly, we need a different approach to using the spectrum to separate signal and noise...

::: {.callout-tip collapse="true"}
# The covariance structure of random matrices

Even random matrices have some covariance structure that result in non-zero eigenvalues. This structure has been mathematically characterized by the [Marchenko-Pastur distribution](https://en.wikipedia.org/wiki/Marchenko-Pastur_distribution). This makes it especially difficult to infer anything about the reliability or significance of the dimension from the magnitude of the eigenvalue -- especially when the eigenvalues are close to zero.

```{python}


def simulate_marchenko_pastur(
    n_stimuli: int = 500,
    n_neurons: int = 500,
    n_repetitions: int = 50,
) -> np.ndarray:
    data = rng.standard_normal((n_repetitions, n_stimuli, n_neurons))
    data -= data.mean(axis=-2, keepdims=True)

    singular_values = np.linalg.svd(data, compute_uv=False)
    return (
        xr.DataArray(
            name="eigenvalue",
            data=singular_values**2 / (n_stimuli - 1),
            dims=("repetition", "rank"),
            coords={"rank": ("rank", 1 + np.arange(singular_values.shape[-1]))},
        )
        .to_dataframe()
        .reset_index()
    )


def view_marchenko_pastur(eigenvalues: xr.DataArray, *, log: bool = False) -> None:
    fig, ax = plt.subplots()
    sns.lineplot(
        ax=ax,
        data=eigenvalues,
        x="rank",
        y="eigenvalue",
        estimator="mean",
        errorbar="sd",
        err_style="band",
    )
    if log:
        ax.set_xscale("log")
        ax.set_yscale("log")
        ax.set_ylim(bottom=1e-2)

    sns.despine(ax=ax, offset=20)
    fig.show()


eigenvalues = simulate_marchenko_pastur()
view_marchenko_pastur(eigenvalues)
```
:::

### Cross-validated PCA

In a whole-brain calcium-imaging study where mice viewed 2,800 natural images while neural responses were recorded, Stringer et al. (2021) developed cross-validated PCA (CV-PCA) as a method to reliably estimate the covariance structure of the neural responses. Since each image was viewed twice, these repetitions could be used as train/test splits to estimate covariance eigenvalues. Specifically, eigenvectors are computed on a training set of data, and *cross-validated* eigenvalues are computed by computing the covariance between two independent sets of data:

$X_\text{train}^\top X_\text{train} / (n - 1) = V \Lambda V^\top$

$\Lambda_\text{test} = \left( X_\text{train} V \right) ^\top \left( X_\text{test} V \right) / (n - 1)$

These cross-validated eigenvalues represent the covariance reliably shared across two presentations of the visual stimulus -- which is a quantity we are interested in as neuroscientists: what is the "stable" part of the visual representation of a natural image? Notably, these "eigenvalues" need not be positive: if there is no shared covariance between the two systems at a rank, the expected value of the eigenvalue is $0$.

Let's apply CV-PCA to our fMRI data to see what it looks like!

```{python}
# | code-summary: Load the dataset

data = load_dataset(subject=0, roi="general")

display(data)
```

Note that the data contain fMRI responses to two repetitions of each image.

```{python}


class PLSSVD:
    def __init__(self) -> None:
        self.left_mean: np.ndarray
        self.right_mean: np.ndarray
        self.left_singular_vectors: np.ndarray
        self.right_singular_vectors: np.ndarray

    def fit(self, /, x: np.ndarray, y: np.ndarray) -> None:
        self.left_mean = x.mean(axis=-2)  # <1>
        self.right_mean = y.mean(axis=-2)  # <1>

        x_centered = x - self.left_mean  # <1>
        y_centered = y - self.right_mean  # <1>

        n_stimuli = x.shape[-2]

        cross_covariance = (np.swapaxes(x_centered, -1, -2) @ y_centered) / (
            n_stimuli - 1
        )  # <2>

        (
            self.left_singular_vectors,
            self.singular_values,
            self.right_singular_vectors,
        ) = svd(
            torch.from_numpy(cross_covariance),
            n_components=min([*x.shape, *y.shape]),
            truncated=True,
            seed=random_state,
        )  # <3>

        n_stimuli = data.shape[-2]

        self.left_singular_vectors = self.left_singular_vectors.cpu().numpy()
        self.singular_values = self.singular_values.cpu().numpy()
        self.right_singular_vectors = self.right_singular_vectors.cpu().numpy()

    def transform(self, /, z: np.ndarray, *, direction: str) -> np.ndarray:
        match direction:
            case "left":
                return (z - self.left_mean) @ self.left_singular_vectors  # <4>
            case "right":
                return (z - self.right_mean) @ self.right_singular_vectors  # <4>
            case _:
                raise ValueError("direction must be 'left' or 'right'")


def bin_logarithmically(
    data: Sequence[int | float],
    *,
    base: int = 10,
    points_per_bin: int,
    min_: int | float,
    max_: int | float,
) -> Sequence[float]:
    n = points_per_bin * int(np.ceil((np.log(max_) - np.log(min_)) / (np.log(base))))
    bin_edges = np.geomspace(min_, max_, num=n)
    bin_centers = np.exp(np.log(bin_edges)[:-1] + np.diff(np.log(bin_edges)) / 2)

    bin_edges[-1] = np.inf
    labels = bin_centers[np.digitize(data, bin_edges) - 1]

    return labels


# data_repetition_1 = data.isel({"presentation": data["rep_id"] == 0})
# data_repetition_2 = data.isel({"presentation": data["rep_id"] == 1})
# data_repetition_1 = data_repetition_1.sortby("stimulus_id")
# data_repetition_2 = data_repetition_2.sortby("stimulus_id")
# assert np.array_equal(
#     data_repetition_1["stimulus_id"].values, data_repetition_2["stimulus_id"].values
# )

# plssvd = PLSSVD()
# plssvd.fit(
#     data_repetition_1.values,
#     data_repetition_1.values,
# )
# rep_1_transformed = plssvd.transform(data_repetition_1.values, direction="left")
# rep_2_transformed = plssvd.transform(data_repetition_2.values, direction="right")


# n_neurons_1 = rep_1_transformed.shape[-1]
# cv_spectrum = np.diag(
#     np.cov(rep_1_transformed, rep_2_transformed, rowvar=False)[
#         :n_neurons_1, n_neurons_1:
#     ]
# )
# cv_spectrum = pd.DataFrame(
#     cv_spectrum, columns=["cross-validated singular value"]
# ).assign(rank=np.arange(len(cv_spectrum)) + 1)
# cv_spectrum = cv_spectrum.assign(
#     bin=bin_logarithmically(
#         cv_spectrum["cross-validated singular value"],
#         points_per_bin=5,
#         min_=1,
#         max_=10_000,
#     )
# )

# fig, ax = plt.subplots()
# sns.lineplot(ax=ax, data=cv_spectrum, x="rank", y="cross-validated singular value")
# sns.despine(ax=ax, offset=20)

# fig, ax = plt.subplots()
# sns.lineplot(ax=ax, data=cv_spectrum, x="rank", y="cross-validated singular value")
# ax.set_xscale("log")
# ax.set_yscale("log")
# ax.set_ylim(bottom=1e-3)
# sns.despine(ax=ax, offset=20)

# fig, ax = plt.subplots()
# sns.lineplot(
#     ax=ax,
#     data=cv_spectrum.groupby("bin").mean().rename(columns={"bin": "rank"}),
#     x="rank",
#     y="cross-validated singular value",
#     ls=None,
#     marker="o",
# )
# ax.set_xscale("log")
# ax.set_yscale("log")
# sns.despine(ax=ax, offset=20)
```