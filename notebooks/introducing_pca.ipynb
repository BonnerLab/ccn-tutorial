{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1811f52b5df744cf3e567a2e7c0722cc3abdf5f6-0",
   "metadata": {},
   "source": [
    "# An introduction to PCA\n",
    "\n",
    "Using principal component analysis to reveal latent structure in data\n",
    "\n",
    "PCA is often used to reduce the dimensionality of large data while\n",
    "preserving a significant amount of variance. More fundamentally, it is a\n",
    "framework for studying the covariance statistics of data. In this\n",
    "section, we will introduce the concept of PCA with some toy examples.\n",
    "\n",
    "## A simple experiment\n",
    "\n",
    "Let’s perform an imaginary neuroscience experiment! We’ll record\n",
    "voltages from $P = 2$ neurons in visual cortex while the participant\n",
    "passively views $N = 1000$ dots of different *colors* and *sizes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716327115344ecefe9e785605ab1f75e382cadde-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/BonnerLab/ccn-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1047287f888aa6cc0e4d3e6a2dad5603a14f1-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Callable\n",
    "import warnings\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d7be547b34d5bf28cd6cbfa700f707e29d410-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"white\",\n",
    "    palette=\"deep\",\n",
    ")\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "\n",
    "xr.set_options(display_max_rows=3, display_expand_data=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81958efb039da8a9d6d3dcfc190f98e5a010f2c7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "rng = np.random.default_rng(seed=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838db8983f029563c4091f071f9b43a75fa32506-0",
   "metadata": {},
   "source": [
    "### Creating the stimuli\n",
    "\n",
    "Let’s create $N = 1000$ dots of different *colors* and *sizes*. From the\n",
    "scatterplot, we can see that the two latent variables are\n",
    "[uncorrelated](reference.qmd#correlation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create_the_stimuli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stimuli(\n",
    "    *,\n",
    "    n_stimuli: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"color\": rng.random(size=(n_stimuli,)),\n",
    "            \"size\": rng.random(size=(n_stimuli,)),\n",
    "        }\n",
    "    ).set_index(1 + np.arange(n_stimuli))\n",
    "\n",
    "\n",
    "def view_stimuli(data: pd.DataFrame) -> mpl.figure.Figure:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data,\n",
    "        x=\"color\",\n",
    "        y=\"size\",\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        palette=\"flare\",\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(ax=ax, trim=True)\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "stimuli = create_stimuli(n_stimuli=1_000, rng=rng)\n",
    "\n",
    "view_stimuli(stimuli)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7255d0bf746f4560529263a009737665861414c8-0",
   "metadata": {},
   "source": [
    "### Simulating neural responses\n",
    "\n",
    "Now, let’s simulate some neural data. We need to decide how the $P = 2$\n",
    "neurons might respond to these $N = 1000$ stimulus dots. Each neuron\n",
    "could respond to either one or both of the latent features that define\n",
    "these stimuli – $\\text{color}$ and $\\text{size}$. The neuron’s responses\n",
    "could also be subject to noise. Hence, we model each neuron’s response\n",
    "$r_\\text{neuron}$ as a simple linear combination of the two latent\n",
    "features with stimulus-independent Gaussian noise $\\epsilon$:\n",
    "\n",
    "$r_{\\text{neuron}} \\sim \\beta_{\\text{color}} \\left( \\text{color} \\right) + \\beta_{\\text{size}} \\left( \\text{size} \\right) + \\epsilon$,\n",
    "where\n",
    "$\\epsilon \\sim \\mathcal{N}(\\mu_{\\text{neuron}}, \\sigma_{\\text{neuron}}^2)$\n",
    "\n",
    "This procedure produces a data matrix $X \\in \\mathbb{R}^{N \\times P}$\n",
    "containing the $P = 2$ neurons’ responses to the $N = 1000$ stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-simulate_the_neuron_responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = NamedTuple(\n",
    "    \"Neuron\",\n",
    "    beta_color=float,\n",
    "    beta_size=float,\n",
    "    mean=float,\n",
    "    std=float,\n",
    ")\n",
    "\n",
    "\n",
    "def simulate_neuron_responses(\n",
    "    stimuli: pd.DataFrame,\n",
    "    neuron: Neuron,\n",
    "    *,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    def z_score(x: np.ndarray) -> np.ndarray:\n",
    "        return (x - x.mean()) / x.std()\n",
    "\n",
    "    return (\n",
    "        neuron.beta_color * z_score(stimuli[\"color\"])\n",
    "        + neuron.beta_size * z_score(stimuli[\"size\"])\n",
    "        + neuron.std * rng.standard_normal(size=(len(stimuli),))\n",
    "        + neuron.mean\n",
    "    )\n",
    "\n",
    "\n",
    "def simulate_multiple_neuron_responses(\n",
    "    *,\n",
    "    stimuli: pd.DataFrame,\n",
    "    neurons: Sequence[Neuron],\n",
    "    rng: np.random.Generator,\n",
    ") -> xr.DataArray:\n",
    "    data = []\n",
    "    for i_neuron, neuron in enumerate(neurons):\n",
    "        data.append(\n",
    "            xr.DataArray(\n",
    "                data=simulate_neuron_responses(\n",
    "                    stimuli=stimuli,\n",
    "                    neuron=neuron,\n",
    "                    rng=rng,\n",
    "                ),\n",
    "                dims=(\"stimulus\",),\n",
    "                coords={\n",
    "                    column: (\"stimulus\", values)\n",
    "                    for column, values in stimuli.reset_index(names=\"stimulus\").items()\n",
    "                },\n",
    "            )\n",
    "            .expand_dims({\"neuron\": [i_neuron + 1]})\n",
    "            .assign_coords(\n",
    "                {\n",
    "                    field: (\"neuron\", [float(value)])\n",
    "                    for field, value in neuron._asdict().items()\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    data = (\n",
    "        xr.concat(data, dim=\"neuron\")\n",
    "        .rename(\"neuron responses\")\n",
    "        .transpose(\"stimulus\", \"neuron\")\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "neurons = (\n",
    "    Neuron(beta_color=3, beta_size=-2, std=1, mean=7),\n",
    "    Neuron(beta_color=-2, beta_size=5, std=3, mean=-6),\n",
    ")\n",
    "\n",
    "data = simulate_multiple_neuron_responses(\n",
    "    stimuli=stimuli,\n",
    "    neurons=neurons,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01114c50f066cc0017b78d1130c0deaa8ec0482-0",
   "metadata": {},
   "source": [
    "### Visualizing the neurons\n",
    "\n",
    "We can visualize the responses of each neuron to each dot. Note that\n",
    "this is a 1-dimensional scatterplot; the spread along the vertical axis\n",
    "is just for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_neuron_responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_individual_scatter(\n",
    "    data: xr.DataArray, *, coord: str, dim: str, template_func: Callable[[int], str]\n",
    ") -> mpl.figure.Figure:\n",
    "    rng = np.random.default_rng()\n",
    "    data_ = data.assign_coords(\n",
    "        {\"arbitrary\": (\"stimulus\", rng.random(data.sizes[\"stimulus\"]))}\n",
    "    )\n",
    "    min_, max_ = data_.min(), data_.max()\n",
    "\n",
    "    n_features = data.sizes[dim]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_features, figsize=(7, 2 * n_features))\n",
    "\n",
    "    for index, ax in zip(data[coord].values, axes.flat):\n",
    "        label = template_func(index)\n",
    "        sns.scatterplot(\n",
    "            ax=ax,\n",
    "            data=(\n",
    "                data_.isel({dim: data[coord].values == index})\n",
    "                .rename(label)\n",
    "                .to_dataframe()\n",
    "            ),\n",
    "            x=label,\n",
    "            y=\"arbitrary\",\n",
    "            hue=\"color\",\n",
    "            size=\"size\",\n",
    "            palette=\"flare\",\n",
    "            legend=False,\n",
    "        )\n",
    "        sns.despine(ax=ax, left=True, offset=10)\n",
    "\n",
    "        ax.set_xlim([min_, max_])\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout(h_pad=3)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_individual_scatter(\n",
    "    data,\n",
    "    coord=\"neuron\",\n",
    "    dim=\"neuron\",\n",
    "    template_func=lambda x: f\"neuron {x} response\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138402cb07f42d6fe1fd4a5427748ab629008294-0",
   "metadata": {},
   "source": [
    "We can see that each neuron is tuned to both color *and* size.\n",
    "Additionally, note that the neurons’ responses have different variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481bfb05e0b0bc3fda8ace1c0567d8b37a52816-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = data.var(dim=\"stimulus\", ddof=1).round(3).rename(\"neuron variances\")\n",
    "for i_neuron in range(variances.sizes[\"neuron\"]):\n",
    "    print(f\"variance of neuron {i_neuron + 1} responses: {variances[i_neuron].values}\")\n",
    "print(f\"total variance: {variances.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d48a2f1f028c013cbc8212cbcf0f48056d05ec-0",
   "metadata": {},
   "source": [
    "## Understanding the neural code\n",
    "\n",
    "### Linear encoding models\n",
    "\n",
    "### Representational similarity analysis\n",
    "\n",
    "## Studying the latent dimensions\n",
    "\n",
    "We have a population of neurons that contain information about the\n",
    "stimulus: that is, from the activity pattern we recorded, we expect to\n",
    "be able to reliably decode the color and size of the dot presented. How\n",
    "is this information encoded in the population activity? Is there a\n",
    "neuron that is sensitive to color and another that is sensitive to size?\n",
    "Or are there neural responses more complicated than this? If so, is\n",
    "there another view of the population code that might be more\n",
    "informative?\n",
    "\n",
    "### Some geometric intuition\n",
    "\n",
    "Since we only have $P = 2$ neurons, we can visualize these data as a\n",
    "scatterplot, which makes their [covariance](reference.qmd#covariance)\n",
    "apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-joint-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_joint_scatter(\n",
    "    data: xr.DataArray, *, coord: str, dim: str, template_func: Callable[[int], str]\n",
    ") -> mpl.figure.Figure:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    data_ = pd.DataFrame(\n",
    "        {coord_: data[coord_].values for coord_ in (\"color\", \"size\")}\n",
    "        | {\n",
    "            template_func(index): data.isel({dim: index - 1}).to_dataframe()[coord]\n",
    "            for index in (1, 2)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data_,\n",
    "        x=template_func(1),\n",
    "        y=template_func(2),\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        legend=False,\n",
    "        palette=\"flare\",\n",
    "    )\n",
    "    ax.axhline(0, c=\"gray\", ls=\"--\")\n",
    "    ax.axvline(0, c=\"gray\", ls=\"--\")\n",
    "\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_joint_scatter(\n",
    "    data,\n",
    "    coord=\"neuron responses\",\n",
    "    dim=\"neuron\",\n",
    "    template_func=lambda x: f\"neuron {x} response\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfbadf0d0a3de0c2311b76f9c3c8457097bfe9-0",
   "metadata": {},
   "source": [
    "The organization of these data in this 2-dimensional space suggests an\n",
    "obvious way to change our viewpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_pca_animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_pca_transformation(\n",
    "    data: xr.DataArray,\n",
    "    *,\n",
    "    durations: dict[str, int] = {\n",
    "        \"center\": 1_000,\n",
    "        \"rotate\": 1_000,\n",
    "        \"pause\": 1_500,\n",
    "    },\n",
    "    interval: int = 50,\n",
    ") -> str:\n",
    "    def _compute_2d_rotation_matrix(theta: float) -> np.ndarray:\n",
    "        return np.array(\n",
    "            [\n",
    "                [np.cos(theta), -np.sin(theta)],\n",
    "                [np.sin(theta), np.cos(theta)],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    fig = view_joint_scatter(\n",
    "        data,\n",
    "        coord=\"neuron responses\",\n",
    "        dim=\"neuron\",\n",
    "        template_func=lambda x: f\"neuron {x} response\",\n",
    "    )\n",
    "    ax = fig.get_axes()[0]\n",
    "    scatter = ax.get_children()[0]\n",
    "    title = fig.suptitle(\"neuron responses\")\n",
    "\n",
    "    n_frames = {key: value // interval + 1 for key, value in durations.items()}\n",
    "\n",
    "    x_mean, y_mean = data.mean(\"stimulus\").values\n",
    "    delta = np.array([x_mean, y_mean]) / n_frames[\"center\"]\n",
    "\n",
    "    _, _, v_h = np.linalg.svd(data - data.mean(\"stimulus\"))\n",
    "    v = v_h.transpose()\n",
    "    theta = np.arccos(v[0, 0])\n",
    "    rotation = _compute_2d_rotation_matrix(-theta / n_frames[\"rotate\"])\n",
    "\n",
    "    transformed = (data - data.mean(\"stimulus\")).values @ v\n",
    "\n",
    "    radius = max(np.linalg.norm(transformed, axis=-1))\n",
    "    limit = max(np.abs(data).max(), np.abs(transformed).max(), radius)\n",
    "    ax.set_xlim([-limit, limit])\n",
    "    ax.set_ylim([-limit, limit])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    frame_to_retitle_center = 2 * n_frames[\"pause\"]\n",
    "    frame_to_start_centering = frame_to_retitle_center + n_frames[\"pause\"]\n",
    "    frame_to_stop_centering = frame_to_start_centering + n_frames[\"center\"]\n",
    "    frame_to_retitle_rotate = frame_to_stop_centering + n_frames[\"pause\"]\n",
    "    frame_to_start_rotating = frame_to_retitle_rotate + n_frames[\"pause\"]\n",
    "    frame_to_stop_rotating = frame_to_start_rotating + n_frames[\"rotate\"]\n",
    "    frame_to_retitle_transformed = frame_to_stop_rotating + n_frames[\"pause\"]\n",
    "    frame_to_end = frame_to_retitle_transformed + 2 * n_frames[\"pause\"]\n",
    "\n",
    "    def _update(frame: int) -> None:\n",
    "        if frame < frame_to_retitle_center:\n",
    "            return\n",
    "        elif frame == frame_to_retitle_center:\n",
    "            title.set_text(\"center the data\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        elif frame < frame_to_start_centering:\n",
    "            return\n",
    "        elif frame <= frame_to_stop_centering:\n",
    "            scatter.set_offsets(scatter.get_offsets() - delta)\n",
    "        elif frame == frame_to_retitle_rotate:\n",
    "            title.set_text(\"rotate the data\")\n",
    "        elif frame < frame_to_start_rotating:\n",
    "            return\n",
    "        elif frame <= frame_to_stop_rotating:\n",
    "            scatter.set_offsets(scatter.get_offsets().data @ rotation)\n",
    "        elif frame < frame_to_retitle_transformed:\n",
    "            return\n",
    "        elif frame == frame_to_retitle_transformed:\n",
    "            title.set_text(\"principal components\")\n",
    "            ax.set_xlabel(\"principal component 1\")\n",
    "            ax.set_ylabel(\"principal component 2\")\n",
    "        elif frame <= frame_to_end:\n",
    "            return\n",
    "\n",
    "    animation = FuncAnimation(\n",
    "        fig=fig,\n",
    "        func=_update,\n",
    "        frames=frame_to_end,\n",
    "        interval=interval,\n",
    "        repeat=True,\n",
    "        repeat_delay=2_500,\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    return animation.to_html5_video()\n",
    "\n",
    "\n",
    "display(HTML(animate_pca_transformation(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486eb3b1c2fb1e1331ad2d8b25d6f9737eaafa24-0",
   "metadata": {},
   "source": [
    "### The mathematical definition\n",
    "\n",
    "Given a data matrix $X \\in \\mathbb{R}^{N \\times P}$, we need to compute\n",
    "the [eigendecomposition](reference.qmd#eigendecomposition)[1] of its\n",
    "[auto-covariance](reference.qmd#auto-covariance)[2]:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{cov}(X)\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})\\\\\n",
    "    &= V \\Lambda V^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The columns of $V$ are *eigenvectors* that specify the directions of\n",
    "variance while the corresponding diagonal elements of $\\Lambda$ are\n",
    "*eigenvalues* that specify the amount of variance along the\n",
    "eigenvector[3].\n",
    "\n",
    "The original data matrix can be transformed by projecting it onto the\n",
    "eigenvectors: $\\tilde{X} = \\left(X - \\overline{X}\\right) V$.\n",
    "\n",
    "> **Viewing PCA as an optimization**\n",
    ">\n",
    "> PCA can be used to project data into a lower-dimensional space\n",
    "> (i.e. $p \\le f$) in a way that best preserves the geometry of the\n",
    "> data. Specifically, computing a PCA decomposition of $X$ yields a\n",
    "> matrix $V \\in \\mathbb{R}^{f \\times p}$ such that\n",
    "> $V = \\argmin_{V \\in \\mathbb{U_{f \\times p}}} \\sum_{i=1}^n \\left|| x_i - VV^\\top x_i \\right||_2$,\n",
    "> where $||\\cdot||_2$ denotes the $L_2$-norm and\n",
    "> $\\mathbb{U_{f \\times p}}$ denotes the set of orthonormal matrices with\n",
    "> shape $f \\times p$.\n",
    "\n",
    "### Transforming the data\n",
    "\n",
    "[1] The eigendecomposition of a symmetric matrix\n",
    "$X \\in \\mathbb{R}^{n \\times n}$ involves rewriting it as the product of\n",
    "three matrices $X = V \\Lambda V^\\top$, where $V \\in \\mathbb{n \\times n}$\n",
    "is orthonormal and $\\Lambda \\in \\mathbb{n \\times n}$ is diagonal with\n",
    "non-negative entries.\n",
    "\n",
    "[2] Given a data matrix $X \\in \\mathbb{R}^{n \\times f}$ containing\n",
    "neural responses to $n$ stimuli from $f$ neurons, the *auto-covariance*\n",
    "of $X$ (or simply its *covariance*) is defined as:\n",
    "\n",
    "$\\text{cov}(X) = \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})$\n",
    "\n",
    "This is an $f \\times f$ matrix where the $(i, j)$-th element measures\n",
    "how much neuron $i$ covaries with neuron $j$. If the covariance is\n",
    "positive, they tend to have similar activation: a stimulus that\n",
    "activates one neuron will tend to activate the other. If the covariance\n",
    "is negative, the neurons will have dissimilar activation: a stimulus\n",
    "that activates one neuron will likely not activate the other.\n",
    "\n",
    "[3] Let’s compute the auto-covariance of the projected data $\\tilde{X}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{cov}(\\tilde{X})\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\tilde{X}^\\top \\tilde{X}\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\left((X - \\overline{X})V\\right)^\\top \\left((X - \\overline{X})V\\right)\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) V^\\top (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left( V \\Lambda V^\\top \\right) V\\\\\n",
    "    &= I \\Lambda I\\\\\n",
    "    &= \\Lambda\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f6637fc0317cbc4b3551b5e176f41f71321b6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean: np.ndarray\n",
    "        self.eigenvectors: np.ndarray\n",
    "        self.eigenvalues: np.ndarray\n",
    "\n",
    "    def fit(self, /, data: np.ndarray) -> None:\n",
    "        self.mean = data.mean(axis=-2)\n",
    "\n",
    "        data_centered = data - self.mean\n",
    "        _, s, v_t = np.linalg.svd(data_centered)\n",
    "\n",
    "        n_stimuli = data.shape[-2]\n",
    "\n",
    "        self.eigenvectors = np.swapaxes(v_t, -1, -2)\n",
    "        self.eigenvalues = s**2 / (n_stimuli - 1)\n",
    "\n",
    "    def transform(self, /, data: np.ndarray) -> np.ndarray:\n",
    "        return (data - self.mean) @ self.eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f30a170133d0284dd0a4343e4a783520cc8695-0",
   "metadata": {},
   "source": [
    "> **Why do we compute PCA this way instead of\n",
    "> $\\text{eig}(\\text{cov}(X))$?**\n",
    ">\n",
    "> To apply PCA to a data matrix, we might be tempted to use the\n",
    "> definition and naively compute its\n",
    "> [auto-covariance](reference.qmd#auto-covariance) followed by an\n",
    "> [eigendecomposition](reference.qmd#eigendecomposition). However, when\n",
    "> the number of neurons $P$ is large, this approach is memory-intensive\n",
    "> and prone to numerical errors.\n",
    ">\n",
    "> Instead, we can use the [singular value\n",
    "> decomposition](reference.qmd#singular-value-decomposition) (SVD) of\n",
    "> $X$ to efficiently compute its PCA transformation. Specifically,\n",
    "> $X = U \\Sigma V^\\top$ is a singular value decomposition, where $U$ and\n",
    "> $V$ are orthonormal and $\\Sigma$ is diagonal.\n",
    ">\n",
    "> The auto-covariance matrix reduces to\n",
    "> $X^\\top X / (n - 1) = V \\left(\\frac{\\Sigma^2}{n - 1} \\right) V^\\top$,\n",
    "> which is exactly the eigendecomposition required.\n",
    ">\n",
    "> Specifically, the eigenvalues $\\lambda_i$ of the auto-covariance\n",
    "> matrix are related to the singular values $\\sigma_i$ of the data\n",
    "> matrix as $\\lambda_i = \\sigma_i^2 / (N - 1)$, while the eigenvectors\n",
    "> of the auto-covariance matrix are exactly the right singular vectors\n",
    "> $V$ of the data matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pca-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(data: xr.DataArray) -> xr.Dataset:\n",
    "    pca = PCA()\n",
    "    pca.fit(data.values)\n",
    "\n",
    "    data_transformed = pca.transform(data.values)\n",
    "\n",
    "    return xr.Dataset(\n",
    "        data_vars={\n",
    "            \"score\": xr.DataArray(\n",
    "                data=data_transformed,\n",
    "                dims=(\"stimulus\", \"component\"),\n",
    "            ),\n",
    "            \"eigenvector\": xr.DataArray(\n",
    "                data=pca.eigenvectors,\n",
    "                dims=(\"component\", \"neuron\"),\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"rank\": (\"component\", 1 + np.arange(data_transformed.shape[-1])),\n",
    "            \"eigenvalue\": (\"component\", pca.eigenvalues),\n",
    "        }\n",
    "        | {coord: (data[coord].dims[0], data[coord].values) for coord in data.coords},\n",
    "    )\n",
    "\n",
    "\n",
    "pca = compute_pca(data)\n",
    "view_joint_scatter(\n",
    "    pca[\"score\"],\n",
    "    coord=\"score\",\n",
    "    dim=\"component\",\n",
    "    template_func=lambda x: f\"principal component {x}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1cc3289fc71c8a0f53f1a7f1d2b9d055df9e28-0",
   "metadata": {},
   "source": [
    "Let’s view the data projected onto each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-view_principal_components",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_individual_scatter(\n",
    "    pca[\"score\"],\n",
    "    coord=\"rank\",\n",
    "    dim=\"component\",\n",
    "    template_func=lambda x: f\"principal component {x}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48320d33c24dcb3bb7b77273b5900b25966c0aa4-0",
   "metadata": {},
   "source": [
    "We can observe that:\n",
    "\n",
    "-   the *first* principal component is largely driven by the *size* of\n",
    "    the stimulus\n",
    "-   the *second* principal component is largely driven by the *color* of\n",
    "    the stimulus\n",
    "\n",
    "However, note that these components do not directly correspond to either\n",
    "of the latent variables; rather, each is a mixture of stimulus-dependent\n",
    "signal and noise.\n",
    "\n",
    "### Analyzing the covariance statistics\n",
    "\n",
    "We can compute the variance along each eigenvector. The total variance\n",
    "along all eigenvectors is the same as the total variance of the original\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dcbf335c29593d34f7b0e29810b0ce622374d-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca[\"eigenvalue\"].round(3)\n",
    "for i_neuron in range(eigenvalues.sizes[\"component\"]):\n",
    "    print(\n",
    "        f\"variance along eigenvector {i_neuron + 1} (eigenvalue {i_neuron + 1}):\"\n",
    "        f\" {eigenvalues[i_neuron].values}\"\n",
    "    )\n",
    "print(f\"total variance: {eigenvalues.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_eigenspectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_eigenspectrum(pca: xr.DataArray) -> mpl.figure.Figure:\n",
    "    fig, ax = plt.subplots(figsize=(pca.sizes[\"component\"], 5))\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=pca[\"component\"].to_dataframe(),\n",
    "        x=\"rank\",\n",
    "        y=\"eigenvalue\",\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax.set_xticks(pca[\"rank\"].values)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_eigenspectrum(pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec112d46512c1ed846935c0319402bb37a3d1f1c-0",
   "metadata": {},
   "source": [
    "## More neurons!\n",
    "\n",
    "Let’s record from more neurons (say $P = 10$)! Here, the *ambient\n",
    "dimensionality* of the feature space is $P = 10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-simulate_more_neurons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate_random_neuron(rng: np.random.Generator) -> Neuron:\n",
    "    return Neuron(\n",
    "        beta_color=rng.integers(-10, 11),\n",
    "        beta_size=rng.integers(-10, 11),\n",
    "        std=rng.integers(-10, 11),\n",
    "        mean=rng.integers(-10, 11),\n",
    "    )\n",
    "\n",
    "\n",
    "neurons = tuple([_simulate_random_neuron(rng) for _ in range(10)])\n",
    "\n",
    "big_data = simulate_multiple_neuron_responses(\n",
    "    stimuli=stimuli,\n",
    "    neurons=neurons,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "display(big_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-view_principal_components_big",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_pca = compute_pca(big_data)\n",
    "\n",
    "view_individual_scatter(\n",
    "    big_pca[\"score\"],\n",
    "    coord=\"rank\",\n",
    "    dim=\"component\",\n",
    "    template_func=lambda x: f\"principal component {x}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_eigenspectrum_big",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_eigenspectrum(big_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa326086ab81fab77b12529260bb437c51ece4-0",
   "metadata": {},
   "source": [
    "However, we know that this data was generated from exactly *2* latent\n",
    "variables – *color* and *size*. We refer to these as the *effective\n",
    "dimensions*. In this example, the remaining dimensions, which correspond\n",
    "to noise, have relatively low variance, and we can see a signficant drop\n",
    "in the eigenspectrum after the second eigenvector.\n",
    "\n",
    "### Quantifying dimensionality\n",
    "\n",
    "Based on the spectrum, several approaches are used to quantify the\n",
    "latent dimensionality of a dataset:\n",
    "\n",
    "#### Rank of the covariance matrix\n",
    "\n",
    "The *rank* of the covariance matrix – equal to the number of nonzero\n",
    "eigenvalues – would be the latent dimensionality in the ideal setting\n",
    "where the data has zero noise. In real data, the rank is typically equal\n",
    "to the ambient dimensionality, since there is typically some variance\n",
    "along every dimension.\n",
    "\n",
    "#### Setting an arbitrary variance threshold\n",
    "\n",
    "Though not typically used today, another approach is to set an arbitrary\n",
    "threshold on the variance (historically recommended as $1$ for\n",
    "normalized data); only dimensions with variance above that threshold are\n",
    "considered useful.\n",
    "\n",
    "#### Setting an arbitrary *cumulative* variance threshold\n",
    "\n",
    "A very commonly used method is to set a threshold based on the\n",
    "cumulative variance of the data: the number of dimensions required to\n",
    "exceed, say $80\\%$ of the variance, is taken as the latent\n",
    "dimensionality.\n",
    "\n",
    "#### Eyeballing the “knee” of the spectrum\n",
    "\n",
    "When the number of latent dimensions is low, eigenspectra often have a\n",
    "sharp discontinuity (the “knee”), where a small number of dimensions\n",
    "have high-variance and the remainder have much have lower variance. The\n",
    "latent dimensionality is then taken to be the number of dimensions above\n",
    "the threshold, determined by eye.\n",
    "\n",
    "#### Computing a summary statistic over the entire spectrum\n",
    "\n",
    "A metric such as *effective dimensionality* summarizes the spectrum\n",
    "using an entropy-like measure, taking into account variances along all\n",
    "the dimensions:\n",
    "\n",
    "$$\\text{effective dimensionality}(\\lambda_1, \\dots \\lambda_n) = \\dfrac{\\left( \\sum_{i=1}^n \\lambda_i \\right)^2}{\\sum_{i=1}^n \\lambda_i^2}$$\n",
    "\n",
    "However, keep in mind that this is a toy example with idealized data. As\n",
    "we will see, when using standard PCA on real data it may be impossible\n",
    "to identify a clear distinction between meaningful dimensions and noise.\n",
    "\n",
    "> **Only need the first few PCs?**\n",
    ">\n",
    "> Check out [truncated\n",
    "> SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
