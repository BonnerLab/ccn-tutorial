{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1811f52b5df744cf3e567a2e7c0722cc3abdf5f6-0",
   "metadata": {},
   "source": [
    "# An introduction to PCA\n",
    "\n",
    "Using principal component analysis to reveal latent structure in data\n",
    "\n",
    "PCA is often used to reduce the dimensionality of large data while\n",
    "preserving a significant amount of variance. More fundamentally, it is a\n",
    "framework for studying the covariance statistics of data. In this\n",
    "section, we will introduce the concept of PCA with some toy examples.\n",
    "\n",
    "## A simple experiment\n",
    "\n",
    "Let’s perform an imaginary neuroscience experiment! We’ll record\n",
    "voltages from $P = 2$ neurons in visual cortex while the participant\n",
    "passively views $N = 1000$ dots of different *colors* and *sizes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac96e5899afd051fca82d6225e3c5b69d3104a02-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO uncomment before final packaging\n",
    "# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa634cca2f2cbce0aa636d516ce2d091a244dc3-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Callable\n",
    "import warnings\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf00b08e74464463d4cfec60b6e479b74bb0b6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"white\",\n",
    "    palette=\"deep\",\n",
    "    rc={\"legend.edgecolor\": \"None\"},\n",
    ")\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "\n",
    "xr.set_options(display_max_rows=3, display_expand_data=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81958efb039da8a9d6d3dcfc190f98e5a010f2c7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "rng = np.random.default_rng(seed=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ab78bf5b089003c269fb4fe71efbb6d6d3cd9-0",
   "metadata": {},
   "source": [
    "### Creating the stimuli\n",
    "\n",
    "Let’s create $N = 1000$ dots of different *colors* and *sizes*. From the\n",
    "scatterplot, we can see that the two latent variables are\n",
    "[uncorrelated](reference.qmd#correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f97322fd56f5f1039eec952a19d431b9d7d8f1-0",
   "metadata": {
    "add-from": "../src/utilities/toy_example.py",
    "code-filename": "(Function) Create stimulus dots of various colors and sizes",
    "end-line": 31,
    "source-lang": "python",
    "start-line": 21
   },
   "outputs": [],
   "source": [
    "def create_stimuli(\n",
    "    n: int,\n",
    "    *,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"color\": rng.random(size=(n,)),\n",
    "            \"size\": rng.random(size=(n,)),\n",
    "        }\n",
    "    ).set_index(1 + np.arange(n))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-create_the_stimuli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_stimuli(data: pd.DataFrame) -> Figure:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data,\n",
    "        x=\"color\",\n",
    "        y=\"size\",\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        palette=\"flare\",\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(ax=ax, trim=True)\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "stimuli = create_stimuli(n=1_000, rng=rng)\n",
    "\n",
    "view_stimuli(stimuli)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd30730d22428120b4c7d15060f5a11fb114a20-0",
   "metadata": {},
   "source": [
    "### Simulating neural responses\n",
    "\n",
    "Now, let’s simulate some neural data. We need to decide how the $P = 2$\n",
    "neurons might respond to these $N = 1000$ stimulus dots. Each neuron\n",
    "could respond to either one or both of the latent features that define\n",
    "these stimuli – $\\text{color}$ and $\\text{size}$. The neuron’s responses\n",
    "could also be subject to noise.\n",
    "\n",
    "Here, we model each neuron’s response $r_\\text{neuron}$ as a simple\n",
    "linear combination of the two latent features with stimulus-independent\n",
    "Gaussian noise $\\epsilon$:\n",
    "\n",
    "$r_{\\text{neuron}} \\sim \\beta_{\\text{color}} \\left( \\text{color} \\right) + \\beta_{\\text{size}} \\left( \\text{size} \\right) + \\epsilon$,\n",
    "where\n",
    "$\\epsilon \\sim \\mathcal{N}(\\mu_{\\text{neuron}}, \\sigma_{\\text{neuron}}^2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4d0a4f3a763e4b5ccb42692afdbab074b2d8a4-0",
   "metadata": {
    "add-from": "../src/utilities/toy_example.py",
    "code-filename": "(Definition) Parameters controlling neuron responses",
    "end-line": 18,
    "source-lang": "python",
    "start-line": 12
   },
   "outputs": [],
   "source": [
    "Neuron = NamedTuple(\n",
    "    \"Neuron\",\n",
    "    beta_color=float,\n",
    "    beta_size=float,\n",
    "    mean=float,\n",
    "    std=float,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1d69b5ebeae82a60df1235c73550f170141422-0",
   "metadata": {},
   "source": [
    "As we can see, each neuron’s response is completely defined by exactly\n",
    "four parameters:\n",
    "\n",
    "-   $\\beta_{\\text{color}}$ – how much the neuron cares about the\n",
    "    stimulus $\\text{color}$\n",
    "-   $\\beta_{\\text{size}}$ – how much the neuron cares about the stimulus\n",
    "    $\\text{size}$\n",
    "-   $\\mu_{\\text{neuron}}$ – the mean of the neuron’s responses\n",
    "-   $\\sigma_{\\text{neuron}}^2$ – the stimulus-independent variance of\n",
    "    the neuron’s responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2813aab4756052d17de28f6e49ce75bfe13fb795-0",
   "metadata": {
    "add-from": "../src/utilities/toy_example.py",
    "code-filename": "(Function) Simulate neuron responses",
    "end-line": 48,
    "source-lang": "python",
    "start-line": 34
   },
   "outputs": [],
   "source": [
    "def simulate_neuron_responses(\n",
    "    stimuli: pd.DataFrame,\n",
    "    neuron: Neuron,\n",
    "    *,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    def z_score(x: np.ndarray) -> np.ndarray:\n",
    "        return (x - x.mean()) / x.std()\n",
    "\n",
    "    return (\n",
    "        neuron.beta_color * z_score(stimuli[\"color\"])\n",
    "        + neuron.beta_size * z_score(stimuli[\"size\"])\n",
    "        + neuron.std * rng.standard_normal(size=(len(stimuli),))\n",
    "        + neuron.mean\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c51aaa9effe2e85aebbdc42f537e8de0d75373-0",
   "metadata": {
    "add-from": "../src/utilities/toy_example.py",
    "code-filename": "(Function) Simulate multiple neurons' responses",
    "end-line": 85,
    "source-lang": "python",
    "start-line": 51
   },
   "outputs": [],
   "source": [
    "def simulate_multiple_neuron_responses(\n",
    "    *,\n",
    "    stimuli: pd.DataFrame,\n",
    "    neurons: Sequence[Neuron],\n",
    "    rng: np.random.Generator,\n",
    ") -> xr.DataArray:\n",
    "    data = []\n",
    "    for i_neuron, neuron in enumerate(neurons):\n",
    "        data.append(\n",
    "            xr.DataArray(\n",
    "                data=simulate_neuron_responses(\n",
    "                    stimuli=stimuli,\n",
    "                    neuron=neuron,\n",
    "                    rng=rng,\n",
    "                ),\n",
    "                dims=(\"stimulus\",),\n",
    "                coords={\n",
    "                    column: (\"stimulus\", values)\n",
    "                    for column, values in stimuli.reset_index(names=\"stimulus\").items()\n",
    "                },\n",
    "            )\n",
    "            .expand_dims({\"neuron\": [i_neuron + 1]})\n",
    "            .assign_coords(\n",
    "                {\n",
    "                    field: (\"neuron\", [float(value)])\n",
    "                    for field, value in neuron._asdict().items()\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return (\n",
    "        xr.concat(data, dim=\"neuron\")\n",
    "        .rename(\"neuron responses\")\n",
    "        .transpose(\"stimulus\", \"neuron\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519fbf842da470e45d4b1806aff50c28f59013b3-0",
   "metadata": {},
   "source": [
    "This procedure produces a data matrix $X \\in \\mathbb{R}^{N \\times P}$\n",
    "containing the $P = 2$ neurons’ responses to the $N = 1000$ stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-simulate_the_neuron_responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = (\n",
    "    Neuron(beta_color=3, beta_size=-2, std=1, mean=7),\n",
    "    Neuron(beta_color=-2, beta_size=5, std=3, mean=-6),\n",
    ")\n",
    "\n",
    "data = simulate_multiple_neuron_responses(\n",
    "    stimuli=stimuli,\n",
    "    neurons=neurons,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec3b6ccff70ef7f1bb1df0f7d582ae9a5cb807-0",
   "metadata": {},
   "source": [
    "## Understanding the neural code\n",
    "\n",
    "In this toy example, we know the ground truth of the neurons’ responses\n",
    "– how exactly it depends on the stimulus features. Unfortunately, in\n",
    "real experiments, we don’t have this luxury and need to derive the\n",
    "neural coding strategy from the data. Let’s now pretend that this toy\n",
    "example is real data, where we don’t know the ground truth.\n",
    "\n",
    "How is information about the stimulus encoded in the population\n",
    "activity? Is there a neuron that is sensitive to color and another that\n",
    "is sensitive to size?\n",
    "\n",
    "One way to understand this is by studying the neurons directly. Let’s\n",
    "start by visualizing the response of each neuron to our stimuli.\n",
    "\n",
    "<span class=\"column-margin\">These plots are 1-dimensional scatterplots\n",
    "and the spread along the vertical axis is just for visualization\n",
    "purposes.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd37dc38d54c4b0766da0dab88be5b90f4c8840d-0",
   "metadata": {
    "add-from": "../src/utilities/toy_example.py",
    "code-filename": "(Function) Visualize individual scatter",
    "end-line": 129,
    "source-lang": "python",
    "start-line": 88
   },
   "outputs": [],
   "source": [
    "def view_individual_scatter(\n",
    "    data: xr.DataArray,\n",
    "    *,\n",
    "    coord: str,\n",
    "    dim: str,\n",
    "    template_func: Callable[[int], str],\n",
    ") -> Figure:\n",
    "    rng = np.random.default_rng()\n",
    "    data_ = data.assign_coords(\n",
    "        {\"arbitrary\": (\"stimulus\", rng.random(data.sizes[\"stimulus\"]))}\n",
    "    )\n",
    "    min_, max_ = data_.min(), data_.max()\n",
    "\n",
    "    n_features = data.sizes[dim]\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=n_features, figsize=(7, 2 * n_features))\n",
    "\n",
    "    for index, ax in zip(data[coord].values, axes.flat):\n",
    "        label = template_func(index)\n",
    "        sns.scatterplot(\n",
    "            ax=ax,\n",
    "            data=(\n",
    "                data_.isel({dim: data[coord].values == index})\n",
    "                .rename(label)\n",
    "                .to_dataframe()\n",
    "            ),\n",
    "            x=label,\n",
    "            y=\"arbitrary\",\n",
    "            hue=\"color\",\n",
    "            size=\"size\",\n",
    "            palette=\"flare\",\n",
    "            legend=False,\n",
    "        )\n",
    "        sns.despine(ax=ax, left=True, offset=10)\n",
    "\n",
    "        ax.set_xlim([min_, max_])\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout(h_pad=3)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_neuron_responses",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_individual_scatter(\n",
    "    data,\n",
    "    coord=\"neuron\",\n",
    "    dim=\"neuron\",\n",
    "    template_func=lambda x: f\"neuron {x} response\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e54b9915f07dbfb2a95d477071f702a0555875f-0",
   "metadata": {},
   "source": [
    "By visualizing the responses, we can see that each neuron is tuned to\n",
    "both color *and* size.\n",
    "\n",
    "We can also use methods such as Representational Similarity Analysis\n",
    "(Kriegeskorte 2008) to study the information content of the two neurons.\n",
    "In RSA, we compute the pairwise dissimilarities between the\n",
    "representations of the stimuli and organize them into a\n",
    "*representational dissimilarity matrix*, or RDM.\n",
    "\n",
    "The entries of an RDM indicate the degree to which each pair of stimuli\n",
    "is distinguished by the neurons and the RDM as a whole measures the\n",
    "overall population geometry of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-display-rdm",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_rdm(data: xr.DataArray) -> np.ndarray:\n",
    "    data_ = data.copy()\n",
    "    data_[\"color bin\"] = (\n",
    "        \"stimulus\",\n",
    "        np.digitize(data_[\"color\"].values, bins=np.linspace(0, 1, 10)),\n",
    "    )\n",
    "    data_[\"size bin\"] = (\n",
    "        \"stimulus\",\n",
    "        np.digitize(data_[\"size\"].values, bins=np.linspace(0, 1, 10)),\n",
    "    )\n",
    "    data_ = data_.sortby([\"color bin\", \"size bin\"])\n",
    "    rdm = squareform(pdist(data_.values))\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        figsize=(8, 8),\n",
    "        nrows=2,\n",
    "        ncols=2,\n",
    "        height_ratios=[1, 20],\n",
    "        width_ratios=[1, 20],\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "    )\n",
    "    axes[0, 1].scatter(\n",
    "        np.arange(data_.sizes[\"stimulus\"]),\n",
    "        np.zeros(data_.sizes[\"stimulus\"]),\n",
    "        c=data_[\"color\"].values,\n",
    "        s=data_[\"size\"].values * 100,\n",
    "    )\n",
    "    axes[0, 1].axis(\"off\")\n",
    "\n",
    "    axes[1, 0].scatter(\n",
    "        np.zeros(data_.sizes[\"stimulus\"]),\n",
    "        np.arange(data_.sizes[\"stimulus\"]),\n",
    "        c=data_[\"color\"].values,\n",
    "        s=data_[\"size\"].values * 100,\n",
    "    )\n",
    "\n",
    "    axes[1, 1].imshow(rdm, cmap=\"viridis\")\n",
    "    axes[1, 1].set_aspect(\"equal\", \"box\")\n",
    "\n",
    "    for ax in axes.flat:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout(w_pad=0, h_pad=0)\n",
    "    plt.close(fig)\n",
    "    return fig\n",
    "\n",
    "\n",
    "display_rdm(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34248a5e04d4788fbbd211aa58cbd9f837fe992d-0",
   "metadata": {},
   "source": [
    "This RDM reveals that this population of $P = 2$ neurons distinguishes\n",
    "the stimulus dots based on their color and size – as expected!\n",
    "\n",
    "What if we want to study the underlying structure of this code? Is there\n",
    "another view of the population code that might be more informative?\n",
    "\n",
    "## Studying the latent dimensions\n",
    "\n",
    "Instead of directly studying the neurons, we can focus on the underlying\n",
    "factors that capture the structure and variance in the data.\n",
    "\n",
    "### Some geometric intuition\n",
    "\n",
    "Since we only have $P = 2$ neurons, we can visualize these data as a\n",
    "scatterplot, which makes their [covariance](reference.qmd#covariance)\n",
    "apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-view_joint_scatter",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_joint_scatter(\n",
    "    data: xr.DataArray,\n",
    "    *,\n",
    "    coord: str,\n",
    "    dim: str,\n",
    "    template_func: Callable[[int], str],\n",
    "    draw_axes: bool = False,\n",
    ") -> Figure:\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    data_ = pd.DataFrame(\n",
    "        {coord_: data[coord_].values for coord_ in (\"color\", \"size\")}\n",
    "        | {\n",
    "            template_func(index): data.isel({dim: index - 1}).to_dataframe()[coord]\n",
    "            for index in (1, 2)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data_,\n",
    "        x=template_func(1),\n",
    "        y=template_func(2),\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        legend=False,\n",
    "        palette=\"flare\",\n",
    "    )\n",
    "    if draw_axes:\n",
    "        ax.axhline(0, c=\"gray\", ls=\"--\")\n",
    "        ax.axvline(0, c=\"gray\", ls=\"--\")\n",
    "\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_joint_scatter(\n",
    "    data,\n",
    "    coord=\"neuron responses\",\n",
    "    dim=\"neuron\",\n",
    "    template_func=lambda x: f\"neuron {x} response\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa746095ddc11c81e7c5a14594674abdac0619a-0",
   "metadata": {},
   "source": [
    "We can use the covariance to change the way we view the data.\n",
    "\n",
    "> **Tip**\n",
    ">\n",
    "> Click on the animation above to visualize the PCA transformation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_pca_animation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def animate_pca_transformation(\n",
    "    data: xr.DataArray,\n",
    "    *,\n",
    "    durations: dict[str, int] = {\n",
    "        \"center\": 1_000,\n",
    "        \"rotate\": 1_000,\n",
    "        \"pause\": 500,\n",
    "    },\n",
    "    interval: int = 50,\n",
    ") -> str:\n",
    "    def _compute_2d_rotation_matrix(theta: float) -> np.ndarray:\n",
    "        return np.array(\n",
    "            [\n",
    "                [np.cos(theta), -np.sin(theta)],\n",
    "                [np.sin(theta), np.cos(theta)],\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    fig = view_joint_scatter(\n",
    "        data,\n",
    "        coord=\"neuron responses\",\n",
    "        dim=\"neuron\",\n",
    "        template_func=lambda x: f\"neuron {x} response\",\n",
    "        draw_axes=True,\n",
    "    )\n",
    "    ax = fig.get_axes()[0]\n",
    "    scatter = ax.get_children()[0]\n",
    "    title = fig.suptitle(\"neuron responses\")\n",
    "\n",
    "    n_frames = {key: value // interval + 1 for key, value in durations.items()}\n",
    "\n",
    "    x_mean, y_mean = data.mean(\"stimulus\").values\n",
    "    delta = np.array([x_mean, y_mean]) / n_frames[\"center\"]\n",
    "\n",
    "    _, _, v_h = np.linalg.svd(data - data.mean(\"stimulus\"))\n",
    "    v = v_h.transpose()\n",
    "    theta = np.arccos(v[0, 0])\n",
    "    rotation = _compute_2d_rotation_matrix(-theta / n_frames[\"rotate\"])\n",
    "\n",
    "    transformed = (data - data.mean(\"stimulus\")).values @ v\n",
    "\n",
    "    radius = max(np.linalg.norm(transformed, axis=-1))\n",
    "    limit = max(np.abs(data).max(), np.abs(transformed).max(), radius)\n",
    "    ax.set_xlim([-limit, limit])\n",
    "    ax.set_ylim([-limit, limit])\n",
    "    fig.tight_layout()\n",
    "\n",
    "    frame_to_retitle_center = 2 * n_frames[\"pause\"]\n",
    "    frame_to_start_centering = frame_to_retitle_center + n_frames[\"pause\"]\n",
    "    frame_to_stop_centering = frame_to_start_centering + n_frames[\"center\"]\n",
    "    frame_to_retitle_rotate = frame_to_stop_centering + n_frames[\"pause\"]\n",
    "    frame_to_start_rotating = frame_to_retitle_rotate + n_frames[\"pause\"]\n",
    "    frame_to_stop_rotating = frame_to_start_rotating + n_frames[\"rotate\"]\n",
    "    frame_to_retitle_transformed = frame_to_stop_rotating + n_frames[\"pause\"]\n",
    "    frame_to_end = frame_to_retitle_transformed + 2 * n_frames[\"pause\"]\n",
    "\n",
    "    def _update(frame: int) -> None:\n",
    "        if frame < frame_to_retitle_center:\n",
    "            return\n",
    "        elif frame == frame_to_retitle_center:\n",
    "            title.set_text(\"step 1 of 2: center the data\")\n",
    "            ax.set_xlabel(\"\")\n",
    "            ax.set_ylabel(\"\")\n",
    "        elif frame < frame_to_start_centering:\n",
    "            return\n",
    "        elif frame <= frame_to_stop_centering:\n",
    "            scatter.set_offsets(scatter.get_offsets() - delta)\n",
    "        elif frame == frame_to_retitle_rotate:\n",
    "            title.set_text(\"step 2 of 2: rotate the data\")\n",
    "        elif frame < frame_to_start_rotating:\n",
    "            return\n",
    "        elif frame <= frame_to_stop_rotating:\n",
    "            scatter.set_offsets(scatter.get_offsets().data @ rotation)\n",
    "        elif frame < frame_to_retitle_transformed:\n",
    "            return\n",
    "        elif frame == frame_to_retitle_transformed:\n",
    "            title.set_text(\"principal components\")\n",
    "            ax.set_xlabel(\"principal component 1\")\n",
    "            ax.set_ylabel(\"principal component 2\")\n",
    "        elif frame <= frame_to_end:\n",
    "            return\n",
    "\n",
    "    animation = FuncAnimation(\n",
    "        fig=fig,\n",
    "        func=_update,\n",
    "        frames=frame_to_end,\n",
    "        interval=interval,\n",
    "        repeat=False,\n",
    "    )\n",
    "    plt.close(fig)\n",
    "    return animation.to_html5_video()\n",
    "\n",
    "\n",
    "display(HTML(animate_pca_transformation(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e844ef005ed1fb87ffa8498d288dc696552db-0",
   "metadata": {},
   "source": [
    "As seen in the animation, we can transform our data to view the\n",
    "directions of maximum variance. These directions are the principal\n",
    "components of our data.\n",
    "\n",
    "### The mathematical definition\n",
    "\n",
    "Given a data matrix $X \\in \\mathbb{R}^{N \\times P}$, we need to compute\n",
    "the [eigendecomposition](reference.qmd#eigendecomposition)[1] of its\n",
    "[covariance](reference.qmd#auto-covariance)[2]:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(X)\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})\\\\\n",
    "    &= V \\Lambda V^\\top\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "To do this, we start by computing the covariance of our data matrix,\n",
    "where $X$ is centered (i.e. $X - \\overline{X}$)\n",
    "\n",
    "![](https://github.com/BonnerLab/ccn-tutorial/blob/main/resources/autocovariance.svg?raw=true)\n",
    "\n",
    "Next, we compute the eigendecomposition of the covariance:\n",
    "\n",
    "<span class=\"column-margin\">The computational steps we take to\n",
    "diagonalize the covariance are slightly different, as is described later\n",
    "in the notebook.</span>\n",
    "\n",
    "![](https://github.com/BonnerLab/ccn-tutorial/blob/main/resources/eigendecomposition.svg?raw=true)\n",
    "\n",
    "The columns of $V$ are *eigenvectors* that specify the directions of\n",
    "variance while the corresponding diagonal elements of $\\Lambda$ are\n",
    "*eigenvalues* that specify the amount of variance along the\n",
    "eigenvector[3].\n",
    "\n",
    "Finally, the original data matrix can be transformed by projecting it\n",
    "onto the eigenvectors:\n",
    "$\\widetilde{X} = \\left(X - \\overline{X}\\right) V$.\n",
    "\n",
    "![](https://github.com/BonnerLab/ccn-tutorial/blob/main/resources/projection.svg?raw=true)\n",
    "\n",
    "### A computational recipe\n",
    "\n",
    "[1] The eigendecomposition of a symmetric matrix\n",
    "$X \\in \\mathbb{R}^{n \\times n}$ involves rewriting it as the product of\n",
    "three matrices $X = V \\Lambda V^\\top$, where $V \\in \\mathbb{n \\times n}$\n",
    "is orthonormal and $\\Lambda \\in \\mathbb{n \\times n}$ is diagonal with\n",
    "non-negative entries.\n",
    "\n",
    "[2] Given a data matrix $X \\in \\mathbb{R}^{n \\times f}$ containing\n",
    "neural responses to $n$ stimuli from $f$ neurons, the *auto-covariance*\n",
    "of $X$ (or simply its *covariance*) is defined as:\n",
    "\n",
    "$$\\text{cov}(X) = \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})$$\n",
    "\n",
    "This is an $f \\times f$ matrix where the $(i, j)$-th element measures\n",
    "how much neuron $i$ covaries with neuron $j$. If the covariance is\n",
    "positive, they tend to have similar activation: a stimulus that\n",
    "activates one neuron will tend to activate the other. If the covariance\n",
    "is negative, the neurons will have dissimilar activation: a stimulus\n",
    "that activates one neuron will likely not activate the other.\n",
    "\n",
    "[3] Let’s compute the covariance of the projected data $\\widetilde{X}$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(\\widetilde{X})\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\widetilde{X}^\\top \\widetilde{X}\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\left((X - \\overline{X})V\\right)^\\top \\left((X - \\overline{X})V\\right)\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) V^\\top (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left( V \\Lambda V^\\top \\right) V\\\\\n",
    "    &= I \\Lambda I\\\\\n",
    "    &= \\Lambda\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f6637fc0317cbc4b3551b5e176f41f71321b6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean: np.ndarray\n",
    "        self.eigenvectors: np.ndarray\n",
    "        self.eigenvalues: np.ndarray\n",
    "\n",
    "    def fit(self, /, data: np.ndarray) -> None:\n",
    "        self.mean = data.mean(axis=-2)\n",
    "\n",
    "        data_centered = data - self.mean\n",
    "        _, s, v_t = np.linalg.svd(data_centered)\n",
    "\n",
    "        n_stimuli = data.shape[-2]\n",
    "\n",
    "        self.eigenvectors = np.swapaxes(v_t, -1, -2)\n",
    "        self.eigenvalues = s**2 / (n_stimuli - 1)\n",
    "\n",
    "    def transform(self, /, data: np.ndarray) -> np.ndarray:\n",
    "        return (data - self.mean) @ self.eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cec30d48b3f70d48cb7817982d9cc63a698aa8-0",
   "metadata": {},
   "source": [
    "> **Why do we compute PCA this way instead of\n",
    "> $\\text{eig}(\\text{cov}(X))$?**\n",
    ">\n",
    "> To apply PCA to a data matrix, we might be tempted to use the\n",
    "> definition and naively compute its\n",
    "> [covariance](reference.qmd#auto-covariance) followed by an\n",
    "> [eigendecomposition](reference.qmd#eigendecomposition). However, when\n",
    "> the number of neurons $P$ is large, this approach is memory-intensive\n",
    "> and prone to numerical errors.\n",
    ">\n",
    "> Instead, we can use the [singular value\n",
    "> decomposition](reference.qmd#singular-value-decomposition) (SVD) of\n",
    "> $X$ to efficiently compute its PCA transformation. Specifically,\n",
    "> $X = U \\Sigma V^\\top$ is a singular value decomposition, where $U$ and\n",
    "> $V$ are orthonormal and $\\Sigma$ is diagonal.\n",
    ">\n",
    "> The covariance matrix reduces to\n",
    "> $X^\\top X / (n - 1) = V \\left(\\frac{\\Sigma^2}{n - 1} \\right) V^\\top$,\n",
    "> which is exactly the eigendecomposition required.\n",
    ">\n",
    "> Specifically, the eigenvalues $\\lambda_i$ of the covariance matrix are\n",
    "> related to the singular values $\\sigma_i$ of the data matrix as\n",
    "> $\\lambda_i = \\sigma_i^2 / (N - 1)$, while the eigenvectors of the\n",
    "> covariance matrix are exactly the right singular vectors $V$ of the\n",
    "> data matrix $X$.\n",
    "\n",
    "> **Only need the first few PCs?**\n",
    ">\n",
    "> Check out [truncated\n",
    "> SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)!\n",
    "\n",
    "## Transforming the dataset\n",
    "\n",
    "Let’s now project our data onto its principal components and analyze it\n",
    "in this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-transforming-the-dataset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(data: xr.DataArray) -> xr.Dataset:\n",
    "    pca = PCA()\n",
    "    pca.fit(data.values)\n",
    "\n",
    "    data_transformed = pca.transform(data.values)\n",
    "\n",
    "    return xr.Dataset(\n",
    "        data_vars={\n",
    "            \"score\": xr.DataArray(\n",
    "                data=data_transformed,\n",
    "                dims=(\"stimulus\", \"component\"),\n",
    "            ),\n",
    "            \"eigenvector\": xr.DataArray(\n",
    "                data=pca.eigenvectors,\n",
    "                dims=(\"component\", \"neuron\"),\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"rank\": (\"component\", 1 + np.arange(data_transformed.shape[-1])),\n",
    "            \"eigenvalue\": (\"component\", pca.eigenvalues),\n",
    "        }\n",
    "        | {coord: (data[coord].dims[0], data[coord].values) for coord in data.coords},\n",
    "    )\n",
    "\n",
    "\n",
    "pca = compute_pca(data)\n",
    "display(pca[\"score\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ca2a05e1048eed4e462f5f208e95ab21f95a0-0",
   "metadata": {},
   "source": [
    "### Inspecting the eigenvectors\n",
    "\n",
    "The eigenvectors represent the directions of variance in our data,\n",
    "sorted in descending order of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0156a9849542a7bd65968651964d6f4775dad5-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.set_options(display_expand_data=True):\n",
    "    display(pca[\"eigenvector\"])\n",
    "\n",
    "\n",
    "fig = view_joint_scatter(\n",
    "    data,\n",
    "    coord=\"neuron responses\",\n",
    "    dim=\"neuron\",\n",
    "    template_func=lambda x: f\"neuron {x} response\",\n",
    ")\n",
    "\n",
    "ax = fig.get_axes()[0]\n",
    "ax.axline(\n",
    "    data.mean(\"stimulus\").values,\n",
    "    slope=-pca[\"eigenvector\"].values[0, 1] / pca[\"eigenvector\"].values[0, 0],\n",
    "    c=\"k\",\n",
    "    lw=3,\n",
    "    label=\"PC 1\",\n",
    ")\n",
    "ax.axline(\n",
    "    data.mean(\"stimulus\").values,\n",
    "    slope=-pca[\"eigenvector\"].values[1, 1] / pca[\"eigenvector\"].values[1, 0],\n",
    "    c=\"gray\",\n",
    "    lw=2,\n",
    "    label=\"PC 2\",\n",
    ")\n",
    "ax.legend()\n",
    "display(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dcc9cf17c3100088a4804f97f62bda0b08065-0",
   "metadata": {},
   "source": [
    "### Interpreting the transformed data\n",
    "\n",
    "We can view the data projected onto each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-view_principal_components",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_individual_scatter(\n",
    "    pca[\"score\"],\n",
    "    coord=\"rank\",\n",
    "    dim=\"component\",\n",
    "    template_func=lambda x: f\"principal component {x}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856f08d89399a0bc5a3274eeabfca21cc8919fc1-0",
   "metadata": {},
   "source": [
    "We observe that:\n",
    "\n",
    "-   the *first* principal component is largely driven by the *size* of\n",
    "    the stimulus\n",
    "-   the *second* principal component is largely driven by the *color* of\n",
    "    the stimulus\n",
    "\n",
    "> **Important**\n",
    ">\n",
    "> Note that these components do *not* directly correspond to either of\n",
    "> the latent variables. Rather, each is a mixture of stimulus-dependent\n",
    "> signal and noise.\n",
    "\n",
    "### Inspecting the eigenspectrum\n",
    "\n",
    "The eigenvalues show the variance along each eigenvector. The total\n",
    "variance along all the eigenvectors is the sum of the eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6dcbf335c29593d34f7b0e29810b0ce622374d-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca[\"eigenvalue\"].round(3)\n",
    "for i_neuron in range(eigenvalues.sizes[\"component\"]):\n",
    "    print(\n",
    "        f\"variance along eigenvector {i_neuron + 1} (eigenvalue {i_neuron + 1}):\"\n",
    "        f\" {eigenvalues[i_neuron].values}\"\n",
    "    )\n",
    "print(f\"total variance: {eigenvalues.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0479036b68be268294bd7041792f8c9507fece-0",
   "metadata": {},
   "source": [
    "We can also see that this is equal to the total variance in the original\n",
    "data, since the PCA transformation corresponds to a simple rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481bfb05e0b0bc3fda8ace1c0567d8b37a52816-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = data.var(dim=\"stimulus\", ddof=1).round(3).rename(\"neuron variances\")\n",
    "for i_neuron in range(variances.sizes[\"neuron\"]):\n",
    "    print(f\"variance of neuron {i_neuron + 1} responses: {variances[i_neuron].values}\")\n",
    "print(f\"total variance: {variances.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963d87ff52dd6490cc1239a21d42628aaa926136-0",
   "metadata": {},
   "source": [
    "We can plot the eigenvalues as a function of their rank to visualize the\n",
    "*eigenspectrum*. As we will see shortly, the eigenspectrum provides\n",
    "valuable insights about the latent dimensionality of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_eigenspectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_eigenspectrum(pca: xr.DataArray) -> Figure:\n",
    "    fig, ax = plt.subplots(figsize=(pca.sizes[\"component\"], 5))\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=pca[\"component\"].to_dataframe(),\n",
    "        x=\"rank\",\n",
    "        y=\"eigenvalue\",\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax.set_xticks(pca[\"rank\"].values)\n",
    "    ax.set_ylim(bottom=0)\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_eigenspectrum(pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5742cddd51203f9bba157217e260fb6f433f98a9-0",
   "metadata": {},
   "source": [
    "## Quantifying dimensionality\n",
    "\n",
    "In these data, the dimensionality is clear: there are two latent\n",
    "variables and both are evident in the principal components. However, in\n",
    "real data, we typically record from more than $P = 2$ neurons;\n",
    "therefore, judging the dimensionality becomes tricky. To simulate such a\n",
    "scenario, let’s record from more neurons (say $P = 10$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-simulate_more_neurons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate_random_neuron(rng: np.random.Generator) -> Neuron:\n",
    "    return Neuron(\n",
    "        beta_color=rng.integers(-10, 11),\n",
    "        beta_size=rng.integers(-10, 11),\n",
    "        std=rng.integers(-10, 11),\n",
    "        mean=rng.integers(-10, 11),\n",
    "    )\n",
    "\n",
    "\n",
    "neurons = tuple([_simulate_random_neuron(rng) for _ in range(10)])\n",
    "\n",
    "big_data = simulate_multiple_neuron_responses(\n",
    "    stimuli=stimuli,\n",
    "    neurons=neurons,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "display(big_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c4451c4b1c528c7e5c3f66f290fae7ebfee55-0",
   "metadata": {},
   "source": [
    "As before, we can visualize each principal component and plot the\n",
    "eigenspectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-view_principal_components_big",
   "metadata": {},
   "outputs": [],
   "source": [
    "big_pca = compute_pca(big_data)\n",
    "\n",
    "view_individual_scatter(\n",
    "    big_pca[\"score\"],\n",
    "    coord=\"rank\",\n",
    "    dim=\"component\",\n",
    "    template_func=lambda x: f\"principal component {x}\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-visualize_eigenspectrum_big",
   "metadata": {},
   "outputs": [],
   "source": [
    "view_eigenspectrum(big_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7fe89f72261dfd7bff78525699b9ae77f11ed9-0",
   "metadata": {},
   "source": [
    "We know by design that this data was generated from *2* latent variables\n",
    "– *color* and *size*. However, in real datasets with naturalistic\n",
    "stimuli, we often don’t know what the latent variables are! It’s common\n",
    "to use the eigenspectrum to estimate the latent dimensionality of the\n",
    "data. For instance, inspecting the eigenspectrum of our toy example\n",
    "tells us that the first two dimensions have much higher variance than\n",
    "the rest. We refer to these as the *effective dimensions*.\n",
    "\n",
    "In general, there are several approaches for estimating dimensionality\n",
    "based on the eigenspectrum:\n",
    "\n",
    "#### Rank of the covariance matrix\n",
    "\n",
    "The *rank* of the covariance matrix – equal to the number of *nonzero*\n",
    "eigenvalues – would be the latent dimensionality in the ideal setting\n",
    "where the data has zero noise. In real data, the rank is typically equal\n",
    "to the ambient dimensionality (which here is the number of neurons we\n",
    "record from), since there is typically some variance along every\n",
    "dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27378e0118625b70d733ef4139e32efbfecfcbc6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"rank = {(big_pca.eigenvalue > 0).sum().values}\")\n",
    "\n",
    "\n",
    "def view_thresholded_eigenspectrum(pca: PCA, *, threshold: int | float) -> Figure:\n",
    "    fig = view_eigenspectrum(pca)\n",
    "    ax = fig.get_axes()[0]\n",
    "\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.set_ylabel(\"proportion of variance\")\n",
    "    ax_twin.axhline(threshold, ls=\"--\", c=\"gray\")\n",
    "    ax_twin.yaxis.set_major_formatter(ticker.PercentFormatter(1))\n",
    "\n",
    "    total_variance = pca[\"eigenvalue\"].sum().values\n",
    "    ylim /= total_variance\n",
    "    ax_twin.set_ylim(ylim)\n",
    "\n",
    "    ax_twin.fill_between(x=xlim, y1=ylim[-1], y2=threshold, color=\"green\", alpha=0.1)\n",
    "    ax_twin.fill_between(x=xlim, y1=ylim[0], y2=threshold, color=\"red\", alpha=0.1)\n",
    "\n",
    "    sns.despine(ax=ax_twin, offset=20, left=True, bottom=True, right=False, top=True)\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_thresholded_eigenspectrum(big_pca, threshold=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54f4c06364093347f6aa69c6c1e8eea4e9610b-0",
   "metadata": {},
   "source": [
    "#### Setting an arbitrary *cumulative* variance threshold\n",
    "\n",
    "A very commonly used method is to set a threshold based on the\n",
    "cumulative variance of the data: the number of dimensions required to\n",
    "exceed, say $90\\%$ of the variance, is taken as the latent\n",
    "dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edef080243aed4b1d0df4c97c5a82359d9967f83-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_cumulative_eigenspectrum(\n",
    "    pca: xr.DataArray, *, threshold: float = 0.9\n",
    ") -> Figure:\n",
    "    fig, ax = plt.subplots(figsize=(pca.sizes[\"component\"], 5))\n",
    "\n",
    "    data = pca[\"eigenvalue\"].copy()\n",
    "    total_variance = data.sum().values\n",
    "\n",
    "    data[\"eigenvalue\"] = data.cumsum()\n",
    "    data = data.rename({\"eigenvalue\": \"cumulative variance\"})\n",
    "    data[\"cumulative proportion of variance\"] = (\n",
    "        data[\"cumulative variance\"] / pca[\"eigenvalue\"].sum()\n",
    "    )\n",
    "\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=data.to_dataframe(),\n",
    "        x=\"rank\",\n",
    "        y=\"cumulative variance\",\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax.set_xticks(pca[\"rank\"].values)\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    ylim /= total_variance\n",
    "\n",
    "    ax_twin = ax.twinx()\n",
    "    ax_twin.set_ylabel(\"cumulative proportion of variance\")\n",
    "    ax_twin.set_ylim(ylim)\n",
    "\n",
    "    ax_twin.axhline(threshold, ls=\"--\", c=\"gray\")\n",
    "\n",
    "    ax_twin.fill_between(x=xlim, y1=ylim[-1], y2=threshold, color=\"red\", alpha=0.1)\n",
    "    ax_twin.fill_between(x=xlim, y1=ylim[0], y2=threshold, color=\"green\", alpha=0.1)\n",
    "\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "    sns.despine(ax=ax_twin, offset=20, left=True, bottom=True, right=False, top=True)\n",
    "    ax_twin.yaxis.set_major_formatter(ticker.PercentFormatter(1))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_cumulative_eigenspectrum(big_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad1ed4f3965e4cb7b47615ee6bbdad912180038-0",
   "metadata": {},
   "source": [
    "#### Eyeballing the knee of the spectrum\n",
    "\n",
    "When the number of latent dimensions is low, eigenspectra often have a\n",
    "sharp discontinuity (the\n",
    "[“knee”](https://en.wikipedia.org/wiki/Knee_of_a_curve), or “elbow”),\n",
    "where a small number of dimensions have high-variance and the remainder\n",
    "have much have lower variance. The latent dimensionality is then taken\n",
    "to be the number of dimensions above this threshold determined by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ca9093d1e46e6c673bdcb1c08818d0c3bbeab7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_eigenspectrum_knee(pca: PCA, *, knee: int) -> Figure:\n",
    "    fig = view_eigenspectrum(pca)\n",
    "    ax = fig.get_axes()[0]\n",
    "    ax.plot(\n",
    "        knee,\n",
    "        big_pca[\"eigenvalue\"].isel({\"component\": big_pca[\"rank\"] == knee}).values,\n",
    "        \"o\",\n",
    "        ms=30,\n",
    "        mec=\"r\",\n",
    "        mfc=\"none\",\n",
    "        mew=3,\n",
    "    )\n",
    "\n",
    "    ax.axvline(knee, ls=\"--\", c=\"gray\")\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "\n",
    "    ax.fill_betweenx(y=ylim, x1=xlim[-1], x2=knee, color=\"red\", alpha=0.1)\n",
    "    ax.fill_betweenx(y=ylim, x1=xlim[0], x2=knee, color=\"green\", alpha=0.1)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "view_eigenspectrum_knee(big_pca, knee=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b385a9b68c26037985deb0dd8e8219feb6b2d-0",
   "metadata": {},
   "source": [
    "So far, all of the techniques discussed depend on a paramter that needs\n",
    "to be decided by the researcher. For instance, a cumulative threshold of\n",
    "90% is an arbitrary choice, and if we decided to choose 95% instead, we\n",
    "would get a different number of dimensions. This is why sometimes a\n",
    "parameter-free estimate is preferred.\n",
    "\n",
    "#### Computing a summary statistic over the entire spectrum\n",
    "\n",
    "A metric such as *effective dimensionality* summarizes the spectrum\n",
    "using an entropy-like measure, taking into account variances along all\n",
    "the dimensions:\n",
    "\n",
    "$$d_\\text{eff}(\\lambda_1, \\dots \\lambda_n) = \\dfrac{\\left( \\sum_{i=1}^n \\lambda_i \\right)^2}{\\sum_{i=1}^n \\lambda_i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a404b7b7562accdebf23c3201cad68b0d57ee4a3-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_effective_dimensionality(eigenspectrum: np.ndarray) -> float:\n",
    "    return (np.sum(eigenspectrum) ** 2) / (eigenspectrum**2).sum()\n",
    "\n",
    "\n",
    "def view_effective_dimensionality_examples(pca: PCA) -> Figure:\n",
    "    n_components = pca.sizes[\"component\"]\n",
    "\n",
    "    spectrum_1 = [float(pca[\"eigenvalue\"].mean().values)] * n_components\n",
    "\n",
    "    weights = 1 / (1 + np.arange(n_components, dtype=np.float32))\n",
    "    weights /= weights.sum()\n",
    "\n",
    "    spectrum_2 = list(weights * pca[\"eigenvalue\"].sum().values)\n",
    "    spectrum_3 = [float(pca[\"eigenvalue\"].sum().values)] + [0] * (n_components - 1)\n",
    "\n",
    "    data = pd.DataFrame(\n",
    "        {\n",
    "            \"eigenvalue\": spectrum_1 + spectrum_2 + spectrum_3,\n",
    "            \"rank\": list(np.tile(pca[\"rank\"].values, 3)),\n",
    "            \"example\": [0] * n_components + [1] * n_components + [2] * n_components,\n",
    "        }\n",
    "    )\n",
    "    # return data\n",
    "    g = sns.relplot(\n",
    "        kind=\"line\",\n",
    "        data=data,\n",
    "        col=\"example\",\n",
    "        x=\"rank\",\n",
    "        y=\"eigenvalue\",\n",
    "        marker=\"o\",\n",
    "        height=3,\n",
    "        aspect=1,\n",
    "        facet_kws={\n",
    "            \"sharey\": False,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    for i_spectrum, ax in enumerate(g.axes.flat):\n",
    "        d = compute_effective_dimensionality(\n",
    "            data.loc[data[\"example\"] == i_spectrum, \"eigenvalue\"]\n",
    "        )\n",
    "        ax.set_title(\"$d_{eff}$\" + f\" = {d.round(2)}\")\n",
    "        ax.set_xticks([1, 10])\n",
    "\n",
    "    sns.despine(g.figure, offset=10)\n",
    "    g.figure.tight_layout(w_pad=2)\n",
    "    plt.close(g.figure)\n",
    "    return g.figure\n",
    "\n",
    "\n",
    "print(\n",
    "    \"effective dimensionality =\"\n",
    "    f\" {compute_effective_dimensionality(big_pca['eigenvalue']).values.round(2)}\"\n",
    ")\n",
    "\n",
    "view_effective_dimensionality_examples(big_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50d781fa39969ee92369a35d8eefad81b0ecdd3-0",
   "metadata": {},
   "source": [
    "## Takeaways and further thoughts\n",
    "\n",
    "-   PCA is a useful tool for studying the neural code and estimating the\n",
    "    latent dimensionality of representations.\n",
    "-   However, PCA focuses on how much variance is present along each\n",
    "    dimension, and not the usefulness of the dimensions.\n",
    "-   Here we used a toy example with idealized data, where the high\n",
    "    variance dimensions are meaningful and the low variance ones are\n",
    "    noise.\n",
    "-   This may not always be the case, we could instead have high variance\n",
    "    noise dimensions, and low variance meaningful dimensions.\n",
    "-   Later in the tutorial, we will discuss more powerful techniques that\n",
    "    can handle such problems. But before that, let’s analyze a real\n",
    "    neuroscience dataset.\n",
    "\n",
    "## Additional notes\n",
    "\n",
    "### Preprocessing the data\n",
    "\n",
    "Before PCA, it’s often recommended to preprocess the data by Z-scoring\n",
    "each of the input features $X$ – ensuring that they have zero mean and\n",
    "unit variance:\n",
    "\n",
    "$$Z = \\dfrac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "> **When and why should we standardize the data?**\n",
    ">\n",
    "> Often, PCA is applied to data where the features are fundamentally\n",
    "> different from each other. For example, we might have a dataset where\n",
    "> the features of interest are the prices of cars (in dollars) and their\n",
    "> masses (in kilograms). Since these two features have different units,\n",
    "> the variances of the features are not directly comparable – there’s no\n",
    "> obvious way to numerically compare a variance of\n",
    "> (\\$20,000)<sup>2</sup> in price and a variance of (1,000\n",
    "> kg)<sup>2</sup> in mass. Even if the features being compared are all\n",
    "> the same, if they are in different units – say euros, dollars, and\n",
    "> cents – the raw variances of the data matrix are meaningless.\n",
    ">\n",
    "> Since PCA implicitly assumes that the variances along each dimension\n",
    "> are comparable, we can Z-score each of the features before applying\n",
    "> PCA to ensure that they are on a common scale.\n",
    ">\n",
    "> Note, however, that this transformation reduces the information in the\n",
    "> system – it is possible that the variances of the features are\n",
    "> informative.\n",
    "\n",
    "## Further reading\n",
    "\n",
    "1.  [This StackOverflow\n",
    "    answer](https://stats.stackexchange.com/a/134283) is a great\n",
    "    explanation for how [singular value\n",
    "    decomposition](reference.qmd#singular-value-decomposition) is\n",
    "    related to principal component analysis.\n",
    "2.  [This tutorial](https://doi.org/10.1080/00273171.2020.1743631) by\n",
    "    Giudice (2020) on effective dimensionality provides a great overview\n",
    "    of different notions of dimensionality and metrics to quantify it.\n",
    "\n",
    "Giudice, Marco Del. 2020. “Effective Dimensionality: A Tutorial.”\n",
    "*Multivariate Behavioral Research* 56 (3): 527–42.\n",
    "<https://doi.org/10.1080/00273171.2020.1743631>.\n",
    "\n",
    "Kriegeskorte, Nikolaus. 2008. “Representational Similarity Analysis\n",
    "Connecting the Branches of Systems Neuroscience.” *Frontiers in Systems\n",
    "Neuroscience*. <https://doi.org/10.3389/neuro.06.004.2008>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
