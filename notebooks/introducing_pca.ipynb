{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8732cd2e-7aaa-4063-920f-10db6dafb78f",
   "metadata": {},
   "source": [
    "# An introduction to PCA\n",
    "\n",
    "Using principal component analysis to reveal latent structure in data\n",
    "\n",
    "PCA is often used to reduce the dimensionality of large data while\n",
    "preserving a significant amount of variance. More fundamentally, it is a\n",
    "framework for studying the covariance statistics of data. In this\n",
    "section, we will introduce the concept of PCA with some toy examples.\n",
    "\n",
    "## A simple experiment\n",
    "\n",
    "Let’s perform an imaginary neuroscience experiment! We’ll record\n",
    "voltages from $P = 2$ neurons in visual cortex while the participant\n",
    "passively views $N = 500$ dots of different *colors* and *sizes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-package",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/BonnerLab/ccn-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import warnings\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defaults",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"white\",\n",
    "    palette=\"deep\",\n",
    ")\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "\n",
    "xr.set_options(display_max_rows=3, display_expand_data=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rng",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "rng = np.random.default_rng(seed=random_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b4ff24-d8b8-4455-9b7b-a6c5071e679f",
   "metadata": {},
   "source": [
    "### Creating the stimuli\n",
    "\n",
    "Let’s create dots of different *colors* and *sizes*, ensuring that these\n",
    "two latent variables are [uncorrelated](reference.qmd#correlation))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-stimuli",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stimuli(\n",
    "    *,\n",
    "    n_stimuli: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> pd.DataFrame:\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"color\": rng.random(size=(n_stimuli,)),\n",
    "            \"size\": rng.random(size=(n_stimuli,)),\n",
    "        }\n",
    "    ).set_index(1 + np.arange(n_stimuli))\n",
    "\n",
    "\n",
    "def view_stimuli(data: pd.DataFrame) -> None:\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data,\n",
    "        x=\"color\",\n",
    "        y=\"size\",\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        palette=\"flare\",\n",
    "        legend=False,\n",
    "    )\n",
    "    sns.despine(ax=ax, trim=True)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "stimuli = create_stimuli(n_stimuli=500, rng=rng)\n",
    "\n",
    "view_stimuli(stimuli)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f27e6e5-cc27-4ff4-bdb0-d779a6ef491e",
   "metadata": {},
   "source": [
    "### Simulating neural responses\n",
    "\n",
    "Now, we need to decide how the neurons might respond to these dots. Each\n",
    "neuron could respond to either one or both of the latent features that\n",
    "defines these stimuli. The neuronal responses could also be subject to\n",
    "noise. Hence, we model each neuron’s response as a simple linear\n",
    "combination of the two latent features - color and size - and noise:\n",
    "\n",
    "$r_{\\text{neuron}} = \\beta_{\\text{color}} \\left( \\text{color} \\right) + \\beta_{\\text{size}} \\left( \\text{size} \\right) + \\epsilon$,\n",
    "where\n",
    "$\\epsilon \\sim \\mathcal{N}(\\mu_{\\text{neuron}}, \\sigma_{\\text{neuron}}^2)$\n",
    "\n",
    "This procedure produces a data matrix $X \\in \\mathbb{R}^{N \\times P}$\n",
    "containing the $P = 2$ neurons’ responses to the $N = 500$ stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-neuron-responses-simulate",
   "metadata": {},
   "outputs": [],
   "source": [
    "Neuron = NamedTuple(\n",
    "    \"Neuron\",\n",
    "    beta_color=float,\n",
    "    beta_size=float,\n",
    "    mean=float,\n",
    "    std=float,\n",
    ")\n",
    "\n",
    "\n",
    "def simulate_neuron_responses(\n",
    "    stimuli: pd.DataFrame,\n",
    "    neuron: Neuron,\n",
    "    *,\n",
    "    rng: np.random.Generator,\n",
    ") -> np.ndarray:\n",
    "    def z_score(x: np.ndarray) -> np.ndarray:\n",
    "        return (x - x.mean()) / x.std()\n",
    "\n",
    "    return (\n",
    "        neuron.beta_color * z_score(stimuli[\"color\"])\n",
    "        + neuron.beta_size * z_score(stimuli[\"size\"])\n",
    "        + neuron.std * rng.standard_normal(size=(len(stimuli),))\n",
    "        + neuron.mean\n",
    "    )\n",
    "\n",
    "\n",
    "def simulate_multiple_neuron_responses(\n",
    "    *,\n",
    "    stimuli: pd.DataFrame,\n",
    "    neurons: Sequence[Neuron],\n",
    "    rng: np.random.Generator,\n",
    ") -> xr.DataArray:\n",
    "    data = []\n",
    "    for i_neuron, neuron in enumerate(neurons):\n",
    "        data.append(\n",
    "            xr.DataArray(\n",
    "                data=simulate_neuron_responses(\n",
    "                    stimuli=stimuli,\n",
    "                    neuron=neuron,\n",
    "                    rng=rng,\n",
    "                ),\n",
    "                dims=(\"stimulus\",),\n",
    "                coords={\n",
    "                    column: (\"stimulus\", values)\n",
    "                    for column, values in stimuli.reset_index(names=\"stimulus\").items()\n",
    "                },\n",
    "            )\n",
    "            .expand_dims({\"neuron\": [i_neuron + 1]})\n",
    "            .assign_coords(\n",
    "                {\n",
    "                    field: (\"neuron\", [float(value)])\n",
    "                    for field, value in neuron._asdict().items()\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "\n",
    "    data = (\n",
    "        xr.concat(data, dim=\"neuron\")\n",
    "        .rename(\"neuron responses\")\n",
    "        .transpose(\"stimulus\", \"neuron\")\n",
    "    )\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "neurons = (\n",
    "    Neuron(beta_color=3, beta_size=-2, std=1, mean=7),\n",
    "    Neuron(beta_color=-2, beta_size=5, std=3, mean=-6),\n",
    ")\n",
    "\n",
    "data = simulate_multiple_neuron_responses(\n",
    "    stimuli=stimuli,\n",
    "    neurons=neurons,\n",
    "    rng=rng,\n",
    ")\n",
    "\n",
    "display(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e51974-c40a-4fad-b93a-175a4e0deb35",
   "metadata": {},
   "source": [
    "We can visualize the responses of each neuron to each dot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-neuron-responses-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_neuron_responses(data: xr.DataArray) -> None:\n",
    "    rng = np.random.default_rng()\n",
    "    data_ = data.assign_coords(\n",
    "        {\"arbitrary\": (\"stimulus\", rng.random(data.sizes[\"stimulus\"]))}\n",
    "    )\n",
    "    min_, max_ = data_.min(), data_.max()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=data.sizes[\"neuron\"], figsize=(5, 4))\n",
    "\n",
    "    for neuron, ax in zip(data[\"neuron\"].values, axes.flat):\n",
    "        sns.scatterplot(\n",
    "            ax=ax,\n",
    "            data=(\n",
    "                data_.sel(neuron=neuron)\n",
    "                .rename(f\"neuron {neuron} response (a.u.)\")\n",
    "                .to_dataframe()\n",
    "            ),\n",
    "            x=f\"neuron {neuron} response (a.u.)\",\n",
    "            y=\"arbitrary\",\n",
    "            hue=\"color\",\n",
    "            size=\"size\",\n",
    "            palette=\"flare\",\n",
    "            legend=False,\n",
    "        )\n",
    "        sns.despine(ax=ax, left=True, offset=10)\n",
    "\n",
    "        ax.set_xlim([min_, max_])\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout(h_pad=4)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "view_neuron_responses(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997ffa0-a624-46a1-9594-0a206ad94e00",
   "metadata": {},
   "source": [
    "We can also compute the variance in each neuron. The total variance in\n",
    "the data is the sum of their variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = data.var(dim=\"stimulus\", ddof=1).round(3).rename(\"neuron variances\")\n",
    "for i_neuron in range(variances.sizes[\"neuron\"]):\n",
    "    print(f\"variance of neuron {i_neuron + 1} responses: {variances[i_neuron].values}\")\n",
    "print(f\"total variance: {variances.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15c91ec-b6ae-4946-99d0-69844bc8a40b",
   "metadata": {},
   "source": [
    "Since we only have $P=2$ neurons, we can visualize this data as a\n",
    "scatterplot, which makes their [covariance](reference.qmd#covariance)\n",
    "apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-joint-response",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_joint_neuron_responses(\n",
    "    data: xr.DataArray, *, ax: mpl.axes.Axes = None\n",
    ") -> None:\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    else:\n",
    "        fig = ax.get_figure()\n",
    "\n",
    "    data_ = pd.DataFrame(\n",
    "        {coord: data[coord].values for coord in (\"color\", \"size\")}\n",
    "        | {\n",
    "            f\"neuron {neuron} response (a.u.)\": data.sel(neuron=neuron).to_dataframe()[\n",
    "                \"neuron responses\"\n",
    "            ]\n",
    "            for neuron in (1, 2)\n",
    "        }\n",
    "    )\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data_,\n",
    "        x=\"neuron 1 response (a.u.)\",\n",
    "        y=\"neuron 2 response (a.u.)\",\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        legend=False,\n",
    "        palette=\"flare\",\n",
    "    )\n",
    "    ax.axhline(0, c=\"gray\", ls=\"--\")\n",
    "    ax.axvline(0, c=\"gray\", ls=\"--\")\n",
    "    ax.axis(\"square\")\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "view_joint_neuron_responses(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covariance",
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = (\n",
    "    xr.cov(\n",
    "        data.sel(neuron=1),\n",
    "        data.sel(neuron=2),\n",
    "        dim=\"stimulus\",\n",
    "    )\n",
    "    .round(3)\n",
    "    .values\n",
    ")\n",
    "\n",
    "print(f\"covariance between neurons: {covariance}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080b71-d921-4f32-9f77-d920b3c7a2c1",
   "metadata": {},
   "source": [
    "## Analyzing the data with PCA\n",
    "\n",
    "We have a population of neurons that contain information about the\n",
    "stimulus: that is, from the activity pattern we recorded, we expect to\n",
    "be able to reliably decode the color and size of the dot presented. How\n",
    "is this information encoded in the population activity? Is there a\n",
    "neuron that is sensitive to color and another that is sensitive to size?\n",
    "Or are there neural responses more complicated than this? If so, is\n",
    "there another view of the population code that might be more\n",
    "informative?\n",
    "\n",
    "### Some geometric intuition\n",
    "\n",
    "### The mathematical definition\n",
    "\n",
    "Given a data matrix $X \\in \\mathbb{R}^{N \\times P}$, we need to compute\n",
    "the [eigendecomposition](reference.qmd#eigendecomposition)[1] of its\n",
    "[auto-covariance](reference.qmd#auto-covariance)[2]:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{cov}(X)\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})\\\\\n",
    "    &= V \\Lambda V^\\top\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The columns of $V$ are *eigenvectors* that specify the directions of\n",
    "variance while the corresponding diagonal elements of $\\Lambda$ are\n",
    "*eigenvalues* that specify the amount of variance along the\n",
    "eigenvector[3].\n",
    "\n",
    "The original data matrix can be transformed by projecting it onto the\n",
    "eigenvectors: $\\tilde{X} = \\left(X - \\overline{X}\\right) V$.\n",
    "\n",
    "> **Viewing PCA as an optimization**\n",
    ">\n",
    "> PCA can be used to project data into a lower-dimensional space\n",
    "> (i.e. $p \\le f$) in a way that best preserves the geometry of the\n",
    "> data. Specifically, computing a PCA decomposition of $X$ yields a\n",
    "> matrix $V \\in \\mathbb{R}^{f \\times p}$ such that\n",
    "> $V = \\argmin_{V \\in \\mathbb{U_{f \\times p}}} \\sum_{i=1}^n \\left|| x_i - VV^\\top x_i \\right||_2$,\n",
    "> where $||\\cdot||_2$ denotes the $L_2$-norm and\n",
    "> $\\mathbb{U_{f \\times p}}$ denotes the set of orthonormal matrices with\n",
    "> shape $f \\times p$.\n",
    "\n",
    "### Transforming the data\n",
    "\n",
    "[1] The eigendecomposition of a symmetric matrix\n",
    "$X \\in \\mathbb{R}^{n \\times n}$ involves rewriting it as the product of\n",
    "three matrices $X = V \\Lambda V^\\top$, where $V \\in \\mathbb{n \\times n}$\n",
    "is orthonormal and $\\Lambda \\in \\mathbb{n \\times n}$ is diagonal with\n",
    "non-negative entries.\n",
    "\n",
    "[2] Given a data matrix $X \\in \\mathbb{R}^{n \\times f}$ containing\n",
    "neural responses to $n$ stimuli from $f$ neurons, the *auto-covariance*\n",
    "of $X$ (or simply its *covariance*) is defined as:\n",
    "\n",
    "$\\text{cov}(X) = \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})$\n",
    "\n",
    "This is an $f \\times f$ matrix where the $(i, j)$-th element measures\n",
    "how much neuron $i$ covaries with neuron $j$. If the covariance is\n",
    "positive, they tend to have similar activation: a stimulus that\n",
    "activates one neuron will tend to activate the other. If the covariance\n",
    "is negative, the neurons will have dissimilar activation: a stimulus\n",
    "that activates one neuron will likely not activate the other.\n",
    "\n",
    "[3] Let’s compute the auto-covariance of the projected data $\\tilde{X}$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\text{cov}(\\tilde{X})\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\tilde{X}^\\top \\tilde{X}\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) \\left((X - \\overline{X})V\\right)^\\top \\left((X - \\overline{X})V\\right)\\\\\n",
    "    &= \\left(\\dfrac{1}{n - 1}\\right) V^\\top (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})V\\\\\n",
    "    &= V^\\top \\left( V \\Lambda V^\\top \\right) V\\\\\n",
    "    &= I \\Lambda I\\\\\n",
    "    &= \\Lambda\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recipe-pca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCA:\n",
    "    def __init__(self) -> None:\n",
    "        self.mean: np.ndarray\n",
    "        self.eigenvectors: np.ndarray\n",
    "        self.eigenvalues: np.ndarray\n",
    "\n",
    "    def fit(self, /, data: np.ndarray) -> None:\n",
    "        self.mean = data.mean(axis=-2)\n",
    "\n",
    "        data_centered = data - self.mean\n",
    "        _, s, v_t = np.linalg.svd(data_centered)\n",
    "\n",
    "        n_stimuli = data.shape[-2]\n",
    "\n",
    "        self.eigenvectors = np.swapaxes(v_t, -1, -2)\n",
    "        self.eigenvalues = s**2 / (n_stimuli - 1)\n",
    "\n",
    "    def transform(self, /, data: np.ndarray) -> np.ndarray:\n",
    "        return (data - self.mean) @ self.eigenvectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006459b-c4e1-4dae-8675-78af88dc1a88",
   "metadata": {},
   "source": [
    "> **Why do we compute PCA this way instead of\n",
    "> $\\text{eig}(\\text{cov}(X))$?**\n",
    ">\n",
    "> To apply PCA to a data matrix, we might be tempted to use the\n",
    "> definition and naively compute its\n",
    "> [auto-covariance](reference.qmd#auto-covariance) followed by an\n",
    "> [eigendecomposition](reference.qmd#eigendecomposition). However, when\n",
    "> the number of neurons $P$ is large, this approach is memory-intensive\n",
    "> and prone to numerical errors.\n",
    ">\n",
    "> Instead, we can use the [singular value\n",
    "> decomposition](reference.qmd#singular-value-decomposition) (SVD) of\n",
    "> $X$ to efficiently compute its PCA transformation. Specifically,\n",
    "> $X = U \\Sigma V^\\top$ is a singular value decomposition, where $U$ and\n",
    "> $V$ are orthonormal and $\\Sigma$ is diagonal.\n",
    ">\n",
    "> The auto-covariance matrix reduces to\n",
    "> $X^\\top X / (n - 1) = V \\left(\\frac{\\Sigma^2}{n - 1} \\right) V^\\top$,\n",
    "> which is exactly the eigendecomposition required.\n",
    ">\n",
    "> Specifically, the eigenvalues $\\lambda_i$ of the auto-covariance\n",
    "> matrix are related to the singular values $\\sigma_i$ of the data\n",
    "> matrix as $\\lambda_i = \\sigma_i^2 / (N - 1)$, while the eigenvectors\n",
    "> of the auto-covariance matrix are exactly the right singular vectors\n",
    "> $V$ of the data matrix $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pca-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(data: xr.DataArray) -> xr.Dataset:\n",
    "    pca = PCA()\n",
    "    pca.fit(data.values)\n",
    "\n",
    "    data_transformed = pca.transform(data.values)\n",
    "\n",
    "    return xr.Dataset(\n",
    "        data_vars={\n",
    "            \"score\": xr.DataArray(\n",
    "                data=data_transformed,\n",
    "                dims=(\"stimulus\", \"component\"),\n",
    "            ),\n",
    "            \"eigenvector\": xr.DataArray(\n",
    "                data=pca.eigenvectors,\n",
    "                dims=(\"component\", \"neuron\"),\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"rank\": (\"component\", 1 + np.arange(data_transformed.shape[-1])),\n",
    "            \"eigenvalue\": (\"component\", pca.eigenvalues),\n",
    "        }\n",
    "        | {coord: (data[coord].dims[0], data[coord].values) for coord in data.coords},\n",
    "    )\n",
    "\n",
    "\n",
    "def view_eigenspectrum(pca: xr.DataArray) -> None:\n",
    "    fig, ax = plt.subplots(figsize=(2, 5))\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=pca[\"component\"].to_dataframe(),\n",
    "        x=\"rank\",\n",
    "        y=\"eigenvalue\",\n",
    "        marker=\"s\",\n",
    "    )\n",
    "    ax.set_title(\"eigenspectrum\")\n",
    "    ax.set_xticks([1, 2])\n",
    "    ax.set_ylim(bottom=0)\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def view_transformed_data(pca: xr.DataArray) -> None:\n",
    "    data_ = pd.DataFrame(\n",
    "        {coord: pca[coord].values for coord in (\"color\", \"size\")}\n",
    "        | {\n",
    "            f\"principal component {rank}\": (\n",
    "                pca[\"score\"].sel(component=rank - 1).to_dataframe()[\"score\"]\n",
    "            )\n",
    "            for rank in (1, 2)\n",
    "        }\n",
    "    )\n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        ax=ax,\n",
    "        data=data_,\n",
    "        x=\"principal component 1\",\n",
    "        y=\"principal component 2\",\n",
    "        hue=\"color\",\n",
    "        size=\"size\",\n",
    "        palette=\"flare\",\n",
    "        legend=False,\n",
    "    )\n",
    "    ax.axis(\"square\")\n",
    "    ax.axhline(0, c=\"gray\", ls=\"--\")\n",
    "    ax.axvline(0, c=\"gray\", ls=\"--\")\n",
    "    sns.despine(ax=ax, offset=20)\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "data_transformed = compute_pca(data)\n",
    "view_transformed_data(data_transformed)\n",
    "view_eigenspectrum(data_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-pcs-visualize",
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_principal_components(data_transformed: xr.DataArray) -> None:\n",
    "    rng = np.random.default_rng()\n",
    "    data_ = data_transformed.assign_coords(\n",
    "        {\"arbitrary\": (\"stimulus\", rng.random(data_transformed.sizes[\"stimulus\"]))}\n",
    "    )\n",
    "    min_, max_ = data_transformed.min(), data_transformed.max()\n",
    "\n",
    "    fig, axes = plt.subplots(nrows=data_transformed.sizes[\"neuron\"], figsize=(5, 4))\n",
    "\n",
    "    for neuron, ax in zip(data[\"neuron\"].values, axes.flat):\n",
    "        sns.scatterplot(\n",
    "            ax=ax,\n",
    "            data=(\n",
    "                data_.sel(neuron=neuron)\n",
    "                .rename(f\"neuron {neuron} response (a.u.)\")\n",
    "                .to_dataframe()\n",
    "            ),\n",
    "            x=f\"neuron {neuron} response (a.u.)\",\n",
    "            y=\"arbitrary\",\n",
    "            hue=\"color\",\n",
    "            size=\"size\",\n",
    "            palette=\"flare\",\n",
    "            legend=False,\n",
    "        )\n",
    "        sns.despine(ax=ax, left=True, offset=10)\n",
    "\n",
    "        ax.set_xlim([min_, max_])\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    fig.tight_layout(h_pad=4)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "view_neuron_responses(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a66c87-38c7-4874-90f4-6f90f4754ad3",
   "metadata": {},
   "source": [
    "We can compute the variance along each eigenvector. The total variance\n",
    "along all eigenvectors is the same as the total variance of the original\n",
    "data.\n",
    "\n",
    "Next, let’s consider a case where we are recording from a larger number\n",
    "of neurons. In this case, the dimensionality of our features space is\n",
    "10. This number is usually referred to as the *ambient dimensionality*\n",
    "of the data. However, we already know that this data contains only 2\n",
    "useful dimensions (color and size). We refer to these as the *effective\n",
    "dimensions*. In this example, the remaining dimensions, which correspond\n",
    "to noise, have relatively low variance, and we can see a signficant drop\n",
    "in the eigenspectrum after the second eigenvector. However, keep in mind\n",
    "that this is a toy example with idealized data. As we will see, when\n",
    "using standard PCA on real data it may be impossible to identify a clear\n",
    "distinction between meaningful dimensions and noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eigenvalues",
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = data_transformed[\"eigenvalue\"].round(3)\n",
    "for i_neuron in range(eigenvalues.sizes[\"component\"]):\n",
    "    print(\n",
    "        f\"variance along eigenvector {i_neuron + 1} (eigenvalue {i_neuron + 1}):\"\n",
    "        f\" {eigenvalues[i_neuron].values}\"\n",
    "    )\n",
    "print(f\"total variance: {eigenvalues.sum().values}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ee3138-64a5-4181-8a90-b1efe1a9d9b7",
   "metadata": {},
   "source": [
    "### Reducing the dimensionality\n",
    "\n",
    "## Power laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6406402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_data_with_specified_spectrum(\n",
    "    *,\n",
    "    n_stimuli: int,\n",
    "    n_features: int,\n",
    "    eigenspectrum: np.ndarray,\n",
    "    n_components: int = None,\n",
    "    seed: int = 0,\n",
    "    mean: np.ndarray = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulate a data matrix while specifying the eigenvalues of its auto-covariance.\n",
    "\n",
    "    We achieve this using the singular value decomposition. First, we generate two random orthonormal matrices `u` and `v` using the QR decompositions of randomly initialized matrices. Then, we compute the singular value spectrum of the\n",
    "    \"\"\"\n",
    "    if n_components is None:\n",
    "        n_components = min(n_stimuli, n_features)\n",
    "\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    x = rng.standard_normal((n_stimuli, n_components))\n",
    "    y = rng.standard_normal((n_features, n_components))\n",
    "    u, _ = np.linalg.qr(x)\n",
    "    v, _ = np.linalg.qr(y)\n",
    "\n",
    "    singular_values = np.sqrt(eigenspectrum * (n_stimuli - 1))\n",
    "\n",
    "    if mean is None:\n",
    "        mean = np.zeros(n_features)\n",
    "\n",
    "    return u @ np.diag(singular_values) @ v.transpose() + mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb29d9ae-0703-4313-a0d5-94430049c03f",
   "metadata": {},
   "source": [
    "> **Only need the first few PCs?**\n",
    ">\n",
    "> Check out [truncated\n",
    "> SVD](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
