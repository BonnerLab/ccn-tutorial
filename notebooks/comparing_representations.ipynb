{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfdd9effe7b3e4f989b80ceb684104b66d87aa22-0",
   "metadata": {},
   "source": [
    "# Comparing neural representations\n",
    "\n",
    "Generalizing cross-validated PCA to pairs of systems using\n",
    "cross-decomposition\n",
    "\n",
    "Especially after the advent of high-dimensional neural network models of\n",
    "the brain, there has been an explosion of methods to compare\n",
    "high-dimensional representations, including various forms of linear\n",
    "regression, canonical correlation analysis (CCA), centered kernel\n",
    "alignment (CKA), and non-linear methods too! In this part of the\n",
    "tutorial, we’ll describe *cross-decomposition* – a method closely\n",
    "related to cross-validated PCA – that allows us to measure the\n",
    "similarity between two high-dimensional systems.\n",
    "\n",
    "## Cross-decomposition\n",
    "\n",
    "Just as PCA identifies the principal directions of variance of a system,\n",
    "*cross-decomposition* identifies the principal directions of *shared*\n",
    "variance between *two* systems $X$ and $Y$. Specifically, just as PCA\n",
    "computes the eigendecomposition of the *auto-*covariance,\n",
    "cross-decomposition computes the singular value decomposition of the\n",
    "*cross-*covariance:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(X, Y)\n",
    "    &= X^\\top Y / (n - 1)\\\\\n",
    "    &= U \\Sigma V^\\top\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "<span class=\"column-margin\">Note that $X$ and $Y$ have been centered, as\n",
    "usual.</span>\n",
    "\n",
    "Here,\n",
    "\n",
    "-   the left singular vectors $U$ define a rotation of the system $X$\n",
    "    into a latent space\n",
    "-   the right singular vectors $V$ define a rotation of system $Y$ into\n",
    "    the *same* latent space, and\n",
    "-   the singular values $\\Sigma$ define the amount of variance shared by\n",
    "    the two systems along the latent dimensions.\n",
    "\n",
    "![](https://github.com/BonnerLab/ccn-tutorial/blob/main/resources/cross-covariance-svd.svg?raw=true)\n",
    "\n",
    "> **A note on terminology**\n",
    ">\n",
    "> The cross-decomposition method we describe here is more specifically\n",
    "> known as [Partial Least Squares Singular Value Decomposition\n",
    "> (PLS-SVD)](https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSSVD.html).\n",
    "> We simplify it to “cross-decomposition” since we will be developing a\n",
    "> cross-validated version of the typical estimators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716327115344ecefe9e785605ab1f75e382cadde-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/BonnerLab/ccn-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caff3a72f59b994709a615861373d45dcabdc29-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from IPython.display import display\n",
    "\n",
    "from utilities.brain import (\n",
    "    load_dataset,\n",
    "    average_data_across_repetitions,\n",
    "    load_stimuli,\n",
    ")\n",
    "from utilities.computation import svd, assign_logarithmic_bins\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf00b08e74464463d4cfec60b6e479b74bb0b6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"white\",\n",
    "    palette=\"deep\",\n",
    "    rc={\"legend.edgecolor\": \"None\"},\n",
    ")\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "\n",
    "xr.set_options(display_max_rows=3, display_expand_data=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81958efb039da8a9d6d3dcfc190f98e5a010f2c7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "rng = np.random.default_rng(seed=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ff769e79c31bdf307f64c77bcbbb10fea0ea50-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSSVD:\n",
    "    def __init__(self) -> None:\n",
    "        self.left_mean: np.ndarray\n",
    "        self.right_mean: np.ndarray\n",
    "        self.left_singular_vectors: np.ndarray\n",
    "        self.right_singular_vectors: np.ndarray\n",
    "\n",
    "    def fit(self, /, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.left_mean = x.mean(axis=-2)\n",
    "        self.right_mean = y.mean(axis=-2)\n",
    "\n",
    "        x_centered = x - self.left_mean\n",
    "        y_centered = y - self.right_mean\n",
    "\n",
    "        n_stimuli = x.shape[-2]\n",
    "\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        cross_covariance = (np.swapaxes(x_centered, -1, -2) @ y_centered) / (\n",
    "            n_stimuli - 1\n",
    "        )\n",
    "\n",
    "        (\n",
    "            self.left_singular_vectors,\n",
    "            self.singular_values,\n",
    "            self.right_singular_vectors,\n",
    "        ) = svd(\n",
    "            torch.from_numpy(cross_covariance).to(device),\n",
    "            n_components=min([*x.shape, *y.shape]),\n",
    "            truncated=True,\n",
    "            seed=random_state,\n",
    "        )\n",
    "\n",
    "        self.left_singular_vectors = self.left_singular_vectors.cpu().numpy()\n",
    "        self.singular_values = self.singular_values.cpu().numpy()\n",
    "        self.right_singular_vectors = self.right_singular_vectors.cpu().numpy()\n",
    "\n",
    "    def transform(self, /, z: np.ndarray, *, direction: str) -> np.ndarray:\n",
    "        match direction:\n",
    "            case \"left\":\n",
    "                return (z - self.left_mean) @ self.left_singular_vectors\n",
    "            case \"right\":\n",
    "                return (z - self.right_mean) @ self.right_singular_vectors\n",
    "            case _:\n",
    "                raise ValueError(\"direction must be 'left' or 'right'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe3c1fc41c7a7a0f8750cf88dd775eb20c9eecb-0",
   "metadata": {},
   "source": [
    "## Comparing brains\n",
    "\n",
    "In the same way that we can *cross-validated PCA* to estimate the shared\n",
    "variance across presentations of the same stimuli *within* a\n",
    "participant, we can use *cross-decomposition* to estimate the shared\n",
    "variance in the neural representations of the same stimuli *across*\n",
    "participants.\n",
    "\n",
    "We have two data matrices $X \\in \\mathbb{R}^{N \\times P_X}$ and\n",
    "$Y \\in \\mathbb{R}^{N \\times P_Y}$ from two participants, containing\n",
    "neural responses to the same $N$ stimuli. Note that the number of\n",
    "neurons or voxels in the two subjects ($P_X$ and $P_Y$) can be different\n",
    "– we don’t need to assume any sort of anatomical alignment between\n",
    "brains.\n",
    "\n",
    "In cross-validated PCA, we measured stimulus-specific variance based on\n",
    "cross-trial generalization. Here, even if we don’t have different\n",
    "repetitions of the same stimulus, we could use an analogous\n",
    "cross-validation approach. Instead of testing generalization across\n",
    "different *presentations* of the stimuli, we can evaluate the reliable\n",
    "shared variance between the two systems across *stimuli*.\n",
    "\n",
    "Specifically, we can divide the images into two: a training split and a\n",
    "test split. We can compute singular vectors on the training split, and\n",
    "evalute *test* singular values on the test split:\n",
    "\n",
    "### Step 1 – Compute singular vectors on a training set\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{cov}(X_\\text{train}, Y_\\text{train})\n",
    "    &= X_\\text{train}^\\top Y_\\text{train} / (n - 1)\\\\\n",
    "    &= U \\Sigma V^\\top\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "### Step 2 – Compute cross-validated singular values on the test set\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma_\\text{cross-validated}\n",
    "    &= \\text{cov}(X_\\text{test} U, Y_\\text{test} V)\\\\\n",
    "    &= \\left( X_\\text{test} U \\right) ^\\top \\left( Y_\\text{test} V \\right) / (n - 1)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "![](https://github.com/BonnerLab/ccn-tutorial/blob/main/resources/pls_svd.svg?raw=true)\n",
    "\n",
    "### Applying it to neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0938c1cf2e853fb4b796fa84cd375e8412fad7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_1 = average_data_across_repetitions(\n",
    "    load_dataset(subject=0, roi=\"general\")\n",
    ").sortby(\"stimulus_id\")\n",
    "subject_2 = average_data_across_repetitions(load_dataset(subject=1, roi=\"general\"))\n",
    "\n",
    "stimuli = load_stimuli()[\"stimulus_id\"].values\n",
    "\n",
    "display(subject_1)\n",
    "display(subject_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec035e20789b3eb7ecaf591411d71f3ecc58c809-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_participant_spectrum(\n",
    "    x: xr.DataArray,\n",
    "    y: xr.DataArray,\n",
    "    /,\n",
    "    train_fraction: float = 7 / 8,\n",
    ") -> np.ndarray:\n",
    "    stimuli = x[\"stimulus_id\"].values\n",
    "    n_train = int(train_fraction * len(stimuli))\n",
    "    stimuli = rng.permutation(stimuli)[:n_train]\n",
    "\n",
    "    train_indices = np.isin(x[\"stimulus_id\"].values, stimuli)\n",
    "\n",
    "    x_train = x.isel({\"presentation\": train_indices})\n",
    "    y_train = y.isel({\"presentation\": train_indices})\n",
    "\n",
    "    x_test = x.isel({\"presentation\": ~train_indices})\n",
    "    y_test = y.isel({\"presentation\": ~train_indices})\n",
    "\n",
    "    scorer = PLSSVD()\n",
    "    scorer.fit(x_train.values, y_train.values)\n",
    "    x_test_transformed = scorer.transform(x_test.values, direction=\"left\")\n",
    "    y_test_transformed = scorer.transform(y_test.values, direction=\"right\")\n",
    "\n",
    "    return np.diag(\n",
    "        np.cov(\n",
    "            x_test_transformed,\n",
    "            y_test_transformed,\n",
    "            rowvar=False,\n",
    "        )[:n_train, n_train:]\n",
    "    )\n",
    "\n",
    "\n",
    "cross_participant_spectrum = compute_cross_participant_spectrum(subject_1, subject_2)\n",
    "\n",
    "data = pd.DataFrame(\n",
    "    {\n",
    "        \"cross-validated singular value\": cross_participant_spectrum,\n",
    "        \"rank\": assign_logarithmic_bins(\n",
    "            1 + np.arange(len(cross_participant_spectrum)),\n",
    "            min_=1,\n",
    "            max_=10_000,\n",
    "            points_per_bin=5,\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    ax=ax,\n",
    "    data=data.assign(arbitrary=0),\n",
    "    x=\"rank\",\n",
    "    y=\"cross-validated singular value\",\n",
    "    marker=\"o\",\n",
    "    dashes=False,\n",
    "    hue=\"arbitrary\",\n",
    "    palette=[\"#cf6016\"],\n",
    "    ls=\"None\",\n",
    "    err_style=\"bars\",\n",
    "    estimator=\"mean\",\n",
    "    errorbar=\"sd\",\n",
    "    legend=False,\n",
    ")\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_aspect(\"equal\", \"box\")\n",
    "sns.despine(ax=ax, offset=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d76619e950b10ad3c97d1a7b701d70e1bc7bf-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_within_participant_spectrum(data: xr.DataArray) -> np.ndarray:\n",
    "    x_train = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n",
    "    y_train = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n",
    "\n",
    "    x_test = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n",
    "    y_test = data.isel({\"presentation\": data[\"rep_id\"] == 1}).sortby(\"stimulus_id\")\n",
    "\n",
    "    scorer = PLSSVD()\n",
    "    scorer.fit(x_train.values, x_train.values)\n",
    "    x_test_transformed = scorer.transform(x_test.values, direction=\"left\")\n",
    "    y_test_transformed = scorer.transform(y_test.values, direction=\"right\")\n",
    "\n",
    "    n_components = x_test_transformed.shape[-1]\n",
    "\n",
    "    return np.diag(\n",
    "        np.cov(\n",
    "            x_test_transformed,\n",
    "            y_test_transformed,\n",
    "            rowvar=False,\n",
    "        )[:n_components, n_components:]\n",
    "    )\n",
    "\n",
    "\n",
    "within_participant_spectrum = compute_within_participant_spectrum(\n",
    "    load_dataset(subject=0, roi=\"general\")\n",
    ")\n",
    "\n",
    "data = pd.concat(\n",
    "    [\n",
    "        data.assign(comparison=\"cross-individual\"),\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"cross-validated singular value\": within_participant_spectrum,\n",
    "                \"rank\": assign_logarithmic_bins(\n",
    "                    1 + np.arange(len(within_participant_spectrum)),\n",
    "                    min_=1,\n",
    "                    max_=10_000,\n",
    "                    points_per_bin=5,\n",
    "                ),\n",
    "            }\n",
    "        ).assign(comparison=\"within-individual\"),\n",
    "    ],\n",
    "    axis=0,\n",
    ")\n",
    "\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    sns.lineplot(\n",
    "        ax=ax,\n",
    "        data=data,\n",
    "        x=\"rank\",\n",
    "        y=\"cross-validated singular value\",\n",
    "        palette=[\"#514587\", \"#cf6016\"],\n",
    "        hue=\"comparison\",\n",
    "        hue_order=[\"within-individual\", \"cross-individual\"],\n",
    "        style=\"comparison\",\n",
    "        style_order=[\"within-individual\", \"cross-individual\"],\n",
    "        markers=[\"o\", \"s\"],\n",
    "        dashes=False,\n",
    "        ls=\"None\",\n",
    "        err_style=\"bars\",\n",
    "        estimator=\"mean\",\n",
    "        errorbar=\"sd\",\n",
    "    )\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_aspect(\"equal\", \"box\")\n",
    "\n",
    "    ax.grid(True, which=\"minor\", c=\"whitesmoke\")\n",
    "    ax.grid(True, which=\"major\", c=\"lightgray\")\n",
    "    for loc in (\"left\", \"bottom\", \"top\", \"right\"):\n",
    "        ax.spines[loc].set_visible(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72e6a7c8a84b42c6f4becaf13b35ecff8b9a63-0",
   "metadata": {},
   "source": [
    "> **Cross-decomposition is flexible – it supports generalization across\n",
    "> trials, images, and/or individuals!**\n",
    ">\n",
    "> The cross-validated cross-decomposition approach we describe here\n",
    "> allows many possible levels of generalization.\n",
    ">\n",
    "> Cross-trial generalization  \n",
    "> At the most basic level, we can test the generalization of the latent\n",
    "> dimensions across different trials – repetitions of the same stimuli.\n",
    "> This approach identifies stimulus-specific variance that generalizes\n",
    "> over trial-specific noise – and is what we used above for the\n",
    "> within-participant comparison above (cross-validated PCA).\n",
    ">\n",
    "> Cross-image generalization  \n",
    "> Next, we could test the generalization of the latent dimensions across\n",
    "> different stimulus sets – how consistent the directions of covariance\n",
    "> across different datasets. This was used to evaluate the\n",
    "> cross-participant comparison above.\n",
    ">\n",
    "> Cross-individual generalization  \n",
    "> Finally, we could examine the variance shared between two different\n",
    "> high-dimensional systems – here, we compared neural responses from two\n",
    "> different subjects.\n",
    ">\n",
    "> In fact, we could combine several of these to get very strict\n",
    "> generalization criteria: we could even estimate the spectrum of\n",
    "> variance that generalizes across trials, stimuli, and individuals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
