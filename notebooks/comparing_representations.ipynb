{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b89bcce80573ba6506f7aff869c47a1d97e045-0",
   "metadata": {},
   "source": [
    "# Comparing high-dimensional representations\n",
    "\n",
    "Cross-validated methods to separate high-dimensional signal from noise\n",
    "\n",
    "Especially after the advent of high-dimensional neural network models of\n",
    "the brain, there has been an explosion of methods to compare\n",
    "high-dimensional representations, including various forms of linear\n",
    "regression, canonical correlation analysis (CCA), centered kernel\n",
    "alignment (CKA), and non-linear methods too! In this part of the\n",
    "tutorial, we’ll describe PLS-SVD – a method closely related to PCA –\n",
    "that allows us to measure the similarity between two high-dimensional\n",
    "systems in a manner that is relatively interpretable and extensible for\n",
    "various purposes.\n",
    "\n",
    "## PLS-SVD\n",
    "\n",
    "Just as PCA identifies the principal directions of variance of a system,\n",
    "PLS-SVD identifies the principal directions of *shared* variance between\n",
    "*two* systems. Specifically, just as PCA computes the eigendecomposition\n",
    "of the auto-covariance, PLS-SVD computes the singular value\n",
    "decomposition of the cross-covariance:\n",
    "\n",
    "$X^\\top Y = U \\Sigma V^\\top$.\n",
    "\n",
    "Here, the left singular vectors $U$ define a rotation of the system $X$\n",
    "into some latent space, the right singular vectors $V$ define a rotation\n",
    "of system $Y$ into the same latent space, and the singular values\n",
    "$\\Sigma$\n",
    "\n",
    "> **What happens when $X$ = $Y$?**\n",
    ">\n",
    "> Note that if $X = Y$, PLS-SVD reduces to PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716327115344ecefe9e785605ab1f75e382cadde-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/BonnerLab/ccn-tutorial.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d61ccedbe27904eb75aad7edac256810df8bbd6-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from IPython.display import display\n",
    "\n",
    "from utilities.brain import (\n",
    "    load_dataset,\n",
    "    average_data_across_repetitions,\n",
    "    load_stimuli,\n",
    "    plot_brain_map,\n",
    ")\n",
    "from utilities.computation import svd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d7be547b34d5bf28cd6cbfa700f707e29d410-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "sns.set_theme(\n",
    "    context=\"notebook\",\n",
    "    style=\"white\",\n",
    "    palette=\"deep\",\n",
    ")\n",
    "set_matplotlib_formats(\"svg\")\n",
    "\n",
    "pd.set_option(\"display.max_rows\", 5)\n",
    "pd.set_option(\"display.max_columns\", 10)\n",
    "pd.set_option(\"display.precision\", 3)\n",
    "pd.set_option(\"display.show_dimensions\", False)\n",
    "\n",
    "xr.set_options(display_max_rows=3, display_expand_data=False)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81958efb039da8a9d6d3dcfc190f98e5a010f2c7-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 0\n",
    "rng = np.random.default_rng(seed=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d84a61db46b18243d532f083dbbb3a327455db4-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLSSVD:\n",
    "    def __init__(self) -> None:\n",
    "        self.left_mean: np.ndarray\n",
    "        self.right_mean: np.ndarray\n",
    "        self.left_singular_vectors: np.ndarray\n",
    "        self.right_singular_vectors: np.ndarray\n",
    "\n",
    "    def fit(self, /, x: np.ndarray, y: np.ndarray) -> None:\n",
    "        self.left_mean = x.mean(axis=-2)\n",
    "        self.right_mean = y.mean(axis=-2)\n",
    "\n",
    "        x_centered = x - self.left_mean\n",
    "        y_centered = y - self.right_mean\n",
    "\n",
    "        n_stimuli = x.shape[-2]\n",
    "\n",
    "        cross_covariance = (np.swapaxes(x_centered, -1, -2) @ y_centered) / (\n",
    "            n_stimuli - 1\n",
    "        )\n",
    "\n",
    "        (\n",
    "            self.left_singular_vectors,\n",
    "            self.singular_values,\n",
    "            self.right_singular_vectors,\n",
    "        ) = svd(\n",
    "            torch.from_numpy(cross_covariance),\n",
    "            n_components=min([*x.shape, *y.shape]),\n",
    "            truncated=True,\n",
    "            seed=random_state,\n",
    "        )\n",
    "\n",
    "        n_stimuli = data.shape[-2]\n",
    "\n",
    "        self.left_singular_vectors = self.left_singular_vectors.cpu().numpy()\n",
    "        self.singular_values = self.singular_values.cpu().numpy()\n",
    "        self.right_singular_vectors = self.right_singular_vectors.cpu().numpy()\n",
    "\n",
    "    def transform(self, /, z: np.ndarray, *, direction: str) -> np.ndarray:\n",
    "        match direction:\n",
    "            case \"left\":\n",
    "                return (z - self.left_mean) @ self.left_singular_vectors\n",
    "            case \"right\":\n",
    "                return (z - self.right_mean) @ self.right_singular_vectors\n",
    "            case _:\n",
    "                raise ValueError(\"direction must be 'left' or 'right'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e9a831b4cc9576415e96d9a2c3023a056b3a30-0",
   "metadata": {},
   "source": [
    "## Comparing brains\n",
    "\n",
    "In the same way that we can PLS-SVD to estimate the shared variance\n",
    "across presentations of the same stimuli *within* a participant, we can\n",
    "also use it to estimate the shared variance in the neural\n",
    "representations of the same stimuli *across* participants.\n",
    "\n",
    "We have two data matrices $X \\in \\mathbb{R}^{N \\times P_X}$ and\n",
    "$Y \\in \\mathbb{R}^{N \\times P_Y}$ from two participants. We could\n",
    "directly compute the singular values of their cross-covariance, which\n",
    "would be a direct estimate of the shared variance between these two\n",
    "representations.\n",
    "\n",
    "However, we run into the same issue as before: the singular values of a\n",
    "matrix are always positive and we won’t be able to use the magnitude of\n",
    "the singular value to assess the reliability of the variance along that\n",
    "dimension.\n",
    "\n",
    "Instead, we can use a cross-validated approach similar to CV-PCA, except\n",
    "that instead of testing generalization across different *presentations*\n",
    "of the stimuli, we can evaluate the reliable shared variance between the\n",
    "two representations across *stimuli*.\n",
    "\n",
    "Specifically, we can divide the images into two: a training split and a\n",
    "test split. We can compute singular vectors on the training split, and\n",
    "evalute *test* singular values on the test split, analogous to the\n",
    "CV-PCA procedure:\n",
    "\n",
    "$X_\\text{train}^\\top Y_\\text{train} / (n - 1) = U \\Sigma V^\\top$\n",
    "\n",
    "$\\Sigma_\\text{test} = \\left( X_\\text{test} U \\right) ^\\top \\left( Y_\\text{test} V \\right) / (n - 1)$\n",
    "\n",
    "## Comparing brains and DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafb4568393df50a6c37e5594b91b213a4c50ac9-0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject_1 = average_data_across_repetitions(load_dataset(subject=0, roi=\"general\"))\n",
    "# subject_2 = average_data_across_repetitions(load_dataset(subject=1, roi=\"general\"))\n",
    "# subject_1 = subject_1.sortby(\"stimulus_id\")\n",
    "# subject_2 = subject_2.sortby(\"stimulus_id\")\n",
    "\n",
    "# display(subject_1)\n",
    "# display(subject_2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
