[
  {
    "objectID": "pages/reference.html",
    "href": "pages/reference.html",
    "title": "Reference",
    "section": "",
    "text": "A scalar is a single real number.\nExamples:\n\n0\n1\n\\pi\n-3.5\n\n\n\n\nAn n-vector is a collection of n real numbers.\nExamples:\n\nA 1-vector: \\begin{bmatrix}5\\end{bmatrix}\nA 3-vector: \\begin{bmatrix}2\\\\-4.5\\\\\\pi\\end{bmatrix}\nAn n-vector: \\begin{bmatrix}3\\\\\\vdots\\\\-1\\end{bmatrix} (it has n elements, trust me)\nA row vector\n\n\n\n\nAn m \\times n matrix is a collection of"
  },
  {
    "objectID": "pages/reference.html#the-basics",
    "href": "pages/reference.html#the-basics",
    "title": "Reference",
    "section": "",
    "text": "A scalar is a single real number.\nExamples:\n\n0\n1\n\\pi\n-3.5\n\n\n\n\nAn n-vector is a collection of n real numbers.\nExamples:\n\nA 1-vector: \\begin{bmatrix}5\\end{bmatrix}\nA 3-vector: \\begin{bmatrix}2\\\\-4.5\\\\\\pi\\end{bmatrix}\nAn n-vector: \\begin{bmatrix}3\\\\\\vdots\\\\-1\\end{bmatrix} (it has n elements, trust me)\nA row vector\n\n\n\n\nAn m \\times n matrix is a collection of"
  },
  {
    "objectID": "pages/reference.html#matrix-decompositions",
    "href": "pages/reference.html#matrix-decompositions",
    "title": "Reference",
    "section": "Matrix decompositions",
    "text": "Matrix decompositions\n\nSingular value decomposition\nThe singular value decomposition (SVD) involves rewriting a matrix X \\in \\mathbb{R}^{m \\times n} as the product of three matrices X = U \\Sigma V^\\top, where\n\nU \\in \\mathbb{R}^{m \\times m} and V \\in \\mathbb{R}^{n \\times n} are orthonormal matrices and\n\\Sigma \\in \\mathbb{R}^{m \\times n} is zero everywhere except potentially on its main diagonal.\n\n\n\nEigendecomposition\nThe eigendecomposition of a symmetric matrix X \\in \\mathbb{R}^{n \\times n} involves rewriting it as the product of three matrices X = V \\Lambda V^\\top, where\n\nV \\in \\mathbb{n \\times n} is orthonormal and\n\\Lambda \\in \\mathbb{n \\times n} is diagonal with non-negative entries."
  },
  {
    "objectID": "pages/reference.html#covariance",
    "href": "pages/reference.html#covariance",
    "title": "Reference",
    "section": "Covariance",
    "text": "Covariance\n\nAuto-covariance\nGiven a data matrix X \\in \\mathbb{R}^{n \\times f} containing neural responses to n stimuli from f neurons, the auto-covariance of X (or simply its covariance) is defined as:\n\\text{cov}(X) = \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})\nThis is an f \\times f matrix where the (i, j)-th element measures how much neuron i covaries with neuron j. If the covariance is positive, they tend to have similar activation: a stimulus that activates one neuron will tend to activate the other. If the covariance is negative, the neurons will have dissimilar activation: a stimulus that activates one neuron will likely decrease the activity of the other.\n\n\nCross-covariance\nGiven two data matrices X \\in \\mathbb{R}^{n \\times f_X} and Y \\in \\mathbb{R}^{n \\times f_Y} containing neural responses to n stimuli from f_X and f_Y neurons respectively, the cross-covariance of X and Y is defined as:\n\\text{cov}(X, Y) = \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (Y - \\overline{Y})"
  },
  {
    "objectID": "pages/reference.html#correlation",
    "href": "pages/reference.html#correlation",
    "title": "Reference",
    "section": "Correlation",
    "text": "Correlation"
  },
  {
    "objectID": "pages/exploring_neural_data.html",
    "href": "pages/exploring_neural_data.html",
    "title": "Exploring a neural dataset",
    "section": "",
    "text": "Run this notebook interactively!\n\n\n\nHere’s a link to this notebook on Google Colab."
  },
  {
    "objectID": "pages/exploring_neural_data.html#the-natural-scenes-fmri-dataset",
    "href": "pages/exploring_neural_data.html#the-natural-scenes-fmri-dataset",
    "title": "Exploring a neural dataset",
    "section": "The Natural Scenes fMRI Dataset",
    "text": "The Natural Scenes fMRI Dataset\n\nThe natural scenes dataset (NSD) is the largest fMRI dataset on human vision, with 7T fMRI responses (1.8 mm isotropic voxels) obtained from 8 adult participants. The experiment involved a continuous recognition task while participants observed natural scene images from the Microsoft Common Objects in Context (COCO) database (Lin et al., 2014).\nLet’s load the dataset. This data contains neural responses to 700 images from ~15,000 voxels reliably modulated by the visual stimuli during the NSD experiment.\n\n\nInstall all required dependencies\n# TODO uncomment before final packaging\n# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n\n\n\n\nImport various libraries\nfrom collections import Counter\nimport copy\nfrom pathlib import Path\nimport requests\nimport typing\nimport warnings\n\nfrom loguru import logger\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom torchtext.datasets import IMDB\nfrom torchtext.data.utils import get_tokenizer\nimport nibabel as nib\nimport nilearn.plotting\nfrom PIL import Image\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA, NMF\nfrom umap import UMAP\n\nfrom IPython.display import display, SVG, HTML\nfrom matplotlib.figure import Figure\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom matplotlib import pyplot as plt\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom matplotlib.offsetbox import OffsetImage, AnnotationBbox\nimport seaborn as sns\nfrom wordcloud import WordCloud\n\nfrom utilities.brain import (\n    load_dataset,\n    average_data_across_repetitions,\n    load_stimuli,\n    plot_brain_map,\n)\n\n\n\n\nSet some visualization defaults\n%matplotlib inline\n\nsns.set_theme(\n    context=\"notebook\",\n    style=\"white\",\n    palette=\"deep\",\n    rc={\"legend.edgecolor\": \"None\"},\n)\nset_matplotlib_formats(\"svg\")\n\npd.set_option(\"display.max_rows\", 5)\npd.set_option(\"display.max_columns\", 10)\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.show_dimensions\", False)\n\nxr.set_options(display_max_rows=3, display_expand_data=False)\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nInitialize a deterministic random number generator\nrandom_state = 0\nrng = np.random.default_rng(seed=random_state)\n\n\n\n\nLoad the dataset\ndata = average_data_across_repetitions(load_dataset(subject=0, roi=\"general\"))\n\ndisplay(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'fMRI betas' (presentation: 700, neuroid: 15724)&gt;\n0.4915 0.2473 0.08592 0.05828 -0.1315 ... -0.2126 -0.6315 -0.5751 -0.5354\nCoordinates: (3/4)\n    x            (neuroid) uint8 12 12 12 12 12 12 12 ... 71 72 72 72 72 72 72\n    y            (neuroid) uint8 21 22 22 22 22 22 23 ... 34 29 29 30 30 30 31\n    ...           ...\n    stimulus_id  (presentation) object 'image02950' ... 'image72948'\nDimensions without coordinates: presentation, neuroid\nAttributes: (3/8)\n    resolution:      1pt8mm\n    preprocessing:   fithrf_GLMdenoise_RR\n    ...              ...\n    postprocessing:  averaged across first two repetitionsxarray.DataArray'fMRI betas'presentation: 700neuroid: 157240.4915 0.2473 0.08592 0.05828 ... -0.2126 -0.6315 -0.5751 -0.5354array([[ 0.4915219 ,  0.24733381,  0.08592446, ..., -0.366651  ,\n         0.30723202,  0.43520752],\n       [ 0.1664538 , -0.10728736,  0.35630295, ...,  0.8608913 ,\n         0.03464809,  0.11020081],\n       [ 1.0357349 ,  0.77598304,  0.35813144, ...,  0.2419075 ,\n         0.81557286,  0.38667244],\n       ...,\n       [-0.05812129, -0.4539395 ,  0.41060364, ...,  0.5738151 ,\n        -0.718189  , -0.638827  ],\n       [-0.00340644, -1.0050421 ,  0.7278904 , ...,  0.580743  ,\n        -0.50856245, -0.2727615 ],\n       [-1.2668517 , -1.4769105 , -0.3562023 , ..., -0.63146234,\n        -0.575121  , -0.5354325 ]], dtype=float32)Coordinates: (4)x(neuroid)uint812 12 12 12 12 ... 72 72 72 72 72array([12, 12, 12, ..., 72, 72, 72], dtype=uint8)y(neuroid)uint821 22 22 22 22 ... 29 30 30 30 31array([21, 22, 22, ..., 30, 30, 31], dtype=uint8)z(neuroid)uint847 45 46 47 48 ... 46 45 46 49 49array([47, 45, 46, ..., 46, 49, 49], dtype=uint8)stimulus_id(presentation)object'image02950' ... 'image72948'array(['image02950', 'image02990', 'image03049', 'image03077',\n       'image03146', 'image03157', 'image03164', 'image03171',\n       'image03181', 'image03386', 'image03434', 'image03449',\n       'image03489', 'image03626', 'image03682', 'image03687',\n       'image03729', 'image03809', 'image03842', 'image03847',\n       'image03856', 'image03913', 'image03951', 'image04051',\n       'image04058', 'image04129', 'image04156', 'image04249',\n       'image04423', 'image04436', 'image04667', 'image04690',\n       'image04768', 'image04786', 'image04835', 'image04892',\n       'image04930', 'image05034', 'image05106', 'image05204',\n       'image05301', 'image05338', 'image05459', 'image05542',\n       'image05583', 'image05602', 'image05714', 'image06199',\n       'image06222', 'image06431', 'image06444', 'image06489',\n       'image06514', 'image06521', 'image06558', 'image06801',\n       'image07007', 'image07039', 'image07120', 'image07207',\n       'image07366', 'image07418', 'image07480', 'image07654',\n       'image07840', 'image07859', 'image07944', 'image07948',\n       'image08006', 'image08109', 'image08204', 'image08225',\n       'image08394', 'image08415', 'image08435', 'image08465',\n       'image08509', 'image08646', 'image08807', 'image08843',\n...\n       'image64615', 'image64621', 'image64867', 'image64880',\n       'image65010', 'image65148', 'image65253', 'image65267',\n       'image65376', 'image65445', 'image65769', 'image65799',\n       'image65821', 'image65872', 'image65920', 'image65943',\n       'image66004', 'image66216', 'image66278', 'image66330',\n       'image66464', 'image66489', 'image66580', 'image66773',\n       'image66836', 'image66946', 'image66976', 'image67045',\n       'image67113', 'image67204', 'image67237', 'image67295',\n       'image67742', 'image67802', 'image67829', 'image68168',\n       'image68278', 'image68339', 'image68418', 'image68471',\n       'image68741', 'image68814', 'image68842', 'image68858',\n       'image68897', 'image69007', 'image69130', 'image69214',\n       'image69240', 'image69502', 'image69614', 'image69839',\n       'image69854', 'image70075', 'image70095', 'image70193',\n       'image70232', 'image70335', 'image70360', 'image70427',\n       'image70505', 'image71229', 'image71232', 'image71241',\n       'image71410', 'image71450', 'image71753', 'image71894',\n       'image72015', 'image72080', 'image72209', 'image72312',\n       'image72510', 'image72605', 'image72719', 'image72948'],\n      dtype=object)Indexes: (0)Attributes: (8)resolution :1pt8mmpreprocessing :fithrf_GLMdenoise_RRz_score :Trueroi :generalsubject :0brain shape :[ 81 104  83]citation :Allen, E.J., St-Yves, G., Wu, Y. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat Neurosci 25, 116-126 (2022). https://doi.org/10.1038/s41593-021-00962-xpostprocessing :averaged across first two repetitions\n\n\n\n\n\n\n\n\nSome fMRI preprocessing details\n\n\n\nWe utilized the NSD single-trial betas, preprocessed in 1.8 mm volumetric space and denoised using the GLMdenoise technique (version 3; “betas_fithrf_GLMdenoise_RR”). The betas were converted to Z-scores within each scanning session and averaged across repetitions for each stimulus.\n\n\nHere are some examples of stimuli seen by the participants.\n\n\nLoad the stimuli\ndef view_stimuli(stimuli: xr.DataArray, *, n: int = 10) -&gt; None:\n    fig = plt.figure(figsize=(12, 4))\n    image_grid = ImageGrid(\n        fig=fig,\n        rect=(1, 1, 1),\n        nrows_ncols=(1, n),\n        share_all=True,\n    )\n    for i_image in range(n):\n        image_grid[i_image].imshow(stimuli[i_image])\n        image_grid[i_image].axis(\"off\")\n    fig.show()\n\n\nstimuli = load_stimuli()\nview_stimuli(stimuli)\n\n\n\n\n\n\n\n\n\n\nThe neural covariance eigenspectrum\nNow we can apply PCA to the neural responses and plot the eigenspectrum of their covariance!\n\n\nVisualize the eigenspectrum\ndef view_eigenspectrum(pca: PCA, *, log: bool = False) -&gt; None:\n    eigenvalues = pd.DataFrame(pca.explained_variance_, columns=[\"eigenvalue\"]).assign(\n        rank=1 + np.arange(pca.n_components_)\n    )\n\n    fig, ax = plt.subplots(figsize=(6, 6))\n    sns.lineplot(\n        ax=ax,\n        data=eigenvalues.loc[eigenvalues[\"rank\"] &lt; pca.n_components_],\n        x=\"rank\",\n        y=\"eigenvalue\",\n    )\n    sns.despine(ax=ax)\n    if log:\n        ax.set_xscale(\"log\")\n        ax.set_yscale(\"log\")\n        ax.set_ylim(bottom=1)\n    fig.show()\n\n\npca = PCA()\npca.fit(data)\n\nview_eigenspectrum(pca)\n\n\n\n\n\nNeural covariance eigenspectrum with linear scaling\n\n\n\n\n\n\n\n\n\n\nVisualization of principal components\n\n\n\n\n\nThere are some simple ways to visualize and interpret the principal components.\n\n\nUtilities to view images\ndef view_images_as_scatterplot(\n    x: np.ndarray, y: np.ndarray, *, stimuli: xr.DataArray\n) -&gt; None:\n    fig, ax = plt.subplots(figsize=(10, 10))\n    for i_stimulus in range(len(stimuli)):\n        image_box = OffsetImage(stimuli[i_stimulus].values, zoom=0.3)\n        image_box.image.axes = ax\n\n        ab = AnnotationBbox(\n            image_box,\n            xy=(x[i_stimulus], y[i_stimulus]),\n            xycoords=\"data\",\n            frameon=False,\n            pad=0,\n        )\n        ax.add_artist(ab)\n\n    ax.set_xlim([x.min(), x.max()])\n    ax.set_ylim([y.min(), y.max()])\n    ax.axis(\"off\")\n    fig.show()\n\n\ndef view_images_at_poles(\n    x: np.ndarray,\n    *,\n    stimuli: xr.DataArray,\n    n_images_per_pole: int = 5,\n    label: str | None = None,\n) -&gt; None:\n    indices = np.argsort(x, axis=0)\n\n    fig = plt.figure(figsize=(12, 4))\n    image_grid = ImageGrid(\n        fig=fig,\n        rect=(1, 1, 1),\n        nrows_ncols=(1, 2 * n_images_per_pole + 1),\n        share_all=True,\n    )\n    for i_image in range(n_images_per_pole):\n        image_grid[i_image].imshow(stimuli[indices[i_image]])\n        image_grid[i_image].axis(\"off\")\n        image_grid[-i_image - 1].imshow(stimuli[indices[-i_image - 1]])\n        image_grid[-i_image - 1].axis(\"off\")\n\n    for ax in image_grid:\n        ax.axis(\"off\")\n\n    if label is not None:\n        ax = image_grid[n_images_per_pole]\n        ax.text(\n            0.5,\n            0.5,\n            label,\n            horizontalalignment=\"center\",\n            verticalalignment=\"center\",\n            transform=ax.transAxes,\n        )\n    fig.show()\n\n\nThe first method is to plot the stimuli on a scatter plot, designating their X and Y coordinates to be their scores along two principal components of interest. This allows us to observe potential clustering of the stimuli.\n\n\nProject the neural data onto the first two principal components\ndata_pca = pca.transform(data)\nview_images_as_scatterplot(data_pca[:, 0], data_pca[:, 1], stimuli=stimuli)\n\n\n\n\n\n\n\n\n\nAlternatively, we can focus on the stimuli with the highest or lowest scores along a given principal component. This provides simple clues of what this PC might be sensitive to, which could be visual features ranging from low to high complexity.\n\n\nView images that have extreme scores on the PCs\nfor rank in [1, 2, 3, 10, 50, 100]:\n    view_images_at_poles(data_pca[:, rank - 1], stimuli=stimuli, label=f\"rank {rank}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat about other methods?\n\n\n\n\n\nInterpreting PCs can be challenging especially when we rely solely on visual inspection. This difficulty arises in part because many natural features are non-negative. As a result, methods like nonnegative matrix factorization (NMF) often offer more interpretable dimensions than PCA.\n\n\nCompute NMF of neural responses\nscaler = MinMaxScaler()\nscaler.fit(data)\n\nnmf = NMF(n_components=2, random_state=random_state)\ndata_nmf = nmf.fit_transform(scaler.transform(data))\n\nview_images_as_scatterplot(data_nmf[:, 0], data_nmf[:, 1], stimuli=stimuli)\n\n\n\n\n\n\n\n\n\nSimilarly, we can inspect the stimuli with highest or closest-to-zero values along each dimension.\n\n\nView images that have extreme scores on the dimensions\nfor dim in range(2):\n    view_images_at_poles(data_pca[:, dim], stimuli=stimuli, label=f\"dim {dim+1}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNonetheless, PCA has unique benefits that shouldn’t be overlooked. For instance, PCA offers closed-form solutions and non-stochastic outcomes. They’re also well characterized mathematically. Moreover, because PCA is essentially a simple rotation of the data, it preserves all the original information in the dataset.\n\n\n\nOn this plot, it looks like that the first few PCs have substantial variance while the rest are negligible, which suggests a low-dimensional structure.\nHowever, when dealing with high-dimensional data that span several orders of magnitude, it’s more insightful to visualize it on a logarithmic scale, which makes many key statistical trends more apparent. So let’s try visualizing the spectrum on a logarithmic scale for both axes:\n\n\nVisualize the eigenspectrum on a logarithmic scale\nview_eigenspectrum(pca, log=True)\n\n\n\n\n\nNeural covariance eigenspectrum with logarithmic scaling\n\n\n\n\nOn a log-log scale, the spectrum shows a trend that looks remarkably linear, suggesting that the eigenspectrum might obey a power-law distribution:\n\n\\begin{align*}\n    \\log{\\lambda_\\text{rank}} &\\approx \\alpha \\log{\\left( \\text{rank} \\right)} + c\\\\\n    \\lambda_\\text{rank} &\\propto \\left( \\text{rank} \\right)^\\alpha\n\\end{align*}\n\nThere appears to be no obvious cut-off point in this power law, suggesting that there might be information across all ranks. The number of effective dimensions here is likely much higher than what we would have expected from simply viewing the eigenspectrum on a linear scale."
  },
  {
    "objectID": "pages/exploring_neural_data.html#power-laws",
    "href": "pages/exploring_neural_data.html#power-laws",
    "title": "Exploring a neural dataset",
    "section": "Power laws",
    "text": "Power laws\nA power law is a relationship of the form f(x) \\propto x^{\\alpha}, where \\alpha is termed the index of the power law, or the power law exponent. It suggests a scale-free structure because f(kx) \\propto f(x).\nPower laws are ubiquitious in nature, arising in all sorts of systems:\n\nword frequencies in natural language (Zipf’s law)\nwealth distribution (Pareto principle)\nferromagnetic materials near the Curie temperature (Ising model)\n\nNevertheless, a power law relationshop will not be observed when the data\n\nis random, or\nwhen it has a characteristic scale.\n\n\nAn analogy: word frequencies\nZipf’s law suggests a power-law distribution of use frequency of English words. Let’s compute the distribution of frequencies in a small corpus – a collection of IMDb movie reviews.\n\n\nGenerate word frequency distribution\ndataset = IMDB(split=\"test\")\ntokenizer = get_tokenizer(\"basic_english\", language=\"en\")\nunwanted_tokens = {\".\", \",\", \"?\", \"s\", \"t\", \"(\", \")\", \"'\", \"!\"}\n\ncounter = Counter()\nfor _, text in dataset:\n    counter.update(tokenizer(text))\n\n\n\n\nView word frequency distribution on a linear scale\ndef view_word_frequency_distribution(counter: Counter, *, log: bool = False) -&gt; Figure:\n    fig, ax = plt.subplots()\n    sns.lineplot(\n        ax=ax,\n        data=pd.DataFrame(\n            {\n                \"rank\": 1 + np.arange(len(counter)),\n                \"word frequency\": sorted(counter.values())[::-1],\n            }\n        ),\n        x=\"rank\",\n        y=\"word frequency\",\n    )\n    if log:\n        ax.set_xscale(\"log\")\n        ax.set_yscale(\"log\")\n\n    sns.despine(ax=ax, offset=20)\n    plt.close(fig)\n    return fig\n\n\nview_word_frequency_distribution(counter, log=True)\n\n\n\n\n\n\n\n\n\nIf only the high-frequency words are meaningful to human language, then we should be able to reconstruct a movie review with these words only:\n\n\nDisplay an IMDb review\nfor _, line in dataset:\n    tokens = tokenizer(line)\n    break\n\n\ndef postprocess(text: str):\n    new_text = \"\"\n\n    for i_char, char in enumerate(text):\n        if i_char != 0:\n            prev_char = text[i_char - 1]\n        else:\n            prev_char = None\n        if i_char &gt; 1:\n            prev2_char = text[i_char - 2]\n        else:\n            prev2_char = None\n        if i_char != len(text) - 1:\n            next_char = text[i_char + 1]\n        else:\n            next_char = None\n\n        if char == \"i\" and prev_char == \" \" and next_char == \" \":\n            new_text += \"I\"\n        elif char == \" \" and (\n            next_char == \".\"\n            or next_char == \",\"\n            or next_char == \"'\"\n            or prev_char == \"'\"\n            or prev_char == \"(\"\n            or next_char == \")\"\n            or next_char == \"!\"\n        ):\n            continue\n        elif prev_char == \" \" and (prev2_char == \".\" or prev2_char == \"!\"):\n            new_text += char.upper()\n        else:\n            new_text += char\n    return new_text\n\n\nprint(\"An IMDb review\")\ndisplay(HTML(f\"&lt;blockquote&gt;{postprocess(' '.join(tokens))}&lt;/blockquote&gt;\"))\n\n\nAn IMDb review\n\n\ni love sci-fi and am willing to put up with a lot. Sci-fi movies/tv are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good tv sci-fi as babylon 5 is to star trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, cg that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a'sci-fi'setting. (I'm sure there are those of you out there who think babylon 5 is good sci-fi tv. It's not. It's clichéd and uninspiring.) while us viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of earth know it's rubbish as they have to always say gene roddenberry's earth... Otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\n\n\n\n\nReconstruct the review using the top 200 words\nprint(\"Review reconstructed using top 200 words\")\ndisplay(\n    HTML(\n        (\n            \"&lt;blockquote&gt;\"\n            + postprocess(\n                \" \".join(\n                    [\n                        token\n                        for token in tokens\n                        if token in [word for word, _ in counter.most_common(200)]\n                    ]\n                )\n            )\n            + \"&lt;/blockquote&gt;\"\n        )\n    )\n)\n\n\nReview reconstructed using top 200 words\n\n\ni love and to up with a lot. Are, and. I to like this, I really did, but it is to good as is to (the).,,, that doesn't the, and characters be with a''. (I'm there are those of you out there who think is good. It's not. It's and.) while us like and character, is a that does not take (.). It, not as a. It's really to about the characters here as they are not, just a of life. Their and are and, to watch. The of know it's as they have to say's... People would not watching.'s be in their as this,, (watching it really this) of a show into.. So, off a character. And then him back as another.! All over again.\n\n\nThis poor reconstruction demonstrates that the high-rank, low-frequency words also carry meaningful information – in fact, most of it. Analogously, if we try to reconstruct neural data with just a few high-variance principal components – which is exactly what typical dimensionality reduction methods do – we will likely lose valuable information about the presented stimulus."
  },
  {
    "objectID": "pages/comparing_representations.html",
    "href": "pages/comparing_representations.html",
    "title": "Comparing neural representations",
    "section": "",
    "text": "Run this notebook interactively!\n\n\n\nHere’s a link to this notebook on Google Colab."
  },
  {
    "objectID": "pages/comparing_representations.html#cross-decomposition",
    "href": "pages/comparing_representations.html#cross-decomposition",
    "title": "Comparing neural representations",
    "section": "Cross-decomposition",
    "text": "Cross-decomposition\nJust as PCA identifies the principal directions of variance of a system, cross-decomposition identifies the principal directions of shared variance between two systems X and Y. Specifically, just as PCA computes the eigendecomposition of the auto-covariance, cross-decomposition computes the singular value decomposition of the cross-covariance:\n\n\\begin{align*}\n    \\text{cov}(X, Y)\n    &= X^\\top Y / (n - 1)\\\\\n    &= U \\Sigma V^\\top\n\\end{align*}\n\nNote that X and Y have been centered, as usual.\nHere,\n\nthe left singular vectors U define a rotation of the system X into a latent space\nthe right singular vectors V define a rotation of system Y into the same latent space, and\nthe singular values \\Sigma define the amount of variance shared by the two systems along the latent dimensions.\n\n\n\n\n\n\n\n\n\n\nA note on terminology\n\n\n\nThe cross-decomposition method we describe here is more specifically known as Partial Least Squares Singular Value Decomposition (PLS-SVD). We simplify it to “cross-decomposition” since we will be developing a cross-validated version of the typical estimators.\n\n\n\n\n\n\n\n\nWhat happens when X = Y?\n\n\n\nNote that if X = Y, cross-decomposition reduces to PCA:\n\n\n\n\nInstall all required dependencies\n# TODO uncomment before final packaging\n# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n\n\n\n\nImport various libraries\nfrom collections.abc import Sequence\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport torch\nfrom torchdata.datapipes.map import SequenceWrapper\nfrom torchvision.models import alexnet, AlexNet_Weights\nfrom sklearn.decomposition import TruncatedSVD, PCA\nfrom PIL import Image\nimport seaborn as sns\nfrom matplotlib.figure import Figure\nfrom matplotlib import pyplot as plt\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom IPython.display import display\n\nfrom utilities.brain import (\n    load_dataset,\n    average_data_across_repetitions,\n    load_stimuli,\n    plot_brain_map,\n)\nfrom utilities.computation import svd, assign_logarithmic_bins\nfrom utilities.model import (\n    extract_features,\n    SparseRandomProjection,\n    create_image_datapipe,\n)\n\n\n\n\nSet some visualization defaults\n%matplotlib inline\n\nsns.set_theme(\n    context=\"notebook\",\n    style=\"white\",\n    palette=\"deep\",\n    rc={\"legend.edgecolor\": \"None\"},\n)\nset_matplotlib_formats(\"svg\")\n\npd.set_option(\"display.max_rows\", 5)\npd.set_option(\"display.max_columns\", 10)\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.show_dimensions\", False)\n\nxr.set_options(display_max_rows=3, display_expand_data=False)\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nInitialize a deterministic random number generator\nrandom_state = 0\nrng = np.random.default_rng(seed=random_state)\n\n\n\n\nA computational recipe for PLS-SVD\nclass PLSSVD:\n    def __init__(self) -&gt; None:\n        self.left_mean: np.ndarray\n        self.right_mean: np.ndarray\n        self.left_singular_vectors: np.ndarray\n        self.right_singular_vectors: np.ndarray\n\n    def fit(self, /, x: np.ndarray, y: np.ndarray) -&gt; None:\n1        self.left_mean = x.mean(axis=-2)\n        self.right_mean = y.mean(axis=-2)\n\n        x_centered = x - self.left_mean\n        y_centered = y - self.right_mean\n\n        n_stimuli = x.shape[-2]\n\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n        cross_covariance = (np.swapaxes(x_centered, -1, -2) @ y_centered) / (\n            n_stimuli - 1\n2        )\n\n        (\n            self.left_singular_vectors,\n            self.singular_values,\n            self.right_singular_vectors,\n        ) = svd(\n            torch.from_numpy(cross_covariance).to(device),\n            n_components=min([*x.shape, *y.shape]),\n            truncated=True,\n            seed=random_state,\n3        )\n\n        self.left_singular_vectors = self.left_singular_vectors.cpu().numpy()\n        self.singular_values = self.singular_values.cpu().numpy()\n        self.right_singular_vectors = self.right_singular_vectors.cpu().numpy()\n\n    def transform(self, /, z: np.ndarray, *, direction: str) -&gt; np.ndarray:\n        match direction:\n            case \"left\":\n4                return (z - self.left_mean) @ self.left_singular_vectors\n            case \"right\":\n                return (z - self.right_mean) @ self.right_singular_vectors\n            case _:\n                raise ValueError(\"direction must be 'left' or 'right'\")\n\n\n\n1\n\nCenter the data matrices X and Y.\n\n2\n\nCompute their cross-covariance X^\\top Y / (n - 1).\n\n3\n\nCompute the singular value decomposition of the cross-covariance.\n\n4\n\nTo project data from the ambient space (X or Y) to the latent space, we must subtract the mean computed in Step 1, and multiply the data by the corresponding singular vectors."
  },
  {
    "objectID": "pages/comparing_representations.html#comparing-brains",
    "href": "pages/comparing_representations.html#comparing-brains",
    "title": "Comparing neural representations",
    "section": "Comparing brains",
    "text": "Comparing brains\nIn the same way that we can cross-validated PCA to estimate the shared variance across presentations of the same stimuli within a participant, we can use cross-decomposition to estimate the shared variance in the neural representations of the same stimuli across participants.\nWe have two data matrices X \\in \\mathbb{R}^{N \\times P_X} and Y \\in \\mathbb{R}^{N \\times P_Y} from two participants. We could directly compute the singular values of their cross-covariance, which would be a direct estimate of the shared variance between these two representations.\nHowever, we run into the same issue as before: the singular values of a matrix are always positive and we won’t be able to use the magnitude of the singular value to assess the reliability of the variance along that dimension.\nInstead, we can use a cross-validated approach similar to CV-PCA, except that instead of testing generalization across different presentations of the stimuli, we can evaluate the reliable shared variance between the two representations across stimuli.\nSpecifically, we can divide the images into two: a training split and a test split. We can compute singular vectors on the training split, and evalute test singular values on the test split, analogous to the CV-PCA procedure:\n\n\n\n\n\\begin{align*}\n    \\text{cov}(X_\\text{train}, Y_\\text{train})\n    &= X_\\text{train}^\\top Y_\\text{train} / (n - 1)\\\\\n    &= U \\Sigma V^\\top\\\\\n    \\\\\n    \\Sigma_\\text{cross-validated}\n    &= \\text{cov}(X_\\text{train} U, Y_\\text{test} V)\\\\\n    &= \\left( X_\\text{train} U \\right) ^\\top \\left( Y_\\text{test} V \\right) / (n - 1)\n\\end{align*}\n\n\n\nLoad the datasets\nsubject_1 = average_data_across_repetitions(\n    load_dataset(subject=0, roi=\"general\")\n).sortby(\"stimulus_id\")\nsubject_2 = average_data_across_repetitions(load_dataset(subject=1, roi=\"general\"))\n\nstimuli = load_stimuli()[\"stimulus_id\"].values\n\ndisplay(subject_1)\ndisplay(subject_2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'fMRI betas' (presentation: 700, neuroid: 15724)&gt;\n0.4915 0.2473 0.08592 0.05828 -0.1315 ... -0.2126 -0.6315 -0.5751 -0.5354\nCoordinates: (3/4)\n    x            (neuroid) uint8 12 12 12 12 12 12 12 ... 71 72 72 72 72 72 72\n    y            (neuroid) uint8 21 22 22 22 22 22 23 ... 34 29 29 30 30 30 31\n    ...           ...\n    stimulus_id  (presentation) object 'image02950' ... 'image72948'\nDimensions without coordinates: presentation, neuroid\nAttributes: (3/8)\n    resolution:      1pt8mm\n    preprocessing:   fithrf_GLMdenoise_RR\n    ...              ...\n    postprocessing:  averaged across first two repetitionsxarray.DataArray'fMRI betas'presentation: 700neuroid: 157240.4915 0.2473 0.08592 0.05828 ... -0.2126 -0.6315 -0.5751 -0.5354array([[ 0.4915219 ,  0.24733381,  0.08592446, ..., -0.366651  ,\n         0.30723202,  0.43520752],\n       [ 0.1664538 , -0.10728736,  0.35630295, ...,  0.8608913 ,\n         0.03464809,  0.11020081],\n       [ 1.0357349 ,  0.77598304,  0.35813144, ...,  0.2419075 ,\n         0.81557286,  0.38667244],\n       ...,\n       [-0.05812129, -0.4539395 ,  0.41060364, ...,  0.5738151 ,\n        -0.718189  , -0.638827  ],\n       [-0.00340644, -1.0050421 ,  0.7278904 , ...,  0.580743  ,\n        -0.50856245, -0.2727615 ],\n       [-1.2668517 , -1.4769105 , -0.3562023 , ..., -0.63146234,\n        -0.575121  , -0.5354325 ]], dtype=float32)Coordinates: (4)x(neuroid)uint812 12 12 12 12 ... 72 72 72 72 72array([12, 12, 12, ..., 72, 72, 72], dtype=uint8)y(neuroid)uint821 22 22 22 22 ... 29 30 30 30 31array([21, 22, 22, ..., 30, 30, 31], dtype=uint8)z(neuroid)uint847 45 46 47 48 ... 46 45 46 49 49array([47, 45, 46, ..., 46, 49, 49], dtype=uint8)stimulus_id(presentation)object'image02950' ... 'image72948'array(['image02950', 'image02990', 'image03049', 'image03077',\n       'image03146', 'image03157', 'image03164', 'image03171',\n       'image03181', 'image03386', 'image03434', 'image03449',\n       'image03489', 'image03626', 'image03682', 'image03687',\n       'image03729', 'image03809', 'image03842', 'image03847',\n       'image03856', 'image03913', 'image03951', 'image04051',\n       'image04058', 'image04129', 'image04156', 'image04249',\n       'image04423', 'image04436', 'image04667', 'image04690',\n       'image04768', 'image04786', 'image04835', 'image04892',\n       'image04930', 'image05034', 'image05106', 'image05204',\n       'image05301', 'image05338', 'image05459', 'image05542',\n       'image05583', 'image05602', 'image05714', 'image06199',\n       'image06222', 'image06431', 'image06444', 'image06489',\n       'image06514', 'image06521', 'image06558', 'image06801',\n       'image07007', 'image07039', 'image07120', 'image07207',\n       'image07366', 'image07418', 'image07480', 'image07654',\n       'image07840', 'image07859', 'image07944', 'image07948',\n       'image08006', 'image08109', 'image08204', 'image08225',\n       'image08394', 'image08415', 'image08435', 'image08465',\n       'image08509', 'image08646', 'image08807', 'image08843',\n...\n       'image64615', 'image64621', 'image64867', 'image64880',\n       'image65010', 'image65148', 'image65253', 'image65267',\n       'image65376', 'image65445', 'image65769', 'image65799',\n       'image65821', 'image65872', 'image65920', 'image65943',\n       'image66004', 'image66216', 'image66278', 'image66330',\n       'image66464', 'image66489', 'image66580', 'image66773',\n       'image66836', 'image66946', 'image66976', 'image67045',\n       'image67113', 'image67204', 'image67237', 'image67295',\n       'image67742', 'image67802', 'image67829', 'image68168',\n       'image68278', 'image68339', 'image68418', 'image68471',\n       'image68741', 'image68814', 'image68842', 'image68858',\n       'image68897', 'image69007', 'image69130', 'image69214',\n       'image69240', 'image69502', 'image69614', 'image69839',\n       'image69854', 'image70075', 'image70095', 'image70193',\n       'image70232', 'image70335', 'image70360', 'image70427',\n       'image70505', 'image71229', 'image71232', 'image71241',\n       'image71410', 'image71450', 'image71753', 'image71894',\n       'image72015', 'image72080', 'image72209', 'image72312',\n       'image72510', 'image72605', 'image72719', 'image72948'],\n      dtype=object)Indexes: (0)Attributes: (8)resolution :1pt8mmpreprocessing :fithrf_GLMdenoise_RRz_score :Trueroi :generalsubject :0brain shape :[ 81 104  83]citation :Allen, E.J., St-Yves, G., Wu, Y. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat Neurosci 25, 116-126 (2022). https://doi.org/10.1038/s41593-021-00962-xpostprocessing :averaged across first two repetitions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'fMRI betas' (presentation: 700, neuroid: 14278)&gt;\n-0.8554 0.0399 0.09591 -0.4694 -0.4573 ... -1.052 -0.6467 -0.6164 -0.8053\nCoordinates: (3/4)\n    x            (neuroid) uint8 11 11 11 11 12 12 12 ... 71 71 71 71 71 71 71\n    y            (neuroid) uint8 23 25 25 26 22 22 22 ... 28 28 29 29 30 30 30\n    ...           ...\n    stimulus_id  (presentation) object 'image02950' ... 'image72948'\nDimensions without coordinates: presentation, neuroid\nAttributes: (3/8)\n    resolution:      1pt8mm\n    preprocessing:   fithrf_GLMdenoise_RR\n    ...              ...\n    postprocessing:  averaged across first two repetitionsxarray.DataArray'fMRI betas'presentation: 700neuroid: 14278-0.8554 0.0399 0.09591 -0.4694 ... -1.052 -0.6467 -0.6164 -0.8053array([[-0.85538554,  0.03990293,  0.09591419, ..., -1.6162797 ,\n        -1.1637473 , -0.22345664],\n       [-1.1291182 , -0.24890876,  0.27723873, ...,  0.8381426 ,\n         1.2138667 ,  0.9067359 ],\n       [ 1.0714364 , -0.5201533 ,  0.18692002, ...,  1.1055344 ,\n         1.4412578 ,  0.5147082 ],\n       ...,\n       [ 0.6021575 , -0.00671851, -1.2595241 , ..., -0.10587344,\n        -0.15873775, -0.6149586 ],\n       [ 0.61964816, -0.6503349 , -0.42204803, ..., -0.73787045,\n        -1.0440085 , -1.0924375 ],\n       [-0.690616  ,  0.61802167, -1.1264803 , ..., -0.6467064 ,\n        -0.61641246, -0.8052834 ]], dtype=float32)Coordinates: (4)x(neuroid)uint811 11 11 11 12 ... 71 71 71 71 71array([11, 11, 11, ..., 71, 71, 71], dtype=uint8)y(neuroid)uint823 25 25 26 22 ... 29 29 30 30 30array([23, 25, 25, ..., 30, 30, 30], dtype=uint8)z(neuroid)uint837 32 33 38 36 ... 40 41 40 41 42array([37, 32, 33, ..., 40, 41, 42], dtype=uint8)stimulus_id(presentation)object'image02950' ... 'image72948'array(['image02950', 'image02990', 'image03049', 'image03077',\n       'image03146', 'image03157', 'image03164', 'image03171',\n       'image03181', 'image03386', 'image03434', 'image03449',\n       'image03489', 'image03626', 'image03682', 'image03687',\n       'image03729', 'image03809', 'image03842', 'image03847',\n       'image03856', 'image03913', 'image03951', 'image04051',\n       'image04058', 'image04129', 'image04156', 'image04249',\n       'image04423', 'image04436', 'image04667', 'image04690',\n       'image04768', 'image04786', 'image04835', 'image04892',\n       'image04930', 'image05034', 'image05106', 'image05204',\n       'image05301', 'image05338', 'image05459', 'image05542',\n       'image05583', 'image05602', 'image05714', 'image06199',\n       'image06222', 'image06431', 'image06444', 'image06489',\n       'image06514', 'image06521', 'image06558', 'image06801',\n       'image07007', 'image07039', 'image07120', 'image07207',\n       'image07366', 'image07418', 'image07480', 'image07654',\n       'image07840', 'image07859', 'image07944', 'image07948',\n       'image08006', 'image08109', 'image08204', 'image08225',\n       'image08394', 'image08415', 'image08435', 'image08465',\n       'image08509', 'image08646', 'image08807', 'image08843',\n...\n       'image64615', 'image64621', 'image64867', 'image64880',\n       'image65010', 'image65148', 'image65253', 'image65267',\n       'image65376', 'image65445', 'image65769', 'image65799',\n       'image65821', 'image65872', 'image65920', 'image65943',\n       'image66004', 'image66216', 'image66278', 'image66330',\n       'image66464', 'image66489', 'image66580', 'image66773',\n       'image66836', 'image66946', 'image66976', 'image67045',\n       'image67113', 'image67204', 'image67237', 'image67295',\n       'image67742', 'image67802', 'image67829', 'image68168',\n       'image68278', 'image68339', 'image68418', 'image68471',\n       'image68741', 'image68814', 'image68842', 'image68858',\n       'image68897', 'image69007', 'image69130', 'image69214',\n       'image69240', 'image69502', 'image69614', 'image69839',\n       'image69854', 'image70075', 'image70095', 'image70193',\n       'image70232', 'image70335', 'image70360', 'image70427',\n       'image70505', 'image71229', 'image71232', 'image71241',\n       'image71410', 'image71450', 'image71753', 'image71894',\n       'image72015', 'image72080', 'image72209', 'image72312',\n       'image72510', 'image72605', 'image72719', 'image72948'],\n      dtype=object)Indexes: (0)Attributes: (8)resolution :1pt8mmpreprocessing :fithrf_GLMdenoise_RRz_score :Trueroi :generalsubject :1brain shape :[ 82 106  84]citation :Allen, E.J., St-Yves, G., Wu, Y. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat Neurosci 25, 116-126 (2022). https://doi.org/10.1038/s41593-021-00962-xpostprocessing :averaged across first two repetitions\n\n\n\n\nCompute the cross-participant spectrum\ndef compute_cross_participant_spectrum(\n    x: xr.DataArray,\n    y: xr.DataArray,\n    /,\n    train_fraction: float = 7 / 8,\n) -&gt; np.ndarray:\n    stimuli = x[\"stimulus_id\"].values\n    n_train = int(train_fraction * len(stimuli))\n    stimuli = rng.permutation(stimuli)[:n_train]\n\n    train_indices = np.isin(x[\"stimulus_id\"].values, stimuli)\n\n    x_train = x.isel({\"presentation\": train_indices})\n    y_train = y.isel({\"presentation\": train_indices})\n\n    x_test = x.isel({\"presentation\": ~train_indices})\n    y_test = y.isel({\"presentation\": ~train_indices})\n\n    scorer = PLSSVD()\n    scorer.fit(x_train.values, y_train.values)\n    x_test_transformed = scorer.transform(x_test.values, direction=\"left\")\n    y_test_transformed = scorer.transform(y_test.values, direction=\"right\")\n\n    return np.diag(\n        np.cov(\n            x_test_transformed,\n            y_test_transformed,\n            rowvar=False,\n        )[:n_train, n_train:]\n    )\n\n\ncross_participant_spectrum = compute_cross_participant_spectrum(subject_1, subject_2)\n\ndata = pd.DataFrame(\n    {\n        \"cross-validated singular value\": cross_participant_spectrum,\n        \"rank\": assign_logarithmic_bins(\n            1 + np.arange(len(cross_participant_spectrum)),\n            min_=1,\n            max_=10_000,\n            points_per_bin=5,\n        ),\n    }\n)\n\nfig, ax = plt.subplots()\nsns.lineplot(\n    ax=ax,\n    data=data,\n    x=\"rank\",\n    y=\"cross-validated singular value\",\n    marker=\"o\",\n    dashes=False,\n    ls=\"None\",\n    err_style=\"bars\",\n    estimator=\"mean\",\n    errorbar=\"sd\",\n)\nax.set_xscale(\"log\")\nax.set_yscale(\"log\")\nax.set_aspect(\"equal\", \"box\")\nsns.despine(ax=ax, offset=20)\n\n\n\n\n\n\n\n\n\n\n\nPlot both the within- and cross-participant spectra\ndef compute_within_participant_spectrum(data: xr.DataArray) -&gt; np.ndarray:\n    x_train = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n    y_train = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n\n    x_test = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\"stimulus_id\")\n    y_test = data.isel({\"presentation\": data[\"rep_id\"] == 1}).sortby(\"stimulus_id\")\n\n    scorer = PLSSVD()\n    scorer.fit(x_train.values, x_train.values)\n    x_test_transformed = scorer.transform(x_test.values, direction=\"left\")\n    y_test_transformed = scorer.transform(y_test.values, direction=\"right\")\n\n    n_components = x_test_transformed.shape[-1]\n\n    return np.diag(\n        np.cov(\n            x_test_transformed,\n            y_test_transformed,\n            rowvar=False,\n        )[:n_components, n_components:]\n    )\n\n\nwithin_participant_spectrum = compute_within_participant_spectrum(\n    load_dataset(subject=0, roi=\"general\")\n)\n\ndata = pd.concat(\n    [\n        data.assign(comparison=\"cross-individual\"),\n        pd.DataFrame(\n            {\n                \"cross-validated singular value\": within_participant_spectrum,\n                \"rank\": assign_logarithmic_bins(\n                    1 + np.arange(len(within_participant_spectrum)),\n                    min_=1,\n                    max_=10_000,\n                    points_per_bin=5,\n                ),\n            }\n        ).assign(comparison=\"within-individual\"),\n    ],\n    axis=0,\n)\n\n\nwith sns.axes_style(\"whitegrid\"):\n    fig, ax = plt.subplots()\n\n    sns.lineplot(\n        ax=ax,\n        data=data,\n        x=\"rank\",\n        y=\"cross-validated singular value\",\n        hue=\"comparison\",\n        hue_order=[\"cross-individual\", \"within-individual\"],\n        style=\"comparison\",\n        style_order=[\"within-individual\", \"cross-individual\"],\n        markers=[\"o\", \"s\"],\n        dashes=False,\n        ls=\"None\",\n        err_style=\"bars\",\n        estimator=\"mean\",\n        errorbar=\"sd\",\n    )\n    ax.set_xscale(\"log\")\n    ax.set_yscale(\"log\")\n    ax.set_aspect(\"equal\", \"box\")\n\n    ax.grid(True, which=\"minor\", c=\"whitesmoke\")\n    ax.grid(True, which=\"major\", c=\"lightgray\")\n    for loc in (\"left\", \"bottom\", \"top\", \"right\"):\n        ax.spines[loc].set_visible(False)"
  },
  {
    "objectID": "pages/comparing_representations.html#variations",
    "href": "pages/comparing_representations.html#variations",
    "title": "Comparing neural representations",
    "section": "Variations",
    "text": "Variations\nThis cross-decomposition approach is quite flexible, allowing many possible levels of generalization.\n\nCross-trial generalization\n\nAt the most basic level, we can test the generalization of the latent dimensions across different trials – repetitions of the same stimuli. This approach identifies stimulus-specific variance that generalizes over trial-specific noise – and is what we used above for the within-participant comparison above (cross-validated PCA).\n\nCross-image generalization\n\nNext, we could test the generalization of the latent dimensions across different stimulus sets – how consistent the directions of covariance across different datasets. This was used to evaluate the cross-participant comparison above.\n\nCross-individual generalization\n\nFinally, we could examine the variance shared between two different high-dimensional systems – here, we compared neural responses from two different subjects.\n\n\nIn fact, we could combine several of these to get very strict generalization criteria: we could even estimate the spectrum of variance that generalizes across trials, stimuli, and individuals."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A High-Dimensional View of Neuroscience",
    "section": "",
    "text": "This site contains material for a tutorial presented at the conference on Cognitive Computational Neuroscience 2023.\n\n\n\n\n\n\nDon’t miss the tutorial!\n\n\n\n\nWhere\n\nEast Schools\n\nWhen\n\nSaturday, August 26, 2023 @ 10:45 - 12:30\n\n\n\n\n\n\n\n\n\n\nRun the tutorial interactively – or just follow along on the website!\n\n\n\nEach section is a computational notebook that can be run interactively on Google Colab or viewed rendered on this site – just follow the links below!\n\n\n\n\n\nSection\nRead\nInteract\nDownload\n\n\n\n\nIntroducing PCA\nwebsite\nColab\ndownload\n\n\nExploring neural data\nwebsite\nColab\ndownload\n\n\nDealing with noise\nwebsite\nColab\ndownload\n\n\nComparing representations\nwebsite\nColab\ndownload\n\n\nAnalyzing neural networks\nwebsite\nColab\ndownload\n\n\n\n\n\n\n\n\n\nIf you’d prefer to run the notebooks locally…\n\n\n\n\n\nCreate a Python virtual environment with Python &gt;=3.10.12 to run the notebooks. The required dependencies will be automatically installed when you run the first cell of each notebook.\n\n\n\n\n\n\n\n\n\nNotice a typo? Have any feedback?\n\n\n\nUse the Report an issue button on the sidebar of each page to contact us. Feel free to suggest edits by using the Edit this page button too!"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "A High-Dimensional View of Neuroscience",
    "section": "",
    "text": "This site contains material for a tutorial presented at the conference on Cognitive Computational Neuroscience 2023.\n\n\n\n\n\n\nDon’t miss the tutorial!\n\n\n\n\nWhere\n\nEast Schools\n\nWhen\n\nSaturday, August 26, 2023 @ 10:45 - 12:30\n\n\n\n\n\n\n\n\n\n\nRun the tutorial interactively – or just follow along on the website!\n\n\n\nEach section is a computational notebook that can be run interactively on Google Colab or viewed rendered on this site – just follow the links below!\n\n\n\n\n\nSection\nRead\nInteract\nDownload\n\n\n\n\nIntroducing PCA\nwebsite\nColab\ndownload\n\n\nExploring neural data\nwebsite\nColab\ndownload\n\n\nDealing with noise\nwebsite\nColab\ndownload\n\n\nComparing representations\nwebsite\nColab\ndownload\n\n\nAnalyzing neural networks\nwebsite\nColab\ndownload\n\n\n\n\n\n\n\n\n\nIf you’d prefer to run the notebooks locally…\n\n\n\n\n\nCreate a Python virtual environment with Python &gt;=3.10.12 to run the notebooks. The required dependencies will be automatically installed when you run the first cell of each notebook.\n\n\n\n\n\n\n\n\n\nNotice a typo? Have any feedback?\n\n\n\nUse the Report an issue button on the sidebar of each page to contact us. Feel free to suggest edits by using the Edit this page button too!"
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "A High-Dimensional View of Neuroscience",
    "section": "Acknowledgments",
    "text": "Acknowledgments\n\nThanks to the Natural Scene Dataset team for permission to use it for this tutorial and to the Open Science Foundation for hosting the data files."
  },
  {
    "objectID": "pages/analyzing_neural_networks.html",
    "href": "pages/analyzing_neural_networks.html",
    "title": "Analyzing and comparing deep neural networks",
    "section": "",
    "text": "Run this notebook interactively!\n\n\n\nHere’s a link to this notebook on Google Colab."
  },
  {
    "objectID": "pages/analyzing_neural_networks.html#experimental-setup",
    "href": "pages/analyzing_neural_networks.html#experimental-setup",
    "title": "Analyzing and comparing deep neural networks",
    "section": "Experimental setup",
    "text": "Experimental setup\n\n\nInstall all required dependencies\n# TODO uncomment before final packaging\n# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n\n\n\n\nLoad modules and functions\nimport torch\nfrom torch import nn\nimport torch.utils.data.dataset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\n\n\n\nDefine various helper functions\nimport time\nfrom tqdm.notebook import tqdm\nimport copy\n\nimport torch\nfrom torch import nn\nimport torch.utils.data.dataset\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef get_datasets():\n    \"\"\"Returns train and validation datasets.\"\"\"\n    mean = [0.485, 0.456, 0.406]\n    std = [0.229, 0.224, 0.225]\n    data_transforms = [\n        transforms.ToTensor(),\n        transforms.Normalize(mean=mean, std=std),\n        transforms.Grayscale(),\n    ]\n\n    dataset_class = datasets.CIFAR10\n\n    def get_dataset(train: bool):  # Returns the train or validation dataset.\n        root = \"./data\"\n        kwargs = dict(\n            root=root,\n            transform=transforms.Compose(data_transforms),\n            train=train,\n            download=True,\n        )\n        dataset = dataset_class(**kwargs)\n        return dataset\n\n    train_dataset = get_dataset(train=True)\n    val_dataset = get_dataset(train=False)\n    return train_dataset, val_dataset\n\n\ndef get_dataloaders(batch_size):\n    \"\"\"Returns train and validation dataloaders.\"\"\"\n    train_dataset, val_dataset = get_datasets()\n\n    def get_dataloader(dataset, shuffle):\n        return torch.utils.data.DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=shuffle,\n            num_workers=0,\n            pin_memory=True,\n        )\n\n    train_loader = get_dataloader(train_dataset, shuffle=True)\n    val_loader = get_dataloader(val_dataset, shuffle=False)\n    return train_loader, val_loader\n\n\ndef initialize_mlp_model(dims):\n    \"\"\"Create a simple MLP model.\n    :param dims: list of dimensions of each layer, should begin with dimension of input and end with number of classes.\n    :return: Sequential MLP model\n    \"\"\"\n    layers = [torch.nn.Flatten()]\n    for i in range(len(dims) - 1):\n        layers.append(nn.Linear(dims[i], dims[i + 1], bias=False))\n        if i &lt; len(dims) - 2:\n            layers.append(nn.ReLU())\n    model = nn.Sequential(*layers)\n    return model\n\n\ndef initialize_cnn_model(\n    channels, spatial_size=32, kernel_size=5, stride=2, num_classes=10\n):\n    \"\"\"Create a simple CNN model.\n    :param channels: list of channels of each convolutional layer, should begin with number of channels of input.\n    :return Sequential CNN model\n    \"\"\"\n    layers = []\n    for i in range(len(channels) - 1):\n        layers.append(\n            nn.Conv2d(channels[i], channels[i + 1], kernel_size=5, stride=2, bias=False)\n        )\n        spatial_size = ceil_div(spatial_size - kernel_size + 1, stride)\n        layers.append(nn.ReLU())\n\n    layers.extend(\n        [\n            nn.Flatten(),\n            nn.Linear(channels[-1] * spatial_size**2, num_classes, bias=False),\n        ]\n    )\n    model = nn.Sequential(*layers)\n    return model\n\n\ndef ceil_div(a: int, b: int) -&gt; int:\n    \"\"\"Return ceil(a / b).\"\"\"\n    return a // b + (a % b &gt; 0)\n\n\ndef train_model(model, train_loader, val_loader, lr=0.01, momentum=0.9, num_epochs=5):\n    \"\"\"Simple training of a model with SGD.\"\"\"\n    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n    loss_fn = nn.CrossEntropyLoss()\n\n    epoch = 0\n    model.to(DEVICE)\n    while True:\n        # Evaluate on validation set.\n        one_epoch(\n            loader=val_loader,\n            model=model,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            epoch=epoch,\n            is_training=False,\n        )\n\n        # Stop if we are at the last epoch.\n        if epoch == num_epochs:\n            break\n\n        # Train for one epoch (now epoch counts the current training epoch).\n        epoch += 1\n        one_epoch(\n            loader=train_loader,\n            model=model,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n            epoch=epoch,\n            is_training=True,\n        )\n\n    # Ensure evaluation mode and disable gradients before returning trained model.\n    model.eval()\n    for param in model.parameters():\n        param.requires_grad = False\n    return model\n\n\ndef one_epoch(loader, model, loss_fn, optimizer, epoch, is_training):\n    \"\"\"Run one train or validation epoch on the model.\n    :param loader: dataloader to use\n    :param model: model to train or evaluate\n    :param loss_fn: loss function (not used during evaluation)\n    :param optimizer: optimizer (not used during evaluation)\n    :param epoch: current epoch number (for tqdm description)\n    :param is_training: whether to train the model or simply evaluate it\n    :return: average accuracy during the epoch\n    \"\"\"\n    name_epoch = \"Train\" if is_training else \"Val\"\n    name_epoch = f\"{name_epoch} epoch {epoch}\"\n    accuracy_meter = AverageMeter()\n\n    if is_training:\n        model.train()\n    else:\n        model.eval()\n\n    with torch.set_grad_enabled(is_training):\n        it = tqdm(loader, desc=name_epoch)\n        for x, y in it:\n            x = x.to(DEVICE, non_blocking=True)\n            y = y.to(DEVICE, non_blocking=True)\n            y_hat = model(x)\n\n            loss = loss_fn(y_hat, y)\n            accuracy = 100 * torch.sum((y_hat.argmax(-1) == y).float())\n            accuracy_meter.update(val=accuracy.item(), n=y.shape[0])\n            it.desc = f\"{name_epoch}: {accuracy_meter.avg():.2f}% accuracy\"\n\n            if is_training:\n                # compute gradient and do SGD step\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n    return accuracy_meter.avg()\n\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value.\"\"\"\n\n    def __init__(self):\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n):\n        self.sum += val\n        self.count += n\n\n    def avg(self):\n        return self.sum / self.count\n\n\ndef get_activations(x, layers, model):\n    \"\"\"Returns the hidden activations of a model.\n    :param x: input to use, tensor of shape (B, C, [N, N])\n    :param layers: list of integers (j corresponds to output of j-th layer, 0 corresponds to input of model)\n    :param model: model to use (should be Sequential)\n    :return: list of saved activations of same length as layers\n    \"\"\"\n    saved_activations = []\n\n    def hook(self, inputs, output):  # inputs is a tuple, we assume it is of length 1\n        saved_activations.append(inputs[0])\n\n    # Register hooks to save activations of chosen layers.\n    for layer in layers:\n        model[layer].register_forward_hook(hook)\n\n    # Forward of model: hooks will be run and activations will be saved.\n    _ = model(x)\n\n    # Clear hooks.\n    for layer in layers:\n        model[layer]._forward_hooks.clear()\n\n    return saved_activations\n\n\ndef compute_activation_covariances(loader, layers, model1, model2=None):\n    \"\"\"Compute the (cross-)covariance of hidden activations at several layers of one or two models.\n    :param loader: data loader to use\n    :param layers: list of integers (j corresponds to output of j-th layer, 0 corresponds to input of model)\n    :param model1: model to use (should be Sequential)\n    :param model2: optional model for a cross-covariance (if None, compute the self-covariance of model1)\n    :return: list of computed covariances (C1, C2), of same length as layers\n    \"\"\"\n    meters = [AverageMeter() for _ in layers]\n\n    # Version of get_activations which treats spatial dimensions as additional batch dimensions.\n    get_acts = lambda *args: [space_to_batch(act) for act in get_activations(*args)]\n\n    for x, _ in tqdm(loader, desc=\"Computing activation covariance\"):\n        x = x.to(DEVICE)\n        activations1 = get_acts(x, layers, model1)\n        activations2 = activations1 if model2 is None else get_acts(x, layers, model2)\n\n        for i, (act1, act2) in enumerate(zip(activations1, activations2)):\n            cov = act1.T @ act2  # (C1, C2), sum of outer products over the batch\n            meters[i].update(val=cov, n=act1.shape[0])\n\n    return [meter.avg() for meter in meters]\n\n\ndef space_to_batch(x):\n    \"\"\"(B, C, [M, N]) to (B[MN], C).\"\"\"\n    if x.ndim == 4:\n        x = x.permute(0, 2, 3, 1)  # (B, M, N, C)\n        x = x.reshape((-1, x.shape[-1]))  # (BMN, C)\n    return x\n\n\ndef clip(model, layer, basis, dimensions, loader):\n    \"\"\"Clip and evaluate a model.\n    :param model: trained model to clip (should be Sequential)\n    :param layer: index of layer to clip\n    :param basis: ordered orthogonal basis vectors for the clipping, of shape (num_vectors, input_dim)\n    :param dimensions: list of clipping dimensions\n    :param loader: data loader to use for the evaluation\n    :return: list of accuracies\n    \"\"\"\n    models = []\n    for dim in dimensions:\n        projector = basis[:dim].T @ basis[:dim]\n        model_clipped = copy.deepcopy(model)\n        model_clipped[layer].weight = torch.nn.Parameter(\n            model_clipped[layer].weight @ projector, requires_grad=False\n        )\n        models.append(model_clipped)\n\n    return evaluate(models, loader, desc=\"Evaluation after clipping\")\n\n\ndef evaluate(models, loader, desc=\"Evaluation\"):\n    \"\"\"Evaluate a list of models.\n    :param models: list of models to evaluate\n    :param loader: dataloader to use for evaluation\n    :return: list of accuracies, one per model\n    \"\"\"\n    accuracy_meters = [AverageMeter() for _ in models]\n\n    it = tqdm(loader, desc=desc)\n    for x, y in it:\n        x = x.to(DEVICE, non_blocking=True)\n        y = y.to(DEVICE, non_blocking=True)\n        for model, accuracy_meter in zip(models, accuracy_meters):\n            y_hat = model(x)\n            accuracy = 100 * torch.sum((y_hat.argmax(-1) == y).float())\n            accuracy_meter.update(val=accuracy.item(), n=y.shape[0])\n\n    return [accuracy_meter.avg() for accuracy_meter in accuracy_meters]\n\n\ndef generate_network(reference_model, state_dict_keys, train_loader, val_loader):\n    \"\"\"Generate a new network with a similar performance from a reference network.\n    NOTE: this function does not take care of batch norm parameters.\n    They should be recomputed when the previous layer has been generated.\n    NOTE: this function can be extended in many ways: sample only a subset of layers, change the width of some layers,\n    use weight covariances averaged over several models...\n    :param reference_model: reference model for the rainbow model\n    :param state_dict_keys: list of keys in model state dict to sample\n    :param train_loader: dataloader used for computing alignment between reference and generated model\n    :param val_loader: dataloader used to evaluate performance of generated model\n    :return: list of accuracies, corresponding to the reference and generated models\n    \"\"\"\n    # The generated model is initialized as a copy of the reference model.\n    generated_model = copy.deepcopy(reference_model)\n    state_dict = generated_model.state_dict()\n\n    # At beginning of loop, this contains alignment at layer j between generated model and reference model\n    # (used to determine the correct covariance to generate atoms at layer j).\n    alignment = None  # alignment matrix (C_in_reference, C_in_generated)\n\n    results = {}  # metric, layer_idx -&gt; performance\n\n    # Generate each layer iteratively.\n    for j, key in enumerate(state_dict_keys):\n        weight = state_dict[\n            key\n        ]  # (C_out, C_in, [h, w]) depending on conv or fully connected layer\n\n        # Compute new weight in reference space.\n        if j &lt; len(state_dict_keys) - 1:\n            # Sample Gaussian weights with the same covariance as the trained weights.\n            # Compute square root of covariance with an SVD (more efficient when C_out &lt;&lt; C_in*h*w).\n            weight_flat = weight.reshape((weight.shape[0], -1))  # (C_out, C_in*h*w)\n            u, s, vt = torch.linalg.svd(\n                weight_flat, full_matrices=False\n            )  # (C_out, R), (R,), (R, C_in*h*w) where R = rank\n            white_gaussian = torch.randn(\n                u.shape, dtype=weight.dtype, device=weight.device\n            )  # (C_out, R)\n            colored_gaussian = white_gaussian @ (s[:, None] * vt)  # (C_out, C_in*h*w)\n            new_weight = colored_gaussian.reshape(weight.shape)  # (C_out, C_in, [h, w])\n        else:\n            # Use the trained classifier.\n            new_weight = weight\n\n        # Realign the weights from the reference model to the generated model (necessary after first layer).\n        if j &gt; 0:\n            new_weight = contract(\n                new_weight, alignment.T, axis=1\n            )  # C_in_reference to C_in_generated\n\n        # Set the new weights in the generated model.\n        # NOTE: this an intermediate model, as sampling the j-th layer means that the j+1-th layer is no longer aligned.\n        # As such, if evaluated as is, its accuracy would be that of a random guess.\n        state_dict[key] = new_weight\n        generated_model.load_state_dict(state_dict)\n\n        # Then compute alignment of the generated network with the reference model for the next layer.\n        if j &lt; len(state_dict_keys) - 1:\n            next_key = state_dict_keys[j + 1]\n            # Compute index of layer by relying on Sequential naming convention: next_key is \"{layer_idx}.weight\".\n            layer = int(next_key.split(\".\")[0])\n\n            [activation_covariance] = compute_activation_covariances(\n                train_loader, [layer], reference_model, generated_model\n            )  # (C_in_reference, C_in_generated)\n\n            u, s, vh = torch.linalg.svd(\n                activation_covariance, full_matrices=False\n            )  # (C_in_reference, R), (R,), (R, C_in_generated)\n            alignment = u @ vh  # (C_in_reference, C_in_generated)\n\n    # Evaluate models.\n    return evaluate(\n        [reference_model, generated_model], val_loader, desc=\"Evaluation after sampling\"\n    )\n\n\ndef contract(tensor, matrix, axis):\n    \"\"\"tensor is (..., D, ...), matrix is (P, D), returns (..., P, ...).\"\"\"\n    t = torch.moveaxis(tensor, source=axis, destination=-1)  # (..., D)\n    r = t @ matrix.T  # (..., P)\n    return torch.moveaxis(r, source=-1, destination=axis)  # (..., P, ...)\n\n\n\nDataset\nThe CIFAR-10 dataset contains 50,000 training and 10,000 validation images of resolution 32 \\times 32. They are divided in 10 classes:\n\n\n\nSample images from CIFAR-10\n\n\nThe dataset is automatically downloaded and loaded in memory with torchvision (see the CIFAR10 class). Here we do not perform any data augmentation for simplicity, and convert the images to grayscale for computational efficiency.\n\n\nLoad the dataset\ntrain_loader, val_loader = get_dataloaders(batch_size=128)\nclass_labels = [\n    \"airplane\",\n    \"automobile\",\n    \"bird\",\n    \"cat\",\n    \"deer\",\n    \"dog\",\n    \"frog\",\n    \"horse\",\n    \"ship\",\n    \"truck\",\n]\n\n# Draw one batch to show some training images\nx, y = next(train_loader.__iter__())\n\nfig, ax = plt.subplots(ncols=5, figsize=(10, 3))\nfig.suptitle(\"Random images from the training set\")\nfor i in range(5):\n    ax[i].imshow(x[i].cpu().numpy().transpose(1, 2, 0), cmap=\"gray\")\n    ax[i].axis(\"off\")\n    ax[i].set_title(class_labels[y[i]])\n\n\nFiles already downloaded and verified\nFiles already downloaded and verified\n\n\n\n\n\n\n\nNetwork architecture and training\nWe begin with a barebones feedforward network architecture with two hidden layers.\n\n\n\nA simple 2-layer perceptron\n\n\nIt computes the function  f(x) = \\theta \\sigma(W_2 \\sigma(W_1 x))\nwhere:\n\nx \\in \\mathbb R^d is an input image reshaped as a vector (d = 32 \\times 32),\nW_1 \\in \\mathbb R^{n_1 \\times d} are the weights of the n_1 neurons in the first layer,\nW_2 \\in \\mathbb R^{n_2 \\times n_1} are the weights of the n_2 neurons in the second layer,\n\\theta \\in \\mathbb R^{c \\times n_2} are the weights of the classifier, which predicts log-likelihoods for the c = 10 classes,\n\\sigma is a pointwise non-linearity (here a ReLU).\n\n\n\nInitialize the model\ndims = [\n    32 * 32,\n    512,\n    256,\n    10,\n]  # Sizes of layers, [input dim, hidden dims..., output dim (number of classes)]\nmodel = initialize_mlp_model(dims)\nprint(model)\n\n\nSequential(\n  (0): Flatten(start_dim=1, end_dim=-1)\n  (1): Linear(in_features=1024, out_features=512, bias=False)\n  (2): ReLU()\n  (3): Linear(in_features=512, out_features=256, bias=False)\n  (4): ReLU()\n  (5): Linear(in_features=256, out_features=10, bias=False)\n)\n\n\nWe train the network for 5 epochs (passes over the training set). We measure the average accuracy (frequency at which the network correctly predicts the correct class). On a held-out validation set, it increases from 10%, which corresponds to a random guess, to about 43%.\nNote: accuracies below 90% are generally considered low, with state-of-the-art networks reaching 99.5% accuracy. The accuracy can be increased by switching to a convolutional architecture (see below), increasing the width and depth of the network, adding data augmentation, training for a longer time, etc.\n\n\nTrain the model\n_ = train_model(model, train_loader, val_loader, num_epochs=5)"
  },
  {
    "objectID": "pages/analyzing_neural_networks.html#analyzing-a-trained-network",
    "href": "pages/analyzing_neural_networks.html#analyzing-a-trained-network",
    "title": "Analyzing and comparing deep neural networks",
    "section": "Analyzing a trained network",
    "text": "Analyzing a trained network\n\nMeasuring the dimensionality of hidden representations\nAt each layer, we can measure the dimensionality of the corresponding hidden representation with a PCA. We thus compute the covariance of the representation. Let \\varphi(x) denote the representation of the input x at the given layer. Then, we compute its covariance:\n \\mathbb E_x\\left[\\varphi(x) \\, \\varphi(x)^\\top\\right]. \nNote: We shall act as if the mean \\mathbb E_x \\left[ \\varphi(x) \\right] was zero for simplicity. Typically, this is enforced by a normalization layer such as batch normalization or layer normalization.\nOne might want to compute the hidden representations of all the training images:\n\n\\newcommand{\\vertbar}{\\rule[-1ex]{0.5pt}{2.5ex}}\nX =\n\\left[\n  \\begin{array}{cccc}\n    \\vertbar & \\vertbar &        & \\vertbar \\\\\n    \\varphi(x_1)     & \\varphi(x_2)    & \\cdots & \\varphi(x_n)    \\\\\n    \\vertbar & \\vertbar &        & \\vertbar\n  \\end{array}\n\\right]^\\top,\n\nand compute the covariance with \\frac1n X^\\top X. However, this matrix is usually too big to fit into memory (especially for larger datasets)!\nWe thus adopt the approach of mini-batching. We break the dataset into batches (little chunks) and load it in memory one batch at a time. The covariance can then be computed iteratively in this way:\ncovariance_accumulator = 0\nnum_samples = 0\nfor x in dataloader:  # x has shape (batch_size, input_dim)\n    z = phi(x)  # z has shape (batch_size, repr_dim)\n    covariance_accumulator += z.T @ z  # shape (repr_dim, repr_dim)\n    num_samples += z.shape[0]  # We have processed batch_size samples.\ncovariance = covariance_accumulator / num_samples\nThis “trick” works for the expected value of any function over the data. The mini-batching and loading into memory is taken care of by PyTorch’s DataLoader class. Note that contrary to model training, the batch size here does not affect the end result, it influences only the time and memory complexity of the computation. The batch size should be small enough to fit into memory, but as large as possible to exploit the parallelization of computation (as done automatically by PyTorch on a GPU backend).\n\n\nCompute activation covariances\n# Layers 3 and 5 correspond to the output of the two ReLUs.\nlayers = [3, 5]\nactivation_covariances = compute_activation_covariances(train_loader, layers, model)\n# Activation covariances is a list of covariances, one per layer.\n\n\n\n\n\nOnce we have compute the covariance of the hidden representation, we can diagonalize it to compute its eigenvalues, which encode the variance along each principal component (we do not concern ourselves with cross-validation here).\nAs another way of evaluating the importance of each principal component, we can perform clipping. This consists of inserting an operation in the middle of the network, which projects the hidden representation on its first k principal components. We can then evaluate the accuracy of the network as a function of k, which allows measuring the cumulative “information” stored in the first k PCs.\n\n\nVisualize activation covariance spectrum and the result of clipping\nfig, ax = plt.subplots(figsize=(8, 8), nrows=2, ncols=2)\n\nfor layer_idx in range(2):\n    activation_covariance = activation_covariances[layer_idx]\n    activation_eigenvalues, activation_eigenvectors = torch.linalg.eigh(\n        activation_covariance\n    )\n    # (num_eig,), descending order\n    activation_eigenvalues = activation_eigenvalues.flip(-1)\n\n    # (num_eig, dim), descending order\n    activation_eigenvectors = activation_eigenvectors.flip(-1).T\n\n    # Plot the eigenvalues of the activation covariances.\n    plt.sca(ax[0][layer_idx])\n    plt.title(f\"Activation covariance spectrum (layer {layer_idx + 1})\")\n    y = activation_eigenvalues.cpu().numpy()\n    x = 1 + np.arange(len(y))\n    plt.plot(x, y, color=\"tab:green\")\n    plt.yscale(\"log\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Rank\")\n    plt.ylabel(\"Eigenvalue\")\n\n    # Clipping analysis.\n    dimensions = 1 + np.arange(activation_eigenvectors.shape[1])\n    accuracies = clip(\n        model, 2 * layer_idx + 3, activation_eigenvectors, dimensions, val_loader\n    )\n\n    plt.sca(ax[1][layer_idx])\n    plt.plot(dimensions, accuracies, color=\"tab:red\")\n    plt.title(f\"Accuracy after activation clipping (layer {layer_idx + 1})\")\n    plt.xlabel(\"Maximum rank\")\n    plt.ylabel(r\"Accuracy (%)\")\n    plt.xscale(\"log\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe see that the learned representations have a power law spectrum. Further, they can be somewhat reduced in dimension at little cost in accuracy (note that rank is in log scale).\n\n\nMeasuring the dimensionality of neuron weights\nLet us focus on the first layer. The weight matrix W_1 of size n_1 \\times d contains the weights of the n_1 neurons in the first layer, each being a d-dimensional vector. (This also applies to the other layers. For instance, the second weight matrix W_2 of size n_2 \\times n_1 contains the weights of the n_2 neurons in the second layer, each being an n_1-dimensional vector.)\nWe can view these neurons as being samples from a continuous distribution, and then estimate statistical properties (moments) of this distribution. For instance, assuming a zero mean, we can compute the covariance of the neuron weights as:  C = \\frac1{n_1} W_1^\\top W_1 \n\nThe covariance reveals how the global geometry of the representation is modified by the neuron population. Indeed, the representation computed by the first layer is \\varphi_1(x) = \\sigma(W_1 x). Focusing on the weight matrix W_1, we can decompose its effect with a singular value decomposition:  W_1 = U \\Sigma V^\\top.\n\nRecall that the right singular vectors V are also the eigenvectors of the covariance C, while the singular values \\Sigma are directly related to the eigenvalues of the covariance C.\nThis implies that the first layer \\sigma(W_1 x) computes the following steps:\n\nan orthogonal change of basis to the PCA basis of the neuron weights,\na scaling along each principal component (proportional to the square root of the covariance eigenvalues),\nanother orthogonal change of basis, which defines the basis in which the non-linearity \\sigma is applied.\n\nThe covariance eigenvalues and eigenvectors thus define how the neural code globally changes the geometry of its input before the non-linearity. In particular, if the covariance is low-rank, it indicates that the representation only depends on a low-dimensional projection of the input.\nTo interpret the eigenvalues of the covariance, we can compare their values after training with their values at initialization. Neuron weights are initialized from a distribution with covariance identity (in PyTorch, each component is by default independently drawn from a uniform distribution). However, the eigenvalues of the empirical covariance computed from only the n_1 neurons are not all one but follow the Marchenko-Pastur distribution. This comparison allows estimating which eigenvalues are significantly non-random.\n\n\nVisualize weight covariance spectrum and the result of clipping\nfig, ax = plt.subplots(figsize=(8, 8), nrows=2, ncols=2)\n\nfor layer_idx in range(2):\n    weight_matrix = model[2 * layer_idx + 1].weight  # (N, D)\n    # Compute eigenvalues and eigenvectors of weight covariance efficiently from the SVD.\n    _, weight_singular_values, weight_eigenvectors = torch.linalg.svd(\n        weight_matrix, full_matrices=False\n    )  # (num_eig, dim), descending order\n    weight_eigenvalues = weight_singular_values**2 / weight_matrix.shape[0]\n\n    # Compare with eigenvalues at initialization.\n    init_weight_matrix = initialize_mlp_model(dims)[2 * layer_idx + 1].weight\n    init_eigenvalues = (\n        torch.linalg.svdvals(init_weight_matrix) ** 2 / weight_matrix.shape[0]\n    )\n\n    # Plot the eigenvalues of the weight covariances.\n    plt.sca(ax[0][layer_idx])\n    plt.title(f\"Weight covariance eigenvalues (layer {layer_idx + 1})\")\n    y = weight_eigenvalues.cpu().numpy()\n    x = 1 + np.arange(len(y))\n    plt.plot(x, y, color=\"tab:blue\", label=\"trained\")\n    plt.plot(\n        x,\n        init_eigenvalues.detach().cpu().numpy(),\n        linestyle=\"dashed\",\n        color=\"tab:blue\",\n        label=\"init\",\n    )\n    plt.yscale(\"log\")\n    plt.xlabel(\"Rank\")\n    plt.ylabel(\"Eigenvalue\")\n    plt.legend(loc=\"upper right\")\n\n    # Clipping analysis.\n    dimensions = 1 + np.arange(weight_eigenvectors.shape[0])\n    accuracies = clip(\n        model, 2 * layer_idx + 1, weight_eigenvectors, dimensions, val_loader\n    )\n\n    plt.sca(ax[1][layer_idx])\n    plt.plot(dimensions, accuracies, color=\"tab:red\")\n    plt.title(f\"Accuracy after weight clipping (layer {layer_idx + 1})\")\n    plt.xlabel(\"Maximum rank\")\n    plt.ylabel(r\"Accuracy (%)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nWe see that only the largest eigenvalues depart from the Marchenko-Pastur distribution. At low ranks, the spectrum roughly has an exponential decay, as evidenced by the straight line in the log-linear plot (note that rank is in linear scale here to evidence this fact). This gives an estimation of about 50 learned principal components for the first layer, and 15 for the second layer. Further, clipping the neuron weights to these learned principal compoennts incurs a negligible loss of accuracy."
  },
  {
    "objectID": "pages/analyzing_neural_networks.html#comparing-neural-networks",
    "href": "pages/analyzing_neural_networks.html#comparing-neural-networks",
    "title": "Analyzing and comparing deep neural networks",
    "section": "Comparing neural networks",
    "text": "Comparing neural networks\nLet us a train a second network with the same architecture on the same dataset but with a different random initialization.\n\n\nTrain a second network\nmodels = [model] + [\n    train_model(initialize_mlp_model(dims), train_loader, val_loader, num_epochs=5)\n]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis second network reaches the same accuracy as the first one. Has it arrived at the same solution than the first network?\n\nComparing weight and activation covariances\nFor both networks, we can compute their weight and activation covariances at each layer. How do we compare them?\nA first step is to (visually) compare the eigenvalues of the covariance.\nHowever, this is only a partial comparison. A much stronger comparison can be obtained by comparing the eigenvectors of the covariance. This is achieved by computing the orthogonal change of basis from the eigenvectors for the first network to the eigenvectors for the second network. This is a matrix that we can visualize. If it is close to the identity, then the two sets of eigenvectors are almost the same. On the other hand, if the eigenvectors are really different, then no entry of this matrix will be close to unity.\nNote: The eigenvectors of a matrix are determined up to a reverse in direction. This means that v and -v correspond to the same eigenvector. To this end, to compare two sets of eigenvectors, we look at the absolute values of the entries of the change of basis matrix.\n\n\nCompute activation covariances for the second network\nmodel_layer_covs = [activation_covariances] + [\n    compute_activation_covariances(train_loader, layers, models[1])\n]\n\n\n\n\n\n\n\nCompare both networks\nfig, ax = plt.subplots(figsize=(16, 8), nrows=2, ncols=4)\n\nfor j in range(2):\n    # Compute weight spectrum and eigenvectors for each model.\n    weight_spectra = []\n    weight_eigenvectors = []\n    for model in models:\n        w = model[2 * j + 1].weight  # (N, D)\n        u, s, vh = torch.linalg.svd(w, full_matrices=False)  # (N, R), (R,), (R, D)\n        spectrum = s**2 / w.shape[0]\n        weight_spectra.append(spectrum)\n        weight_eigenvectors.append(vh)\n\n    # Compare weights spectra (stable).\n    plt.sca(ax[0][2 * j])\n    plt.title(f\"Weight covariance eigenvalues (layer {j + 1})\")\n    for i, spectrum in enumerate(weight_spectra):\n        y = spectrum.cpu().numpy()\n        x = 1 + np.arange(len(y))\n        plt.plot(\n            x,\n            y,\n            color=[\"tab:blue\", \"tab:orange\"][i],\n            linestyle=[\"solid\", \"dashed\"][i],\n            label=f\"Network {i + 1}\",\n        )\n    plt.yscale(\"log\")\n    plt.xlabel(\"Rank\")\n    plt.ylabel(\"Eigenvalue\")\n    plt.legend(loc=\"upper right\")\n\n    # Compare weight eigenvectors (unstable after first layer because no alignment).\n    plt.sca(ax[1][2 * j])\n    stab = torch.abs(weight_eigenvectors[0] @ weight_eigenvectors[1].T)  # (R, R)\n    plt.imshow(stab.T.cpu().numpy()[:50, :50], cmap=\"cividis\", vmin=0, vmax=1)\n    plt.colorbar(shrink=0.75)\n    plt.title(f\"Cos sim of weight eigenvectors (layer {j + 1})\")\n    plt.xlabel(\"Rank (network 1)\")\n    plt.ylabel(\"Rank (network 2)\")\n    ticks = np.asarray([1, 10, 20, 30, 40, 50])\n    plt.xticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n    plt.yticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n\n    # Compute activations spectrum and eigenvectors for each model.\n    activation_spectra = []\n    activation_eigenvectors = []\n    for layer_covs in model_layer_covs:\n        cov = layer_covs[j]  # (C, C)\n        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n        # Sort in descending order and transpose.\n        eigenvalues, eigenvectors = eigenvalues.flip(-1), eigenvectors.flip(-1)\n        eigenvectors = eigenvectors.T  # (R, D)\n        activation_spectra.append(eigenvalues)\n        activation_eigenvectors.append(eigenvectors)\n\n    # Compare activation spectra (stable).\n    plt.sca(ax[0][2 * j + 1])\n    plt.title(f\"Activation covariance eigenvalues (layer {j + 1})\")\n    for i, spectrum in enumerate(activation_spectra):\n        y = spectrum.cpu().numpy()\n        x = 1 + np.arange(len(y))\n        plt.plot(\n            x,\n            y,\n            color=[\"tab:green\", \"tab:purple\"][i],\n            linestyle=[\"solid\", \"dashed\"][i],\n            label=f\"Network {i + 1}\",\n        )\n    plt.yscale(\"log\")\n    plt.xscale(\"log\")\n    plt.xlabel(\"Rank\")\n    plt.ylabel(\"Eigenvalue\")\n    plt.legend(loc=\"upper right\")\n\n    # Compare activation eigenvectors (unstable because no alignment)\n    plt.sca(ax[1][2 * j + 1])\n    stab = torch.abs(\n        activation_eigenvectors[0] @ activation_eigenvectors[1].T\n    )  # (R, R)\n    plt.imshow(stab.T.cpu().numpy()[:50, :50], cmap=\"cividis\", vmin=0, vmax=1)\n    plt.colorbar(shrink=0.75)\n    plt.title(f\"Cos sim of activation eigenvectors (layer {j + 1})\")\n    plt.xlabel(\"Rank (network 1)\")\n    plt.ylabel(\"Rank (network 2)\")\n    ticks = np.asarray([1, 10, 20, 30, 40, 50])\n    plt.xticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n    plt.yticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nWeight and activation spectra are the same across random initialization. However, eigenvectors are not the same except for the first layer weights. The reason is that deeper eigenvectors depend on the particular individual neurons of the first layer, not just their population statistics. Network alignment is needed to take that into account.\n\n\nAligning two networks together\nThe similarity between activation covariance eigenvalues suggests that the representations learned by the two networks might have shared latent dimensions. The dissimilarity between the activation covariance eigenvectors show that these latent dimensions are expressed in different bases.\nThe two networks can be aligned together with PLS-SVD (hyper-alignment). Given representations \\varphi(x) and \\psi(x) for the two networks, it computes the best orthogonal alignment matrix A that aligns \\varphi to \\psi:  \\min_A \\mathbb E_x[\\|A\\varphi(x) - \\psi(x)\\|^2] \\quad\\text{such that}\\quad A^\\top A = I_d \nThis is akin to a linear regression, but additionally imposes that A is orthogonal in order to preserve the geometry of the latent dimensions.\nThe optimal alignment A can be computed explicitly from an SVD of the cross-covariance matrix:  A = U V^\\top \\quad\\text{with}\\quad \\mathbb E_x[\\psi(x)\\varphi(x)^\\top] = U\\Sigma V^\\top\n\n\n\n\nThe magnitude of the singular values \\Sigma indicate the amount of variance that is explained by the alignment A. Indeed, the unexplained variance (the residual error) can be written as:  \\mathbb E_x[\\|A\\varphi(x) - \\psi(x)\\|^2] = \\mathbb E_x[\\|\\varphi(x)\\|^2] + \\mathbb E_x[\\|\\psi(x)\\|^2] - 2 \\,\\mathrm{tr}\\ \\Sigma \nTo evaluate the effectiveness of the alignment procedure, we can thus compute an r^2 measure as the ratio between the explained variance and the total variance:  r^2 = \\frac{\\mathrm{tr}\\ \\Sigma}{\\sqrt{\\mathbb E_x[\\|\\varphi(x)\\|^2]}\\sqrt{\\mathbb E_x[\\|\\psi(x)\\|^2]}} \n\n\nCompare cross-covariance between model activations\nmodel1, model2 = models\ncross_covs = compute_activation_covariances(train_loader, layers, model1, model2)\n# cross_covs is a list of activation covariances (C1, C2) for each layer.\n\n\n\n\n\n\n\nCompute alignments between models with an SVD, and evaluate alignment accuracy (fraction of explained variance)\naligns = []  # layer_idx -&gt; alignment\nfor j in range(2):\n    cross_cov = cross_covs[j]\n    u, s, vh = torch.linalg.svd(cross_cov, full_matrices=False)\n\n    explained_variance = torch.sum(s)\n    total_variance = torch.sqrt(\n        torch.trace(model_layer_covs[0][j]) * torch.trace(model_layer_covs[1][j])\n    )\n    r_squared = explained_variance / total_variance\n    print(\n        f\"Layer {j + 1}: {100 * r_squared.item():.1f}% of variance explained by\"\n        \" alignment\"\n    )\n    # NOTE: r² should be computed on a test set to control for overfitting.\n\n    align = u @ vh\n    aligns.append(align)\n\n\nLayer 1: 92.3% of variance explained by alignment\nLayer 2: 87.9% of variance explained by alignment\n\n\nWe see that the alignment procedure is able to explain a good fraction of the variance of the representations.\n\n\nUsing aligned networks to compare weight and activation eigenvectors\nLet us now use the alignment matrix A to meaningfully compare weight and activation eigenvectors.\nThe rule of thumb is to always compare eigenvectors that apply on the same representation. Consider two networks, with respective hidden representations \\varphi(x) and \\psi(x). These two representations share their latent dimensions, as evidenced by the fact that A\\varphi(x) \\approx \\psi(x).\nAssume that we have an activation eigenvector u for the first network, and another activation eigenvector v for the second network, which may or may not encode the same latent dimension. How do we compare them? Because A\\varphi(x) \\approx \\psi(x), we should compare Au with v.\nSimilarly, suppose that u and v are now weight eigenvectors respectively for the first and second network. The weight eigenvector u applies on the representation \\varphi(x), computing \\langle u, \\varphi(x) \\rangle, while the weight eigenvector v applies on the the representation \\psi(x), computing \\langle v, \\psi(x) \\rangle. We then have  \\langle u, \\varphi(x) \\rangle = \\langle Au, A\\varphi(x) \\rangle \\approx \\langle Au, \\psi(x) \\rangle,  which again means that we should compare Au with v.\n\n\n\nVisualize the similarity of the aligned eigenvectors\nfig, ax = plt.subplots(figsize=(16, 8), nrows=2, ncols=4)\n\nfor j in range(2):\n    # Compute weight eigenvectors for each model.\n    weight_eigenvectors = []\n    for model in models:\n        w = model[2 * j + 1].weight  # (N, D)\n        u, s, vh = torch.linalg.svd(w, full_matrices=False)  # (N, R), (R,), (R, D)\n        weight_eigenvectors.append(vh)\n\n    # Compute activations eigenvectors for each model.\n    activation_eigenvectors = []\n    for layer_covs in model_layer_covs:\n        cov = layer_covs[j]  # (C, C)\n        eigenvalues, eigenvectors = torch.linalg.eigh(cov)\n        # Sort in descending order and transpose.\n        eigenvalues, eigenvectors = eigenvalues.flip(-1), eigenvectors.flip(-1)\n        eigenvectors = eigenvectors.T  # (R, D)\n        activation_eigenvectors.append(eigenvectors)\n\n    for quantity, eigenvectors in dict(\n        weight=weight_eigenvectors, activation=activation_eigenvectors\n    ).items():\n        for aligned in [False, True]:\n            # Compute stability, with alignment if needed.\n            if aligned and not (quantity == \"weight\" and j == 0):\n                align = aligns[dict(weight=j - 1, activation=j)[quantity]]\n                stab = torch.abs(eigenvectors[0] @ align @ eigenvectors[1].T)  # (R, R)\n            else:\n                stab = torch.abs(eigenvectors[0] @ eigenvectors[1].T)  # (R, R)\n\n            # Plot stability of eigenvectors.\n            plt.sca(ax[int(aligned)][2 * j + dict(weight=0, activation=1)[quantity]])\n            plt.imshow(stab.T.cpu().numpy()[:50, :50], cmap=\"cividis\", vmin=0, vmax=1)\n            plt.colorbar(shrink=0.7)\n            plt.title(\n                f\"Cos sim of {'aligned ' if aligned else ''}{quantity} eigs (layer\"\n                f\" {j + 1})\"\n            )\n            plt.xlabel(\"Rank (network 1)\")\n            plt.ylabel(\"Rank (network 2)\")\n            ticks = np.asarray([1, 10, 20, 30, 40, 50])\n            plt.xticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n            plt.yticks(ticks=ticks - 1, labels=[str(i) for i in ticks])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\nAfter alignment, top eigenvectors strongly correlate across random initializations. We have seen that it is exactly these eigenvectors which matter for performance. The informative, learned weight and activation principal components are thus shared by both networks: they are universal!\n\n\nGenerating a new network without training\nDo the covariances really encode all the information needed to explain the performance of the network?\nWe can check by generating new, random weights which have the same covariance as that of a trained network. This is done by drawing the weights of each neuron independently from a Gaussian distribution. Note: the generated network should be aligned to the original network in order to use the same weight covariances.\nThe generation is iterative, layer by layer:\n\nDraw samples from \\mathcal N\\left(0, \\frac 1{n_1} W_1^{\\rm T} W_1\\right) for the weights W_1' of the first layer\nCompute the alignment A_1 so that A_1 \\sigma(W_1'x) \\approx \\sigma(W_1 x)\nDraw samples from \\mathcal N\\left(0, \\frac 1{n_2} (W_2 A_1)^{\\rm T} (W_2 A_1)\\right) for the weights W_2' of the second layer\nCompute the alignment A_2 so that A_2 \\sigma(W_2' \\sigma(W_1' x)) \\approx \\sigma(W_2 \\sigma(W_1 x))\nSet the classifier weights \\theta' = \\theta A_2\n\n\n\nSample a network using only covariance statistics\nstate_dict_keys = list(model.state_dict().keys())\nprint(f\"Generating {state_dict_keys}\")\nreference_accuracy, generated_accuracy = generate_network(\n    model, state_dict_keys, train_loader, val_loader\n)\nprint(\n    f\"Accuracy after sampling: {generated_accuracy:.1f}% (accuracy after training:\"\n    f\" {reference_accuracy:.1f}%)\"\n)\n\n\nGenerating ['1.weight', '3.weight', '5.weight']\nAccuracy after sampling: 33.7% (accuracy after training: 43.5%)\n\n\n\n\n\n\n\n\n\n\n\nWithout any training, the generated network recovers a good fraction of the performance of the trained network. It implies that the covariances indeed encode most of the task-relevant information learned during training.\n\n\nExtending the analysis to convolutional networks\nWe now consider a CNN, composed of two 5 \\times 5 convolutional layers and a fully-connected classifier.\n\n\n\nInitialize a convolutional neural network\nchannels = [1, 32, 64]\ncnn_model = initialize_cnn_model(channels)\nprint(cnn_model)\n\n\nSequential(\n  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(2, 2), bias=False)\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n  (3): ReLU()\n  (4): Flatten(start_dim=1, end_dim=-1)\n  (5): Linear(in_features=1600, out_features=10, bias=False)\n)\n\n\n\n\nTrain the convolutional neural network\ntrain_model(cnn_model, train_loader, val_loader, num_epochs=5)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequential(\n  (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(2, 2), bias=False)\n  (1): ReLU()\n  (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n  (3): ReLU()\n  (4): Flatten(start_dim=1, end_dim=-1)\n  (5): Linear(in_features=1600, out_features=10, bias=False)\n)\n\n\nIn CNNs, the weights are no longer matrices but fourth-order tensors, of shape C_{\\rm out} \\times C_{\\rm in} \\times k_{\\rm h} \\times k_{\\rm w}. This corresponds to n = C_{\\rm out} neurons which compute linear projections over patches, of dimension d = C_{\\rm out} \\times k_{\\rm h} \\times k_{\\rm w}. We can thus reshape the weight tensor as a n_1 \\times d matrix W_1, and treat it as the weight matrix of a fully-connected network as above. We can thus compute the weight covariance for the first layer as \\frac1{n_1} W_1^{\\rm T} W_1, which is a d \\times d matrix.\nweight = conv_layer.weight  # (C_out, C_in, k_h, k_w)\nweight_flat = weight.reshape((weight.shape[0], -1))  # (C_out, C_in*k_h*k_w)\nweight_cov = (\n    weight_flat.T @ weight_flat / weight_flat.shape[0]\n)  # (C_in*k_h*k_w, C_in*k_h*k_w)\nSimilarly, the activations are no longer vectors but third-order tensors, of shape C \\times H \\times W. Rather than reshaping them as vectors, we will only compute covariances over channels and treat the spatial dimensions as additional samples. This means we will align the channels of two representations, but not their spatial dimensions. This is due to the translation-equivariance of CNNs, which preserve the spatial dimensions. The arbitrariness of the neural code is thus only along the channel dimension.\nactivations = (\n    phi(x)  # (B, C, H, W)\n    .permute(0, 2, 3, 1)  # (B, H, W, C)\n    .flatten(start_dim=0, end_dim=2)  # (BHW, C)\n)\nactivations_cov = activations.T @ activations / activations.shape[0]  # (C, C)\n\n\nSample a CNN using only covariance statistics\nstate_dict_keys = list(cnn_model.state_dict().keys())\nprint(f\"Generating {state_dict_keys}\")\nreference_accuracy, generated_accuracy = generate_network(\n    cnn_model, state_dict_keys, train_loader, val_loader\n)\nprint(\n    f\"Accuracy after sampling: {generated_accuracy:.1f}% (accuracy after training:\"\n    f\" {reference_accuracy:.1f}%)\"\n)\n\n\nGenerating ['0.weight', '2.weight', '5.weight']\nAccuracy after sampling: 40.6% (accuracy after training: 57.8%)\n\n\n\n\n\n\n\n\n\n\n\nAbout two thirds of the performance of the trained network is captured in its covariances. Generating CNN weights with Gaussian distributions does not work as well because the spatial filters are not well-approximated by a Gaussian (due to selectivity in orientation and frequency).\nThe accuracy of the generation procedure can be improved by switching to an architecture which predefines the spatial filters, so that they are not learned nor randomly generated. This is achieved by learned scattering networks (Guth, Zarka, and Mallat 2021), which use wavelet spatial filters and learned weights along channels."
  },
  {
    "objectID": "pages/analyzing_neural_networks.html#further-reading",
    "href": "pages/analyzing_neural_networks.html#further-reading",
    "title": "Analyzing and comparing deep neural networks",
    "section": "Further reading",
    "text": "Further reading\nThe analyses described in this notebook are thoroughly characterized in Guth et al. (2023)."
  },
  {
    "objectID": "pages/dealing_with_noise.html",
    "href": "pages/dealing_with_noise.html",
    "title": "Dealing with noisy data",
    "section": "",
    "text": "Run this notebook interactively!\n\n\n\nHere’s a link to this notebook on Google Colab."
  },
  {
    "objectID": "pages/dealing_with_noise.html#neural-data-is-noisy",
    "href": "pages/dealing_with_noise.html#neural-data-is-noisy",
    "title": "Dealing with noisy data",
    "section": "Neural data is noisy",
    "text": "Neural data is noisy\nUnfortunately for us computational neuroscientists, experimental data is noisy. Let’s consider a neuron from the toy example we discussed earlier.\n\n\nInstall all required dependencies\n# TODO uncomment before final packaging\n# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n\n\n\n\nImport various libraries\nfrom collections.abc import Sequence\nimport functools\nimport warnings\nfrom typing import NamedTuple\n\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nfrom matplotlib.figure import Figure\nfrom matplotlib import pyplot as plt\nfrom matplotlib.colors import SymLogNorm\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nfrom utilities.brain import load_dataset\nfrom utilities.computation import assign_logarithmic_bins\nfrom utilities.toy_example import (\n    Neuron,\n    create_stimuli,\n    simulate_multiple_neuron_responses,\n    view_individual_scatter,\n)\n\n\n\n\nSet some visualization defaults\n%matplotlib inline\n\nsns.set_theme(\n    context=\"notebook\",\n    style=\"white\",\n    palette=\"deep\",\n    rc={\"legend.edgecolor\": \"None\"},\n)\nset_matplotlib_formats(\"svg\")\n\npd.set_option(\"display.max_rows\", 5)\npd.set_option(\"display.max_columns\", 10)\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.show_dimensions\", False)\n\nxr.set_options(display_max_rows=3, display_expand_data=False)\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nInitialize a deterministic random number generator\nrandom_state = 0\nrng = np.random.default_rng(seed=random_state)\n\n\nThe variance in its responses to the N = 1000 presented dots was driven by two factors:\n\nstimulus-dependent signal – how much the neural response varies with the stimulus features \\text{color} and \\text{size}, and\nstimulus-independent noise – sampled from the same Gaussian distribution irrespective of the stimulus.\n\n\n\nView signal/noise decomposition of an individual neuron’s responses\nstimuli = create_stimuli(n=1000, rng=rng)\n\nneurons = {\n    \"neuron response\": Neuron(beta_color=3, beta_size=-2, std=1, mean=7),\n    \"stimulus-dependent signal\": Neuron(beta_color=3, beta_size=-2, std=0, mean=7),\n    \"stimulus-independent noise\": Neuron(beta_color=0, beta_size=0, std=1, mean=7),\n}\n\ndata = simulate_multiple_neuron_responses(\n    stimuli=stimuli,\n    neurons=neurons.values(),\n    rng=np.random.default_rng(random_state),\n)\n\nview_individual_scatter(\n    data,\n    coord=\"neuron\",\n    dim=\"neuron\",\n    template_func=lambda x: f\"{list(neurons.keys())[x - 1]}\",\n)\n\n\n\n\n\n\n\n\n\n\n\nCompute variances\nvariances = np.round(data.var(\"stimulus\").values, 3)\nprint(f\"total variance: {variances[0]}\")\nprint(f\"stimulus-dependent signal variance: {variances[1]}\")\nprint(f\"stimulus-independent noise variance: {variances[2]}\")\n\n\ntotal variance: 14.245\nstimulus-dependent signal variance: 13.106\nstimulus-independent noise variance: 0.964\n\n\nWhen investigating a sensory system, we are typically interested in the former – we want to understand the robust, reproducible portion of the system’s behavior in response to stimuli. The remaining variance is often considered nuisance variance.\n\n\n\n\n\n\nSome sources of variation in experimental data\n\n\n\n\n\nIn general, experimental data in neuroscience contain many sources of variation, some intrinsic to the system and others dependent on our experimental techniques.\n\nIntrinsic stochasticity\n\nThe spiking of neurons is inherently stochastic. Recording responses from a single neuron to the same stimuli would result in different spike trains. Often, however, we compute summary statistics that abstract over the precise timing of spikes, assuming that the neural representation depends only on average firing rates.\n\nArousal\n\nThe level of arousal of participants in an experiment modulates their neural responses. For example, if a participant drinks coffee in the morning before a long scanning session, we’d expect to measure different neural responses.\n\nRepresentational drift\n\nThe representations used by an animal could vary gradually over time. For example, several brain-computer interfaces – e.g. where neural activity is used to actuate a mechanical system – often need to be recalibrated frequently to account for short- to medium-term changes in the representational space.\n\nMeasurement noise\n\nEvery empirical measurement of a neural signal is subject to errors in the experimental procedure that cause the measured response to be different from the “true” response. For example, this could be caused by scanner drift in fMRI, or poor electrode scalp contact in EEG.\n\nStimulus-dependent signal\n\nFinally, the source of variation we are usually most interested in is the stimulus-specific signal – how the neural response changes with the stimulus. This is often called neural tuning – e.g. neurons in the fusiform face area are described as “tuned for faces, but not houses”.\n\n\n\n\n\nIn the previous notebook, we observed that neural responses have high-dimensional structure, as evidenced by the covariance spectrum obtained from principal component analysis.\nHowever, this covariance spectrum is agnostic to the sources of variation in the data: it considers variance due to signal and noise to be identical. How can we use the spectrum to separate signal from noise?\n\n\n\n\n\n\nWhat does noise look like?\n\n\n\n\n\nLet’s take a brief detour and investigate a system that is “pure noise”: a random matrix.\n\nCovariance structure of random matrices\nLet’s consider a simple system: a random matrix X \\in \\mathbb{R}^{N \\times P}, with entries drawn independently from a fixed Gaussian distribution \\mathcal{N}(0, \\sigma^2).\n\n\nCreate a random matrix\nn_stimuli = 500\nn_neurons = 500\n\nrandom_matrix = rng.standard_normal((n_stimuli, n_neurons))\n\n\ndef view_random_matrix(data: np.ndarray, /) -&gt; Figure:\n    fig, ax = plt.subplots()\n    image = ax.imshow(\n        data, cmap=\"Spectral\", norm=SymLogNorm(linthresh=1e-2, linscale=1e-1)\n    )\n    fig.colorbar(image)\n    sns.despine(ax=ax, left=True, bottom=True)\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n    plt.close(fig)\n\n    return fig\n\n\nview_random_matrix(random_matrix)\n\n\n\n\n\n\n\n\n\nWe might expect that a random matrix ought to have no covariance structure, since all the entries were sampled independently. However, this isn’t the case! In fact, random matrices have well-defined covariance structure that is described by the Marchenko-Pastur distribution.\n\n\nVisualize the covariance eigenspectrum of the random matrix\ndef view_eigenspectrum(data: np.ndarray, *, log: bool = False) -&gt; None:\n    data -= data.mean(axis=-2, keepdims=True)\n\n    singular_values = np.linalg.svd(data, compute_uv=False)\n    eigenvalues = (\n        xr.DataArray(\n            name=\"eigenvalue\",\n            data=singular_values**2 / (n_stimuli - 1),\n            dims=(\"rank\",),\n            coords={\"rank\": (\"rank\", 1 + np.arange(singular_values.shape[-1]))},\n        )\n        .to_dataframe()\n        .reset_index()\n    )\n\n    fig, ax = plt.subplots()\n    sns.lineplot(\n        ax=ax,\n        data=eigenvalues,\n        x=\"rank\",\n        y=\"eigenvalue\",\n        estimator=\"mean\",\n        errorbar=\"sd\",\n        err_style=\"band\",\n    )\n    if log:\n        ax.set_xscale(\"log\")\n        ax.set_yscale(\"log\")\n        ax.set_ylim(bottom=1e-2)\n        ax.set_aspect(\"equal\", \"box\")\n\n    sns.despine(ax=ax, offset=20)\n    fig.show()\n\n\nview_eigenspectrum(random_matrix)\n\n\n\n\n\n\n\n\n\nLet’s view the spectrum on a logarithmic scale to see what it looks like.\n\n\nRe-plot the covariance eigenspectrum on a log-log scale\nview_eigenspectrum(random_matrix, log=True)\n\n\n\n\n\n\n\n\n\nEven random matrices have systematic covariance structure that result in non-zero eigenvalues at all ranks. How can we know that a region of our spectrum is driven by signal and not noise?"
  },
  {
    "objectID": "pages/dealing_with_noise.html#decomposing-a-covariance-eigenspectrum",
    "href": "pages/dealing_with_noise.html#decomposing-a-covariance-eigenspectrum",
    "title": "Dealing with noisy data",
    "section": "Decomposing a covariance eigenspectrum",
    "text": "Decomposing a covariance eigenspectrum\nOne of the most common denoising procedures is variance-dependent dimensionality reduction: retaining only the first few high-variance principal components of the data. The implicit assumption here is that high-variance dimensions correspond to signal in the system, while low-variance dimensions represent noise. In this section, we’ll investigate this assumption and demonstrate that this isn’t always true!\nTODO add figure showing two ways to decompose a covariance eigenspectrum into signal and noise\nFor example, consider a very realistic scenario where a participant gets bored performing our experiment inside an fMRI scanner and starts yodeling to entertain themselves. The highest variance components in the measured neural response would likely be motion artifacts and not whatever signal we were interested in measuring. A high-quality data preprocessing pipeline would help mitigate such extreme components of variance – but cannot remove them entirely!\nIn fact, as we saw when we inspected the first couple of latent dimensions in our toy example, the principal components didn’t correspond directly to the latent variables generating the data; rather, they were a mixture of stimulus-dependent variance and nuisance variance.\nThis suggests that using principal component analysis as a dimensionality reduction tool by setting an arbitrary variance threshold is likely too stringent a criterion: there is possible low-variance signal along the many neglected dimensions in the tail!\nClearly, we need a different approach to using the spectrum to separate signal and noise."
  },
  {
    "objectID": "pages/dealing_with_noise.html#cross-validated-pca",
    "href": "pages/dealing_with_noise.html#cross-validated-pca",
    "title": "Dealing with noisy data",
    "section": "Cross-validated PCA",
    "text": "Cross-validated PCA\nIn a whole-brain calcium-imaging study, Stringer et al. (2019) recorded the responses of 10,000 neurons in mouse primary visual cortex to 2,800 natural images.\n\n\n\n\nExperimental setup for the calcium-imaging study – Fig 1 in Stringer et al. (2019)\n\n\nArmed with this large dataset, they set out to develop a method that could reliably estimate the covariance structure of these neural responses: cross-validated PCA.\nSince most neuroscience experiments collect multiple responses to the same stimulus across different trials, Stringer et al. (2019) decided to use one set of responses as a training set and other as a test set when computing the covariance eigenspectrum.\nSpecifically, if there were N unique stimuli – each seen twice – the data matrix X \\in \\mathbb{R}^{2N \\times P} is split into two matrices X_\\text{train} \\in \\mathbb{R}^{N \\times P} and X_\\text{test} \\in \\mathbb{R}^{N \\times P}, where the rows of X_\\text{train} and X_\\text{test} correspond to the same stimuli.\n\n\n\n\n\n\nStep 1 – Compute eigenvectors on the training set\nThe first step is to compute the principal components – the eigenvectors of the covariance matrix – of the training set:\nAs usual, X_\\text{train} must be centered when computing its covariance.\n\n\\begin{align*}\n    \\text{cov}(X_\\text{train}, X_\\text{train})\n    &= X_\\text{train}^\\top X_\\text{train} / (n - 1)\\\\\n    &= V \\Lambda V^\\top\n\\end{align*}\n\n\n\n\n\n\nStep 2 – Compute cross-validated eigenvalues\nThe second step is to compute cross-validated eigenvalues by projecting both the training and the test sets onto the eigenvectors from Step 1, and computing their cross-covariance:\nBoth X_\\text{train} and X_\\text{test} are centered prior to the projection using the mean of X_\\text{train}.\n\n\\begin{align*}\n    \\Lambda_\\text{cross-validated}\n    &= \\text{cov}(X_\\text{train}V, X_\\text{test}V)\\\\\n    &= \\left( X_\\text{train} V \\right) ^\\top \\left( X_\\text{test} V \\right) / (n - 1)\n\\end{align*}\n\n\n\n\nThese cross-validated eigenvalues represent the covariance reliably shared across two presentations of the visual stimulus – the “stable” part of the visual representation of a natural image.\n\n\n\n\n\n\nWarning\n\n\n\nThese cross-validated “eigenvalues” need not be positive: if there is no shared covariance between the two systems along a particular eigenvector, the expected value of the eigenvalue is 0. This makes interpretation simple: if there is reliable variance along a dimension, its cross-validated eigenvalue will be significantly above zero.\n\n\n\n\nSome geometric intuition\nTODO add figure showing two sets of data, showing reliable covariance in one dimension but not the other\n\n\nA computational recipe\n\n\nA computational recipe for cross-validated PCA\nclass CrossValidatedPCA:\n    def __init__(self) -&gt; None:\n        return\n\n    def __call__(self, /, x: np.ndarray, y: np.ndarray) -&gt; np.ndarray:\n        self.pca = PCA()\n        self.pca.fit(x)\n        x_transformed = self.pca.transform(x)\n        y_transformed = self.pca.transform(y)\n\n        cross_covariance = np.cov(\n            x_transformed,\n            y_transformed,\n            rowvar=False,\n        )\n\n        self.cross_validated_spectrum = np.diag(\n            cross_covariance[: self.pca.n_components_, self.pca.n_components_ :]\n        )"
  },
  {
    "objectID": "pages/dealing_with_noise.html#investigating-neural-data",
    "href": "pages/dealing_with_noise.html#investigating-neural-data",
    "title": "Dealing with noisy data",
    "section": "Investigating neural data",
    "text": "Investigating neural data\nLet’s apply CV-PCA to our fMRI data to see what it looks like!\n\n\nLoad the dataset\ndata = load_dataset(subject=0, roi=\"general\").load()\n\ndisplay(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'fMRI betas' (presentation: 1400, neuroid: 15724)&gt;\n-0.2375 -0.4001 -0.7933 0.04382 -0.1157 ... 0.2669 -1.051 -0.179 0.03348 -0.2664\nCoordinates: (3/8)\n    x            (neuroid) uint8 12 12 12 12 12 12 12 ... 71 72 72 72 72 72 72\n    y            (neuroid) uint8 21 22 22 22 22 22 23 ... 34 29 29 30 30 30 31\n    ...           ...\n    rep_id       (presentation) uint8 0 0 0 0 0 0 1 0 0 0 ... 0 0 1 1 1 1 1 1 1\nDimensions without coordinates: presentation, neuroid\nAttributes: (3/7)\n    resolution:     1pt8mm\n    preprocessing:  fithrf_GLMdenoise_RR\n    ...             ...\n    citation:       Allen, E.J., St-Yves, G., Wu, Y. et al. A massive 7T fMRI...xarray.DataArray'fMRI betas'presentation: 1400neuroid: 15724-0.2375 -0.4001 -0.7933 0.04382 ... -1.051 -0.179 0.03348 -0.2664array([[-0.23748732, -0.4000817 , -0.79332644, ...,  0.34429136,\n        -0.8983028 , -1.1036999 ],\n       [ 0.60243624, -1.5812274 , -1.0527056 , ..., -0.63479644,\n        -0.54893786,  0.38300768],\n       [ 0.12001861,  1.5943868 ,  0.11450057, ...,  0.36619487,\n         0.02145388,  0.9037204 ],\n       ...,\n       [ 0.4122271 , -1.9848806 , -1.4696951 , ...,  1.1890962 ,\n         0.10346738, -0.18423374],\n       [-0.32920593,  0.44030157,  0.02951132, ...,  1.1662561 ,\n         1.2881701 ,  0.96182734],\n       [-0.04278437, -0.8031657 , -0.45526204, ..., -0.17902693,\n         0.03348494, -0.26644066]], dtype=float32)Coordinates: (8)x(neuroid)uint812 12 12 12 12 ... 72 72 72 72 72array([12, 12, 12, ..., 72, 72, 72], dtype=uint8)y(neuroid)uint821 22 22 22 22 ... 29 30 30 30 31array([21, 22, 22, ..., 30, 30, 31], dtype=uint8)z(neuroid)uint847 45 46 47 48 ... 46 45 46 49 49array([47, 45, 46, ..., 46, 49, 49], dtype=uint8)stimulus_id(presentation)object'image46002' ... 'image52596'array(['image46002', 'image48617', 'image44980', ..., 'image34186',\n       'image45356', 'image52596'], dtype=object)session_id(presentation)uint80 0 0 0 0 0 0 ... 26 26 26 26 26 26array([ 0,  0,  0, ..., 26, 26, 26], dtype=uint8)trial(presentation)uint160 28 35 44 55 ... 681 684 714 744array([  0,  28,  35, ..., 684, 714, 744], dtype=uint16)run_id(presentation)uint80 0 0 0 0 0 1 ... 9 10 10 10 11 11array([ 0,  0,  0, ..., 10, 11, 11], dtype=uint8)rep_id(presentation)uint80 0 0 0 0 0 1 0 ... 0 1 1 1 1 1 1 1array([0, 0, 0, ..., 1, 1, 1], dtype=uint8)Indexes: (0)Attributes: (7)resolution :1pt8mmpreprocessing :fithrf_GLMdenoise_RRz_score :Trueroi :generalsubject :0brain shape :[ 81 104  83]citation :Allen, E.J., St-Yves, G., Wu, Y. et al. A massive 7T fMRI dataset to bridge cognitive neuroscience and artificial intelligence. Nat Neurosci 25, 116-126 (2022). https://doi.org/10.1038/s41593-021-00962-x\n\n\nNote that the data contain fMRI responses to two repetitions of N = 700 images for a total of 2N = 1400 presentations.\n\n\nCompute the cross-validated spectrum\ndata_repetition_1 = data.isel({\"presentation\": data[\"rep_id\"] == 0}).sortby(\n    \"stimulus_id\"\n)\ndata_repetition_2 = data.isel({\"presentation\": data[\"rep_id\"] == 1}).sortby(\n    \"stimulus_id\"\n)\n\ncv_pca = CrossValidatedPCA()\ncv_pca(data_repetition_1.values, data_repetition_2.values)\n\n\n\n\nPlot the raw cross-validated spectrum\ndef plot_cross_validated_spectrum(\n    cv_pca: CrossValidatedPCA,\n    *,\n    log: bool = False,\n    original: bool = False,\n    square: bool = False,\n    bin_logarithmically: bool = False,\n) -&gt; Figure:\n    fig, ax = plt.subplots()\n\n    data = pd.DataFrame(\n        {\n            \"rank\": 1 + np.arange(len(cv_pca.cross_validated_spectrum) - 1),\n            \"cross-validated\": cv_pca.cross_validated_spectrum[:-1],\n            \"original\": cv_pca.pca.explained_variance_[:-1],\n        }\n    ).melt(\n        id_vars=[\"rank\"],\n        value_vars=[\"cross-validated\", \"original\"],\n        value_name=\"eigenvalue\",\n        var_name=\"spectrum\",\n    )\n\n    if original:\n        sns.lineplot(\n            ax=ax,\n            data=data,\n            x=\"rank\",\n            y=\"eigenvalue\",\n            hue=\"spectrum\",\n        )\n    else:\n        data = (\n            data.loc[data[\"spectrum\"] == \"cross-validated\"]\n            .rename(columns={\"eigenvalue\": \"cross-validated eigenvalue\"})\n            .drop(columns=\"spectrum\")\n        )\n\n        if bin_logarithmically:\n            data[\"rank\"] = assign_logarithmic_bins(\n                data[\"rank\"], points_per_bin=5, min_=1, max_=10_000\n            )\n\n            sns.lineplot(\n                ax=ax,\n                data=data,\n                x=\"rank\",\n                y=\"cross-validated eigenvalue\",\n                marker=\"o\",\n                dashes=False,\n                ls=\"None\",\n                err_style=\"bars\",\n                estimator=\"mean\",\n                errorbar=\"sd\",\n            )\n        else:\n            sns.lineplot(\n                ax=ax,\n                data=data,\n                x=\"rank\",\n                y=\"cross-validated eigenvalue\",\n            )\n\n    if log:\n        ax.axhline(0, ls=\"--\", c=\"gray\")\n        ax.set_xlim(left=1)\n        ax.set_ylim(bottom=1e-2)\n        ax.set_xscale(\"log\")\n        ax.set_yscale(\"log\")\n\n        if square:\n            ax.axis(\"square\")\n\n    sns.despine(ax=ax, offset=20)\n    plt.close(fig)\n\n    return fig\n\n\nplot_cross_validated_spectrum(cv_pca)\n\n\n\n\n\n\n\n\n\nWe can see a typical spectrum that appears to have a knee somewhere around 10 dimensions, beyond which the variance quickly flattens out close to zero.\nLet’s re-plot this cross-validated covariance eigenspectrum on a logarithmic scale.\n\n\nPlot the cross-validated spectrum on a log-log scale\nplot_cross_validated_spectrum(cv_pca, log=True)\n\n\n\n\n\n\n\n\n\nHere, we see that the apparent knee isn’t present – in fact, the spectrum appears to obey a power law at all ranks! Additionally, since these eigenvalues were obtained from a cross-validated analysis, we know that they represent stimulus-related variance that is consistent across different presentations of the same stimuli and not noise.\nTo verify this, let’s plot the covariance spectrum of the training dataset, the one that we would obtain from regular principal components analysis.\n\n\nPlot both the original and the cross-validated spectrum\nplot_cross_validated_spectrum(cv_pca, log=True, original=True)\n\n\n\n\n\n\n\n\n\nThe covariance eigenspectrum of the training data (orange) is higher than the cross-validated spectrum (blue) at all ranks – suggesting that the process is indeed removing trial-specific noise.\n\n\n\n\n\n\nThe neural population code is high-dimensional!\n\n\n\nImportantly, note that after removing trial-specific noise using cross-validated PCA, the covariance eigenspectrum retains its power-law structure at all ranks! This suggests that the neural population code is truly high-dimensional.\n\n\nFinally, let’s re-plot the cross-validated spectrum in a cleaner fashion, where the trends of interest are more clearly visible.\n\n\nBin the cross-validated spectrum\nwith sns.axes_style(\"whitegrid\"):\n    fig = plot_cross_validated_spectrum(\n        cv_pca, log=True, bin_logarithmically=True, square=True\n    )\n    ax = fig.get_axes()[0]\n    ax.grid(True, which=\"minor\", c=\"whitesmoke\")\n    ax.grid(True, which=\"major\", c=\"lightgray\")\n    for loc in (\"left\", \"bottom\"):\n        ax.spines[loc].set_visible(False)\n    display(fig)\n\n\n\n\n\n\n\n\n\nHere, we’ve averaged the spectrum in local bins that expand exponentially in size (and thus appear uniformly spaced on the logarithmic scale). The individual points denote the mean cross-validated eigenvalue in each bin while the error bars denote the standard deviation of these eigenvalues.\nAveraging within such bins allows us to detect signal even in a regime where the cross-validated spectrum oscillates wildly near zero and also visualize the power law more cleanly."
  },
  {
    "objectID": "pages/dealing_with_noise.html#a-universal-power-law-exponent",
    "href": "pages/dealing_with_noise.html#a-universal-power-law-exponent",
    "title": "Dealing with noisy data",
    "section": "A universal power-law exponent",
    "text": "A universal power-law exponent\nThe power-law exponent of the neural data appears very close to -1, as you can see from the slope of the cross-validated spectrum Interestingly, a power-law exponent of -1 appears with surprising regularity in neuroscience.\n\nMouse primary visual cortex\nStringer et al. (2019), who developed cross-validated PCA, discovered that mouse primary visual cortex responses obey a power-law with exponent -1 over several orders of magnitude.\n\n\n\nSimilar eigenspectra in mouse visual cortex – Fig 2 in Stringer et al. (2019)\n\n\n\n\nZebrafish brain\nMore recently, Wang et al. (2023) reported a similar result from whole-brain calcium recordings in zebrafish during hunting and spontaneous behavior.\n\n\n\nSimilar eigenspectra in zebrafish whole-brain Ca2+-imaging – Fig 1 in Wang et al. (2023)\n\n\n\n\nDeep neural networks\nInterestingly, several recent results from the machine learning literature also corroborate this result!\nSpecifically, Agrawal et al. (2022) report that neural networks whose internal representations have a covariance spectrum that decays as a power law with index -1 perform better. This has made the power law index a statistic of interest for assessing model representation quality – and perhaps a target of direct optimization.\n\n\n\nDeep networks with similar covariance spectra perform better – Fig 3 in Agrawal et al. (2022)\n\n\nIn addition, Kong et al. (2022) demonstrated that more adversarially robust neural networks are a better match for macaque V1 eigenspectra.\n\n\n\nDeep networks with similar covariance spectra appear more adversarially robust – Fig 6 in Kong et al. (2022)\n\n\n\n\nHuman visual cortex!\nIn this notebook, we have demonstrated that a similar scale-invariant covariance structure underlies human visual representations of natural scenes too!\n\n\nPlot the cross-validated covariance spectrum for human visual cortex\nwith sns.axes_style(\"whitegrid\"):\n    display(fig)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs the power-law exponent of -1 universal?\n\n\n\nDo we observe the same power-law covariance spectra in other organisms? Could it be a universal statistical property of neural population codes?\nCheck your data!\n\n\n\n\n\n\n\n\nIs high-dimensional coding a hallmark of sensory systems?\n\n\n\nDo all sensory systems use a high-dimensional population code – where an expressive representation of the outside world might allow rapid learning and generalization to various ethological tasks? Perhaps more cognitive systems might use low-dimensional representational formats to enhance robustness and invariance to irrelevant features.\n\n\n\n\n\n\n\n\nWhat makes \\text{variance} \\propto \\text{rank}^{-1} special?\n\n\n\n\n\nA power-law index of -1 implies a special sort of scale-invariance. Consider the total variance captured by such a covariance eigenspectrum within a range of ranks [\\alpha, \\beta]:\n\n\\begin{align*}\n    \\int_\\alpha^\\beta x^{-1} dx &= \\ln{\\left\\lvert\\frac{\\beta}{\\alpha}\\right\\rvert}\n\\end{align*}\n\nThis total variance only depends on the ratio \\beta / \\alpha – indicating that a power law with a -1 slope has equal variance in each decade: i.e., the total variance from rank 1 to rank 10 is equal to the total variance from rank 100 to rank 1000.\nThis suggests that each decade is equally important and contributes as much to the representation, though the later decades have their variance spread out over many more dimensions.\n\n\n\n\n\n\n\n\n\nAn optimality criterion for population codes?\n\n\n\n\n\nStringer et al. (2019) provide a possible explanation for why systems might tend toward a -1 covariance eigenspectrum.\nFor sufficiently high-dimensional data, if a spectrum decays any slower than -1, the representational manifold becomes non-smooth: small changes in the input might lead to large changes in the representational space – clearly an undesirable property for stable perception and cognition.\nIn contrast, if a spectrum decays significantly faster than -1, it loses some expressivity by not utilizing all the dimensions it has available."
  },
  {
    "objectID": "pages/dealing_with_noise.html#summary",
    "href": "pages/dealing_with_noise.html#summary",
    "title": "Dealing with noisy data",
    "section": "Summary",
    "text": "Summary\nOur simple demonstrations in this notebook suggest a profound difference in the way we should think about population coding.\nNeural data is not reducible to a small number of interpretable, high-variance dimensions. Much like a text cannot be understood using only the most frequent words, understanding a population code must involve studying reliable information along all dimensions.\nMoreover, neural data is high-dimensional to an extent previously unappreciated. In fact, the power-law scaling observed doesn’t saturate at the tail of the spectrum, suggesting that with even larger datasets with more stimuli, we might find reliable stimulus-specific variance along even more dimensions!"
  },
  {
    "objectID": "pages/introducing_pca.html",
    "href": "pages/introducing_pca.html",
    "title": "An introduction to PCA",
    "section": "",
    "text": "Run this notebook interactively!\n\n\n\nHere’s a link to this notebook on Google Colab."
  },
  {
    "objectID": "pages/introducing_pca.html#a-simple-experiment",
    "href": "pages/introducing_pca.html#a-simple-experiment",
    "title": "An introduction to PCA",
    "section": "A simple experiment",
    "text": "A simple experiment\nLet’s perform an imaginary neuroscience experiment! We’ll record voltages from P = 2 neurons in visual cortex while the participant passively views N = 1000 dots of different colors and sizes.\n\n\nInstall all required dependencies\n# TODO uncomment before final packaging\n# %pip install git+https://github.com/BonnerLab/ccn-tutorial.git\n\n\n\n\nImport various libraries\nfrom collections.abc import Sequence, Callable\nimport warnings\nfrom typing import NamedTuple\n\nimport numpy as np\nimport pandas as pd\nimport xarray as xr\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom matplotlib.figure import Figure\nimport matplotlib.ticker as ticker\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML\n\n\n\n\nImport utility functions\nfrom utilities.toy_example import (\n    create_stimuli,\n    Neuron,\n    simulate_neuron_responses,\n    simulate_multiple_neuron_responses,\n    view_individual_scatter,\n)\n\n\n\n\nSet some visualization defaults\n%matplotlib inline\n\nsns.set_theme(\n    context=\"notebook\",\n    style=\"white\",\n    palette=\"deep\",\n    rc={\"legend.edgecolor\": \"None\"},\n)\nset_matplotlib_formats(\"svg\")\n\npd.set_option(\"display.max_rows\", 5)\npd.set_option(\"display.max_columns\", 10)\npd.set_option(\"display.precision\", 3)\npd.set_option(\"display.show_dimensions\", False)\n\nxr.set_options(display_max_rows=3, display_expand_data=False)\n\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nInitialize a deterministic random number generator\nrandom_state = 0\nrng = np.random.default_rng(seed=random_state)\n\n\n\nCreating the stimuli\nLet’s create N = 1000 dots of different colors and sizes. From the scatterplot, we can see that the two latent variables are uncorrelated.\n\n\nCode\ndef create_stimuli(\n    n: int,\n    *,\n    rng: np.random.Generator,\n) -&gt; pd.DataFrame:\n    return pd.DataFrame(\n        {\n            \"color\": rng.random(size=(n,)),\n            \"size\": rng.random(size=(n,)),\n        }\n    ).set_index(1 + np.arange(n))\n\n\n\n\nVisualize the stimuli\ndef view_stimuli(data: pd.DataFrame) -&gt; Figure:\n    fig, ax = plt.subplots()\n    sns.scatterplot(\n        ax=ax,\n        data=data,\n        x=\"color\",\n        y=\"size\",\n        hue=\"color\",\n        size=\"size\",\n        palette=\"flare\",\n        legend=False,\n    )\n    sns.despine(ax=ax, trim=True)\n    fig.tight_layout()\n    plt.close(fig)\n\n    return fig\n\n\nstimuli = create_stimuli(n=1_000, rng=rng)\n\nview_stimuli(stimuli)\n\n\n\n\n\n\n\n\n\n\n\nSimulating neural responses\nNow, let’s simulate some neural data. We need to decide how the P = 2 neurons might respond to these N = 1000 stimulus dots. Each neuron could respond to either one or both of the latent features that define these stimuli – \\text{color} and \\text{size}. The neuron’s responses could also be subject to noise.\nHere, we model each neuron’s response r_\\text{neuron} as a simple linear combination of the two latent features with stimulus-independent Gaussian noise \\epsilon:\nr_{\\text{neuron}} \\sim \\beta_{\\text{color}} \\left( \\text{color} \\right) + \\beta_{\\text{size}} \\left( \\text{size} \\right) + \\epsilon, where \\epsilon \\sim \\mathcal{N}(\\mu_{\\text{neuron}}, \\sigma_{\\text{neuron}}^2)\n\n\nCode\nNeuron = NamedTuple(\n    \"Neuron\",\n    beta_color=float,\n    beta_size=float,\n    mean=float,\n    std=float,\n)\n\n\nAs we can see, each neuron’s response is completely defined by exactly four parameters:\n\n\\beta_{\\text{color}} – how much the neuron cares about the stimulus \\text{color}\n\\beta_{\\text{size}} – how much the neuron cares about the stimulus \\text{size}\n\\mu_{\\text{neuron}} – the mean of the neuron’s responses\n\\sigma_{\\text{neuron}}^2 – the stimulus-independent variance of the neuron’s responses\n\n\n\nCode\ndef simulate_neuron_responses(\n    stimuli: pd.DataFrame,\n    neuron: Neuron,\n    *,\n    rng: np.random.Generator,\n) -&gt; np.ndarray:\n    def z_score(x: np.ndarray) -&gt; np.ndarray:\n        return (x - x.mean()) / x.std()\n\n    return (\n        neuron.beta_color * z_score(stimuli[\"color\"])\n        + neuron.beta_size * z_score(stimuli[\"size\"])\n        + neuron.std * rng.standard_normal(size=(len(stimuli),))\n        + neuron.mean\n    )\n\n\n\n\nCode\ndef simulate_multiple_neuron_responses(\n    *,\n    stimuli: pd.DataFrame,\n    neurons: Sequence[Neuron],\n    rng: np.random.Generator,\n) -&gt; xr.DataArray:\n    data = []\n    for i_neuron, neuron in enumerate(neurons):\n        data.append(\n            xr.DataArray(\n                data=simulate_neuron_responses(\n                    stimuli=stimuli,\n                    neuron=neuron,\n                    rng=rng,\n                ),\n                dims=(\"stimulus\",),\n                coords={\n                    column: (\"stimulus\", values)\n                    for column, values in stimuli.reset_index(names=\"stimulus\").items()\n                },\n            )\n            .expand_dims({\"neuron\": [i_neuron + 1]})\n            .assign_coords(\n                {\n                    field: (\"neuron\", [float(value)])\n                    for field, value in neuron._asdict().items()\n                }\n            )\n        )\n\n    return (\n        xr.concat(data, dim=\"neuron\")\n        .rename(\"neuron responses\")\n        .transpose(\"stimulus\", \"neuron\")\n    )\n\n\nThis procedure produces a data matrix X \\in \\mathbb{R}^{N \\times P} containing the P = 2 neurons’ responses to the N = 1000 stimuli.\n\n\nSimulate the responses of two neurons to the stimuli\nneurons = (\n    Neuron(beta_color=3, beta_size=-2, std=1, mean=7),\n    Neuron(beta_color=-2, beta_size=5, std=3, mean=-6),\n)\n\ndata = simulate_multiple_neuron_responses(\n    stimuli=stimuli,\n    neurons=neurons,\n    rng=rng,\n)\n\ndisplay(data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'neuron responses' (stimulus: 1000, neuron: 2)&gt;\n12.97 -15.39 1.005 3.224 1.226 2.431 ... 6.028 -6.32 5.488 -16.32 6.215 -9.989\nCoordinates: (3/8)\n  * neuron      (neuron) int64 1 2\n  * stimulus    (stimulus) int64 1 2 3 4 5 6 7 ... 994 995 996 997 998 999 1000\n    ...          ...\n    std         (neuron) float64 1.0 3.0xarray.DataArray'neuron responses'stimulus: 1000neuron: 212.97 -15.39 1.005 3.224 1.226 ... -6.32 5.488 -16.32 6.215 -9.989array([[ 12.96533678, -15.38921968],\n       [  1.00523852,   3.22448895],\n       [  1.2258057 ,   2.43055775],\n       ...,\n       [  6.02810356,  -6.32031526],\n       [  5.48782478, -16.32351293],\n       [  6.21490603,  -9.98853112]])Coordinates: (8)neuron(neuron)int641 2array([1, 2])stimulus(stimulus)int641 2 3 4 5 ... 996 997 998 999 1000array([   1,    2,    3, ...,  998,  999, 1000])color(stimulus)float640.637 0.2698 ... 0.1649 0.38array([6.36961687e-01, 2.69786714e-01, 4.09735239e-02, 1.65276355e-02,\n       8.13270239e-01, 9.12755577e-01, 6.06635776e-01, 7.29496561e-01,\n       5.43624991e-01, 9.35072424e-01, 8.15853554e-01, 2.73850017e-03,\n       8.57404277e-01, 3.35855753e-02, 7.29655446e-01, 1.75655621e-01,\n       8.63178922e-01, 5.41461220e-01, 2.99711891e-01, 4.22687221e-01,\n       2.83196711e-02, 1.24283276e-01, 6.70624415e-01, 6.47189512e-01,\n       6.15385111e-01, 3.83677554e-01, 9.97209936e-01, 9.80835339e-01,\n       6.85541984e-01, 6.50459276e-01, 6.88446731e-01, 3.88921424e-01,\n       1.35096505e-01, 7.21488340e-01, 5.25354322e-01, 3.10241876e-01,\n       4.85835359e-01, 8.89487834e-01, 9.34043516e-01, 3.57795197e-01,\n       5.71529831e-01, 3.21869391e-01, 5.94300030e-01, 3.37911226e-01,\n       3.91619001e-01, 8.90274352e-01, 2.27157594e-01, 6.23187145e-01,\n       8.40153436e-02, 8.32644148e-01, 7.87098307e-01, 2.39369443e-01,\n       8.76484231e-01, 5.85680348e-02, 3.36117061e-01, 1.50279467e-01,\n       4.50339367e-01, 7.96324270e-01, 2.30642209e-01, 5.20213011e-02,\n       4.04551840e-01, 1.98513045e-01, 9.07530456e-02, 5.80332386e-01,\n       2.98696133e-01, 6.71994878e-01, 1.99515444e-01, 9.42113111e-01,\n       3.65110168e-01, 1.05495280e-01, 6.29108152e-01, 9.27154553e-01,\n       4.40377155e-01, 9.54590494e-01, 4.99895814e-01, 4.25228625e-01,\n       6.20213452e-01, 9.95096505e-01, 9.48943675e-01, 4.60045139e-01,\n...\n       3.39811155e-01, 2.21699710e-04, 4.82537622e-01, 6.08000665e-01,\n       9.29904601e-02, 2.42094402e-01, 8.03991821e-01, 8.40281560e-01,\n       3.87733254e-01, 8.14223731e-01, 2.77140253e-01, 7.06108222e-01,\n       5.45456624e-01, 4.40099079e-01, 6.56442276e-01, 1.33906791e-02,\n       1.62443443e-01, 2.93823464e-01, 6.80562610e-01, 7.06235313e-01,\n       6.80760824e-01, 7.67617107e-01, 7.95515610e-02, 1.05888908e-01,\n       8.55351160e-01, 3.56837753e-01, 5.68371290e-01, 5.03502806e-01,\n       6.26662571e-01, 7.69467112e-02, 7.69790226e-01, 1.23402328e-01,\n       6.81374462e-01, 4.02142099e-01, 4.92261636e-01, 6.71693734e-01,\n       3.71002751e-01, 4.60374714e-02, 9.64211552e-01, 5.22676899e-01,\n       7.42144641e-01, 5.31294834e-01, 8.19686903e-01, 5.64616179e-01,\n       1.22756878e-01, 6.41906565e-01, 1.72740669e-01, 8.23654135e-01,\n       6.81061600e-01, 9.39808638e-01, 6.29080790e-01, 2.25163097e-01,\n       5.57137546e-01, 7.71772277e-01, 7.11888298e-01, 3.42296706e-01,\n       6.55351151e-01, 9.35269061e-01, 6.84810043e-01, 3.67301402e-01,\n       9.10758330e-01, 8.27624193e-01, 8.55183760e-01, 1.06841377e-01,\n       2.90828834e-01, 7.90127803e-01, 2.74807496e-01, 7.37059356e-02,\n       6.83266012e-01, 7.99269956e-01, 6.41767808e-01, 3.44843336e-01,\n       5.59773319e-01, 2.15199514e-02, 5.62661656e-01, 8.56801175e-01,\n       7.80532350e-02, 3.83319397e-01, 1.64863718e-01, 3.80007897e-01])size(stimulus)float640.01301 0.8278 ... 0.08158 0.3216array([0.01300767, 0.82776292, 0.49624331, 0.43591807, 0.60179472,\n       0.8500282 , 0.29126072, 0.26751697, 0.04949421, 0.26639909,\n       0.06621185, 0.04155849, 0.55273056, 0.18383489, 0.07425772,\n       0.91671474, 0.14873389, 0.09483079, 0.97067806, 0.66696696,\n       0.72575404, 0.563204  , 0.07038971, 0.84187724, 0.418029  ,\n       0.39246782, 0.13530924, 0.11321989, 0.52224593, 0.56874359,\n       0.51868554, 0.61312468, 0.87764346, 0.50420484, 0.37914768,\n       0.2565727 , 0.30684664, 0.56080705, 0.79537234, 0.44112103,\n       0.04076233, 0.1881545 , 0.09065223, 0.33334341, 0.68437666,\n       0.59071473, 0.66212759, 0.45459513, 0.10978054, 0.29625596,\n       0.51096048, 0.49716497, 0.24366139, 0.82530156, 0.43331334,\n       0.84545613, 0.26549263, 0.94193959, 0.11185735, 0.76918249,\n       0.02018645, 0.23632066, 0.8705533 , 0.350105  , 0.93247949,\n       0.929417  , 0.80019258, 0.39610545, 0.8582685 , 0.45710434,\n       0.1261722 , 0.85195837, 0.8162467 , 0.13556544, 0.86652652,\n       0.51896305, 0.74359076, 0.26817602, 0.21546148, 0.84831281,\n       0.6002138 , 0.14770547, 0.36587009, 0.85903582, 0.46828358,\n       0.3368529 , 0.34095386, 0.8246442 , 0.45429903, 0.9483535 ,\n       0.31220015, 0.75648033, 0.28570549, 0.7678388 , 0.01759798,\n       0.12982098, 0.25925691, 0.87009196, 0.32249838, 0.48352554,\n...\n       0.25857775, 0.09677339, 0.18039325, 0.25448073, 0.83934999,\n       0.22122572, 0.82839782, 0.74329961, 0.97429452, 0.75359014,\n       0.11511305, 0.94004275, 0.84209462, 0.4436286 , 0.43358894,\n       0.03095428, 0.21764154, 0.71488612, 0.1105268 , 0.99174489,\n       0.02168247, 0.99097346, 0.29620757, 0.46086521, 0.54547145,\n       0.28955319, 0.22049757, 0.0505692 , 0.823433  , 0.97230937,\n       0.12555748, 0.85983084, 0.72053945, 0.75022534, 0.38780546,\n       0.05757491, 0.69042312, 0.96212643, 0.65875825, 0.23405231,\n       0.53673526, 0.12167514, 0.71787855, 0.67286419, 0.446794  ,\n       0.00866449, 0.06036607, 0.68197769, 0.69241334, 0.57520933,\n       0.22769122, 0.66313159, 0.1055052 , 0.62768057, 0.57231791,\n       0.35549364, 0.21916604, 0.7245827 , 0.18099196, 0.15735466,\n       0.63276412, 0.66062453, 0.1019691 , 0.26228831, 0.09859603,\n       0.91383146, 0.00838652, 0.35101158, 0.15682101, 0.46699488,\n       0.90683736, 0.70737638, 0.36017375, 0.1866537 , 0.70501213,\n       0.54191587, 0.72028997, 0.04485529, 0.17314062, 0.3198733 ,\n       0.4665367 , 0.56944602, 0.56209399, 0.54291566, 0.56611634,\n       0.41747562, 0.27881285, 0.51812954, 0.12186051, 0.74927137,\n       0.95293572, 0.05409649, 0.78232714, 0.22383289, 0.33641135,\n       0.03353818, 0.9690858 , 0.56209561, 0.08158143, 0.32155563])beta_color(neuron)float643.0 -2.0array([ 3., -2.])beta_size(neuron)float64-2.0 5.0array([-2.,  5.])mean(neuron)float647.0 -6.0array([ 7., -6.])std(neuron)float641.0 3.0array([1., 3.])Indexes: (2)neuronPandasIndexPandasIndex(Index([1, 2], dtype='int64', name='neuron'))stimulusPandasIndexPandasIndex(Index([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n       ...\n        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000],\n      dtype='int64', name='stimulus', length=1000))Attributes: (0)"
  },
  {
    "objectID": "pages/introducing_pca.html#understanding-the-neural-code",
    "href": "pages/introducing_pca.html#understanding-the-neural-code",
    "title": "An introduction to PCA",
    "section": "Understanding the neural code",
    "text": "Understanding the neural code\nHow is this information encoded in the population activity? Is there a neuron that is sensitive to color and another that is sensitive to size?\nOne way to understand this is by studying the neurons directly. Let’s start by visualizing the response of each neuron to our stimuli. Note that the below plots are 1-dimensional scatterplots; the spread along the vertical axis is just for visualization purposes.\n\n\nCode\ndef view_individual_scatter(\n    data: xr.DataArray,\n    *,\n    coord: str,\n    dim: str,\n    template_func: Callable[[int], str],\n) -&gt; Figure:\n    rng = np.random.default_rng()\n    data_ = data.assign_coords(\n        {\"arbitrary\": (\"stimulus\", rng.random(data.sizes[\"stimulus\"]))}\n    )\n    min_, max_ = data_.min(), data_.max()\n\n    n_features = data.sizes[dim]\n\n    fig, axes = plt.subplots(nrows=n_features, figsize=(7, 2 * n_features))\n\n    for index, ax in zip(data[coord].values, axes.flat):\n        label = template_func(index)\n        sns.scatterplot(\n            ax=ax,\n            data=(\n                data_.isel({dim: data[coord].values == index})\n                .rename(label)\n                .to_dataframe()\n            ),\n            x=label,\n            y=\"arbitrary\",\n            hue=\"color\",\n            size=\"size\",\n            palette=\"flare\",\n            legend=False,\n        )\n        sns.despine(ax=ax, left=True, offset=10)\n\n        ax.set_xlim([min_, max_])\n        ax.get_yaxis().set_visible(False)\n\n    fig.tight_layout(h_pad=3)\n    plt.close(fig)\n\n    return fig\n\n\n\n\nVisualize the individual neuron responses\nview_individual_scatter(\n    data,\n    coord=\"neuron\",\n    dim=\"neuron\",\n    template_func=lambda x: f\"neuron {x} response\",\n)\n\n\n\n\n\n\n\n\n\nBy visualizing the responses, we can see that each neuron is tuned to both color and size.\nWe can also use methods such as Representational Similarity Analysis (RSA)(Kriegeskorte 2008) to study the information content of the two neurons. In RSA, the dissimilarity between the response of a system to each pair of stimuli is obtained and represented in a matrix of dissimilarities (a representational dissimilarity matrix, or RDM). Therefore, an RDM indicates the degree to which each pair of stimuli is distinguished by the neurons.\n\nWhat if we want to study the underlying structure of this code? Is there another view of the population code that might be more informative?"
  },
  {
    "objectID": "pages/introducing_pca.html#studying-the-latent-dimensions",
    "href": "pages/introducing_pca.html#studying-the-latent-dimensions",
    "title": "An introduction to PCA",
    "section": "Studying the latent dimensions",
    "text": "Studying the latent dimensions\nInstead of directly studying the neurons, we can focus on the underlying factors that capture the structure and variance in the data.\n\nSome geometric intuition\nSince we only have P = 2 neurons, we can visualize these data as a scatterplot, which makes their covariance apparent.\n\n\nvisualize the joint response\ndef view_joint_scatter(\n    data: xr.DataArray,\n    *,\n    coord: str,\n    dim: str,\n    template_func: Callable[[int], str],\n    draw_axes: bool = False,\n) -&gt; Figure:\n    fig, ax = plt.subplots()\n\n    data_ = pd.DataFrame(\n        {coord_: data[coord_].values for coord_ in (\"color\", \"size\")}\n        | {\n            template_func(index): data.isel({dim: index - 1}).to_dataframe()[coord]\n            for index in (1, 2)\n        }\n    )\n\n    sns.scatterplot(\n        ax=ax,\n        data=data_,\n        x=template_func(1),\n        y=template_func(2),\n        hue=\"color\",\n        size=\"size\",\n        legend=False,\n        palette=\"flare\",\n    )\n    if draw_axes:\n        ax.axhline(0, c=\"gray\", ls=\"--\")\n        ax.axvline(0, c=\"gray\", ls=\"--\")\n\n    ax.set_aspect(\"equal\", \"box\")\n    sns.despine(ax=ax, offset=20)\n    plt.close(fig)\n\n    return fig\n\n\nview_joint_scatter(\n    data,\n    coord=\"neuron responses\",\n    dim=\"neuron\",\n    template_func=lambda x: f\"neuron {x} response\",\n)\n\n\n\n\n\n\n\n\n\nWe can use the covariance to change the way we view the data. Run the animation below for demonstration.\n\n\nAnimate the PCA transformation\ndef animate_pca_transformation(\n    data: xr.DataArray,\n    *,\n    durations: dict[str, int] = {\n        \"center\": 1_000,\n        \"rotate\": 1_000,\n        \"pause\": 500,\n    },\n    interval: int = 50,\n) -&gt; str:\n    def _compute_2d_rotation_matrix(theta: float) -&gt; np.ndarray:\n        return np.array(\n            [\n                [np.cos(theta), -np.sin(theta)],\n                [np.sin(theta), np.cos(theta)],\n            ]\n        )\n\n    fig = view_joint_scatter(\n        data,\n        coord=\"neuron responses\",\n        dim=\"neuron\",\n        template_func=lambda x: f\"neuron {x} response\",\n        draw_axes=True,\n    )\n    ax = fig.get_axes()[0]\n    scatter = ax.get_children()[0]\n    title = fig.suptitle(\"neuron responses\")\n\n    n_frames = {key: value // interval + 1 for key, value in durations.items()}\n\n    x_mean, y_mean = data.mean(\"stimulus\").values\n    delta = np.array([x_mean, y_mean]) / n_frames[\"center\"]\n\n    _, _, v_h = np.linalg.svd(data - data.mean(\"stimulus\"))\n    v = v_h.transpose()\n    theta = np.arccos(v[0, 0])\n    rotation = _compute_2d_rotation_matrix(-theta / n_frames[\"rotate\"])\n\n    transformed = (data - data.mean(\"stimulus\")).values @ v\n\n    radius = max(np.linalg.norm(transformed, axis=-1))\n    limit = max(np.abs(data).max(), np.abs(transformed).max(), radius)\n    ax.set_xlim([-limit, limit])\n    ax.set_ylim([-limit, limit])\n    fig.tight_layout()\n\n    frame_to_retitle_center = 2 * n_frames[\"pause\"]\n    frame_to_start_centering = frame_to_retitle_center + n_frames[\"pause\"]\n    frame_to_stop_centering = frame_to_start_centering + n_frames[\"center\"]\n    frame_to_retitle_rotate = frame_to_stop_centering + n_frames[\"pause\"]\n    frame_to_start_rotating = frame_to_retitle_rotate + n_frames[\"pause\"]\n    frame_to_stop_rotating = frame_to_start_rotating + n_frames[\"rotate\"]\n    frame_to_retitle_transformed = frame_to_stop_rotating + n_frames[\"pause\"]\n    frame_to_end = frame_to_retitle_transformed + 2 * n_frames[\"pause\"]\n\n    def _update(frame: int) -&gt; None:\n        if frame &lt; frame_to_retitle_center:\n            return\n        elif frame == frame_to_retitle_center:\n            title.set_text(\"step 1 of 2: center the data\")\n            ax.set_xlabel(\"\")\n            ax.set_ylabel(\"\")\n        elif frame &lt; frame_to_start_centering:\n            return\n        elif frame &lt;= frame_to_stop_centering:\n            scatter.set_offsets(scatter.get_offsets() - delta)\n        elif frame == frame_to_retitle_rotate:\n            title.set_text(\"step 2 of 2: rotate the data\")\n        elif frame &lt; frame_to_start_rotating:\n            return\n        elif frame &lt;= frame_to_stop_rotating:\n            scatter.set_offsets(scatter.get_offsets().data @ rotation)\n        elif frame &lt; frame_to_retitle_transformed:\n            return\n        elif frame == frame_to_retitle_transformed:\n            title.set_text(\"principal components\")\n            ax.set_xlabel(\"principal component 1\")\n            ax.set_ylabel(\"principal component 2\")\n        elif frame &lt;= frame_to_end:\n            return\n\n    animation = FuncAnimation(\n        fig=fig,\n        func=_update,\n        frames=frame_to_end,\n        interval=interval,\n        repeat=False,\n    )\n    plt.close(fig)\n    return animation.to_html5_video()\n\n\n# display(HTML(animate_pca_transformation(data)))\n\n\n\n\n\n\n\n\nTip\n\n\n\nClick on the animation above to visualize the PCA transformation!\n\n\nAs seen in the animation, we can transform our data to view the directions of maximum variance. These directions are the principal components of our data.\n\n\nThe mathematical definition\nGiven a data matrix X \\in \\mathbb{R}^{N \\times P}, we need to compute the eigendecomposition1 of its covariance2:\n\n\\begin{align*}\n    \\text{cov}(X)\n    &= \\left(\\dfrac{1}{n - 1}\\right) (X - \\overline{X})^\\top (X - \\overline{X})\\\\\n    &= V \\Lambda V^\\top\n\\end{align*}\n\nTo do this, we start by computing the covariance of our data matrix, where X is centered (i.e. X - \\overline{X})\n\nNext , we compute the the eigendecomposition of the covariance (though the computational steps we take to get there are slightly different, as shown later)\n\nThe columns of V are eigenvectors that specify the directions of variance while the corresponding diagonal elements of \\Lambda are eigenvalues that specify the amount of variance along the eigenvector3.\nFinally, the original data matrix can be transformed by projecting it onto the eigenvectors: \\widetilde{X} = \\left(X - \\overline{X}\\right) V.\n\n\n\n\n\n\n\nViewing PCA as an optimization\n\n\n\n\n\nPCA can be used to project data into a lower-dimensional space (i.e. p \\le f) in a way that best preserves the geometry of the data. Specifically, computing a PCA decomposition of X yields a matrix V \\in \\mathbb{R}^{f \\times p} such that V = \\argmin_{V \\in \\mathbb{U_{f \\times p}}} \\sum_{i=1}^n \\left|| x_i - VV^\\top x_i \\right||_2, where ||\\cdot||_2 denotes the L_2-norm and \\mathbb{U_{f \\times p}} denotes the set of orthonormal matrices with shape f \\times p.\n\n\n\n\n\nA computational recipe\n\nCenter the data matrix.\nCompute its singular value decomposition4.\nThe right singular vectors V of the data matrix are the eigenvectors of its covariance.\nThe singular values \\Sigma of the data matrix are related to the eigenvalues \\Lambda of its covariance as \\Lambda = \\Sigma^2 / (N - 1)\nTo project data from the ambient space to the latent space, we must subtract the mean computed in Step 1, and multiply the data by the eigenvectors.\n\n\n\nA computational recipe for PCA\nclass PCA:\n    def __init__(self) -&gt; None:\n        self.mean: np.ndarray\n        self.eigenvectors: np.ndarray\n        self.eigenvalues: np.ndarray\n\n    def fit(self, /, data: np.ndarray) -&gt; None:\n        self.mean = data.mean(axis=-2)\n\n        data_centered = data - self.mean\n        _, s, v_t = np.linalg.svd(data_centered)\n\n        n_stimuli = data.shape[-2]\n\n        self.eigenvectors = np.swapaxes(v_t, -1, -2)\n        self.eigenvalues = s**2 / (n_stimuli - 1)\n\n    def transform(self, /, data: np.ndarray) -&gt; np.ndarray:\n        return (data - self.mean) @ self.eigenvectors\n\n\n\n\n\n\n\n\nWhy do we compute PCA this way instead of \\text{eig}(\\text{cov}(X))?\n\n\n\n\n\nTo apply PCA to a data matrix, we might be tempted to use the definition and naively compute its covariance followed by an eigendecomposition. However, when the number of neurons P is large, this approach is memory-intensive and prone to numerical errors.\nInstead, we can use the singular value decomposition (SVD) of X to efficiently compute its PCA transformation. Specifically, X = U \\Sigma V^\\top is a singular value decomposition, where U and V are orthonormal and \\Sigma is diagonal.\nThe covariance matrix reduces to X^\\top X / (n - 1) = V \\left(\\frac{\\Sigma^2}{n - 1} \\right) V^\\top, which is exactly the eigendecomposition required.\nSpecifically, the eigenvalues \\lambda_i of the covariance matrix are related to the singular values \\sigma_i of the data matrix as \\lambda_i = \\sigma_i^2 / (N - 1), while the eigenvectors of the covariance matrix are exactly the right singular vectors V of the data matrix X.\n\n\n\n\n\n\n\n\n\nOnly need the first few PCs?\n\n\n\n\n\nCheck out truncated SVD!"
  },
  {
    "objectID": "pages/introducing_pca.html#transforming-the-dataset",
    "href": "pages/introducing_pca.html#transforming-the-dataset",
    "title": "An introduction to PCA",
    "section": "Transforming the dataset",
    "text": "Transforming the dataset\nLet’s now project our data into its principal components and analyse it in this space.\n\n\nApply the PCA transformation\ndef compute_pca(data: xr.DataArray) -&gt; xr.Dataset:\n    pca = PCA()\n    pca.fit(data.values)\n\n    data_transformed = pca.transform(data.values)\n\n    return xr.Dataset(\n        data_vars={\n            \"score\": xr.DataArray(\n                data=data_transformed,\n                dims=(\"stimulus\", \"component\"),\n            ),\n            \"eigenvector\": xr.DataArray(\n                data=pca.eigenvectors,\n                dims=(\"component\", \"neuron\"),\n            ),\n        },\n        coords={\n            \"rank\": (\"component\", 1 + np.arange(data_transformed.shape[-1])),\n            \"eigenvalue\": (\"component\", pca.eigenvalues),\n        }\n        | {coord: (data[coord].dims[0], data[coord].values) for coord in data.coords},\n    )\n\n\npca = compute_pca(data)\ndisplay(pca[\"score\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'score' (stimulus: 1000, component: 2)&gt;\n11.19 1.269 -10.77 -1.394 -9.962 -1.541 ... -1.028 8.777 -5.869 3.39 -2.457\nCoordinates: (3/5)\n  * stimulus    (stimulus) int64 1 2 3 4 5 6 7 ... 994 995 996 997 998 999 1000\n    rank        (component) int64 1 2\n    ...          ...\n    size        (stimulus) float64 0.01301 0.8278 0.4962 ... 0.08158 0.3216\nDimensions without coordinates: componentxarray.DataArray'score'stimulus: 1000component: 211.19 1.269 -10.77 -1.394 -9.962 ... -1.028 8.777 -5.869 3.39 -2.457array([[ 1.11911792e+01,  1.26889981e+00],\n       [-1.07729551e+01, -1.39397546e+00],\n       [-9.96219137e+00, -1.54107707e+00],\n       ...,\n       [ 6.58127224e-03, -1.02783716e+00],\n       [ 8.77667289e+00, -5.86946476e+00],\n       [ 3.39020249e+00, -2.45675640e+00]])Coordinates: (5)stimulus(stimulus)int641 2 3 4 5 ... 996 997 998 999 1000array([   1,    2,    3, ...,  998,  999, 1000])rank(component)int641 2array([1, 2])eigenvalue(component)float6446.3 6.148array([46.29579973,  6.14792569])color(stimulus)float640.637 0.2698 ... 0.1649 0.38array([6.36961687e-01, 2.69786714e-01, 4.09735239e-02, 1.65276355e-02,\n       8.13270239e-01, 9.12755577e-01, 6.06635776e-01, 7.29496561e-01,\n       5.43624991e-01, 9.35072424e-01, 8.15853554e-01, 2.73850017e-03,\n       8.57404277e-01, 3.35855753e-02, 7.29655446e-01, 1.75655621e-01,\n       8.63178922e-01, 5.41461220e-01, 2.99711891e-01, 4.22687221e-01,\n       2.83196711e-02, 1.24283276e-01, 6.70624415e-01, 6.47189512e-01,\n       6.15385111e-01, 3.83677554e-01, 9.97209936e-01, 9.80835339e-01,\n       6.85541984e-01, 6.50459276e-01, 6.88446731e-01, 3.88921424e-01,\n       1.35096505e-01, 7.21488340e-01, 5.25354322e-01, 3.10241876e-01,\n       4.85835359e-01, 8.89487834e-01, 9.34043516e-01, 3.57795197e-01,\n       5.71529831e-01, 3.21869391e-01, 5.94300030e-01, 3.37911226e-01,\n       3.91619001e-01, 8.90274352e-01, 2.27157594e-01, 6.23187145e-01,\n       8.40153436e-02, 8.32644148e-01, 7.87098307e-01, 2.39369443e-01,\n       8.76484231e-01, 5.85680348e-02, 3.36117061e-01, 1.50279467e-01,\n       4.50339367e-01, 7.96324270e-01, 2.30642209e-01, 5.20213011e-02,\n       4.04551840e-01, 1.98513045e-01, 9.07530456e-02, 5.80332386e-01,\n       2.98696133e-01, 6.71994878e-01, 1.99515444e-01, 9.42113111e-01,\n       3.65110168e-01, 1.05495280e-01, 6.29108152e-01, 9.27154553e-01,\n       4.40377155e-01, 9.54590494e-01, 4.99895814e-01, 4.25228625e-01,\n       6.20213452e-01, 9.95096505e-01, 9.48943675e-01, 4.60045139e-01,\n...\n       3.39811155e-01, 2.21699710e-04, 4.82537622e-01, 6.08000665e-01,\n       9.29904601e-02, 2.42094402e-01, 8.03991821e-01, 8.40281560e-01,\n       3.87733254e-01, 8.14223731e-01, 2.77140253e-01, 7.06108222e-01,\n       5.45456624e-01, 4.40099079e-01, 6.56442276e-01, 1.33906791e-02,\n       1.62443443e-01, 2.93823464e-01, 6.80562610e-01, 7.06235313e-01,\n       6.80760824e-01, 7.67617107e-01, 7.95515610e-02, 1.05888908e-01,\n       8.55351160e-01, 3.56837753e-01, 5.68371290e-01, 5.03502806e-01,\n       6.26662571e-01, 7.69467112e-02, 7.69790226e-01, 1.23402328e-01,\n       6.81374462e-01, 4.02142099e-01, 4.92261636e-01, 6.71693734e-01,\n       3.71002751e-01, 4.60374714e-02, 9.64211552e-01, 5.22676899e-01,\n       7.42144641e-01, 5.31294834e-01, 8.19686903e-01, 5.64616179e-01,\n       1.22756878e-01, 6.41906565e-01, 1.72740669e-01, 8.23654135e-01,\n       6.81061600e-01, 9.39808638e-01, 6.29080790e-01, 2.25163097e-01,\n       5.57137546e-01, 7.71772277e-01, 7.11888298e-01, 3.42296706e-01,\n       6.55351151e-01, 9.35269061e-01, 6.84810043e-01, 3.67301402e-01,\n       9.10758330e-01, 8.27624193e-01, 8.55183760e-01, 1.06841377e-01,\n       2.90828834e-01, 7.90127803e-01, 2.74807496e-01, 7.37059356e-02,\n       6.83266012e-01, 7.99269956e-01, 6.41767808e-01, 3.44843336e-01,\n       5.59773319e-01, 2.15199514e-02, 5.62661656e-01, 8.56801175e-01,\n       7.80532350e-02, 3.83319397e-01, 1.64863718e-01, 3.80007897e-01])size(stimulus)float640.01301 0.8278 ... 0.08158 0.3216array([0.01300767, 0.82776292, 0.49624331, 0.43591807, 0.60179472,\n       0.8500282 , 0.29126072, 0.26751697, 0.04949421, 0.26639909,\n       0.06621185, 0.04155849, 0.55273056, 0.18383489, 0.07425772,\n       0.91671474, 0.14873389, 0.09483079, 0.97067806, 0.66696696,\n       0.72575404, 0.563204  , 0.07038971, 0.84187724, 0.418029  ,\n       0.39246782, 0.13530924, 0.11321989, 0.52224593, 0.56874359,\n       0.51868554, 0.61312468, 0.87764346, 0.50420484, 0.37914768,\n       0.2565727 , 0.30684664, 0.56080705, 0.79537234, 0.44112103,\n       0.04076233, 0.1881545 , 0.09065223, 0.33334341, 0.68437666,\n       0.59071473, 0.66212759, 0.45459513, 0.10978054, 0.29625596,\n       0.51096048, 0.49716497, 0.24366139, 0.82530156, 0.43331334,\n       0.84545613, 0.26549263, 0.94193959, 0.11185735, 0.76918249,\n       0.02018645, 0.23632066, 0.8705533 , 0.350105  , 0.93247949,\n       0.929417  , 0.80019258, 0.39610545, 0.8582685 , 0.45710434,\n       0.1261722 , 0.85195837, 0.8162467 , 0.13556544, 0.86652652,\n       0.51896305, 0.74359076, 0.26817602, 0.21546148, 0.84831281,\n       0.6002138 , 0.14770547, 0.36587009, 0.85903582, 0.46828358,\n       0.3368529 , 0.34095386, 0.8246442 , 0.45429903, 0.9483535 ,\n       0.31220015, 0.75648033, 0.28570549, 0.7678388 , 0.01759798,\n       0.12982098, 0.25925691, 0.87009196, 0.32249838, 0.48352554,\n...\n       0.25857775, 0.09677339, 0.18039325, 0.25448073, 0.83934999,\n       0.22122572, 0.82839782, 0.74329961, 0.97429452, 0.75359014,\n       0.11511305, 0.94004275, 0.84209462, 0.4436286 , 0.43358894,\n       0.03095428, 0.21764154, 0.71488612, 0.1105268 , 0.99174489,\n       0.02168247, 0.99097346, 0.29620757, 0.46086521, 0.54547145,\n       0.28955319, 0.22049757, 0.0505692 , 0.823433  , 0.97230937,\n       0.12555748, 0.85983084, 0.72053945, 0.75022534, 0.38780546,\n       0.05757491, 0.69042312, 0.96212643, 0.65875825, 0.23405231,\n       0.53673526, 0.12167514, 0.71787855, 0.67286419, 0.446794  ,\n       0.00866449, 0.06036607, 0.68197769, 0.69241334, 0.57520933,\n       0.22769122, 0.66313159, 0.1055052 , 0.62768057, 0.57231791,\n       0.35549364, 0.21916604, 0.7245827 , 0.18099196, 0.15735466,\n       0.63276412, 0.66062453, 0.1019691 , 0.26228831, 0.09859603,\n       0.91383146, 0.00838652, 0.35101158, 0.15682101, 0.46699488,\n       0.90683736, 0.70737638, 0.36017375, 0.1866537 , 0.70501213,\n       0.54191587, 0.72028997, 0.04485529, 0.17314062, 0.3198733 ,\n       0.4665367 , 0.56944602, 0.56209399, 0.54291566, 0.56611634,\n       0.41747562, 0.27881285, 0.51812954, 0.12186051, 0.74927137,\n       0.95293572, 0.05409649, 0.78232714, 0.22383289, 0.33641135,\n       0.03353818, 0.9690858 , 0.56209561, 0.08158143, 0.32155563])Indexes: (1)stimulusPandasIndexPandasIndex(Index([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n       ...\n        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000],\n      dtype='int64', name='stimulus', length=1000))Attributes: (0)\n\n\n\nInspecting the eigenvectors\nThe eigenvectors represent the directions of maximum variance in our data.\n\n\nDisplay the eigenvectors\nwith xr.set_options(display_expand_data=True):\n    display(pca[\"eigenvector\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'eigenvector' (component: 2, neuron: 2)&gt;\narray([[ 0.43538525,  0.90024423],\n       [-0.90024423,  0.43538525]])\nCoordinates: (3/7)\n  * neuron      (neuron) int64 1 2\n    rank        (component) int64 1 2\n    ...          ...\n    std         (neuron) float64 1.0 3.0\nDimensions without coordinates: componentxarray.DataArray'eigenvector'component: 2neuron: 20.4354 0.9002 -0.9002 0.4354array([[ 0.43538525,  0.90024423],\n       [-0.90024423,  0.43538525]])Coordinates: (7)neuron(neuron)int641 2array([1, 2])rank(component)int641 2array([1, 2])eigenvalue(component)float6446.3 6.148array([46.29579973,  6.14792569])beta_color(neuron)float643.0 -2.0array([ 3., -2.])beta_size(neuron)float64-2.0 5.0array([-2.,  5.])mean(neuron)float647.0 -6.0array([ 7., -6.])std(neuron)float641.0 3.0array([1., 3.])Indexes: (1)neuronPandasIndexPandasIndex(Index([1, 2], dtype='int64', name='neuron'))Attributes: (0)\n\n\n\n\nInterpreting the transformed data\nWe can view the data projected onto each of the principal components.\n\n\nVisualize principal component scores\nview_individual_scatter(\n    pca[\"score\"],\n    coord=\"rank\",\n    dim=\"component\",\n    template_func=lambda x: f\"principal component {x}\",\n)\n\n\n\n\n\n\n\n\n\nWe observe that:\n\nthe first principal component is largely driven by the size of the stimulus\nthe second principal component is largely driven by the color of the stimulus\n\n\n\n\n\n\n\nImportant\n\n\n\nNote that these components do not directly correspond to either of the latent variables. Rather, each is a mixture of stimulus-dependent signal and noise.\n\n\n\n\nInspecting the eigenspectrum\nThe eigenvalues show the variance along each eigenvector. The sum of the eigenvalues is therefore equal to the total variance.\n\n\nDisplay eigenvalues\neigenvalues = pca[\"eigenvalue\"].round(3)\nfor i_neuron in range(eigenvalues.sizes[\"component\"]):\n    print(\n        f\"variance along eigenvector {i_neuron + 1} (eigenvalue {i_neuron + 1}):\"\n        f\" {eigenvalues[i_neuron].values}\"\n    )\nprint(f\"total variance: {eigenvalues.sum().values}\")\n\n\nvariance along eigenvector 1 (eigenvalue 1): 46.296\nvariance along eigenvector 2 (eigenvalue 2): 6.148\ntotal variance: 52.444\n\n\nWe can also see that this is equal to the total variance in the original data.\n\n\nCompute total variance in the original data\nvariances = data.var(dim=\"stimulus\", ddof=1).round(3).rename(\"neuron variances\")\nfor i_neuron in range(variances.sizes[\"neuron\"]):\n    print(f\"variance of neuron {i_neuron + 1} responses: {variances[i_neuron].values}\")\nprint(f\"total variance: {variances.sum().values}\")\n\n\nvariance of neuron 1 responses: 13.758\nvariance of neuron 2 responses: 38.685\ntotal variance: 52.443\n\n\nWe can plot the eigenvalues as a function of their rank to visualize the eigensceptrum. As we will see shortly, the eigenspectrum provides valuable insights about the laten dimensionality of the data.\n\n\nView eigenspectrum\ndef view_eigenspectrum(pca: xr.DataArray) -&gt; Figure:\n    fig, ax = plt.subplots(figsize=(pca.sizes[\"component\"], 5))\n    sns.lineplot(\n        ax=ax,\n        data=pca[\"component\"].to_dataframe(),\n        x=\"rank\",\n        y=\"eigenvalue\",\n        marker=\"s\",\n    )\n    ax.set_xticks(pca[\"rank\"].values)\n    ax.set_ylim(bottom=0)\n    sns.despine(ax=ax, offset=20)\n    plt.close(fig)\n\n    return fig\n\n\nview_eigenspectrum(pca)"
  },
  {
    "objectID": "pages/introducing_pca.html#quantifying-dimensionality",
    "href": "pages/introducing_pca.html#quantifying-dimensionality",
    "title": "An introduction to PCA",
    "section": "Quantifying dimensionality",
    "text": "Quantifying dimensionality\nIn these data, the dimensionality is clear: there are two latent variables and both are evident in the principal components. However, in real data, we typically record from more than P = 2 neurons, therefore judging the dimensionality becomes tricky. To simulate such a scenario, let’s record from more neurons (say P = 10).\n\n\nSimulate responses from more neurons\ndef _simulate_random_neuron(rng: np.random.Generator) -&gt; Neuron:\n    return Neuron(\n        beta_color=rng.integers(-10, 11),\n        beta_size=rng.integers(-10, 11),\n        std=rng.integers(-10, 11),\n        mean=rng.integers(-10, 11),\n    )\n\n\nneurons = tuple([_simulate_random_neuron(rng) for _ in range(10)])\n\nbig_data = simulate_multiple_neuron_responses(\n    stimuli=stimuli,\n    neurons=neurons,\n    rng=rng,\n)\n\ndisplay(big_data)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.DataArray 'neuron responses' (stimulus: 1000, neuron: 10)&gt;\n-1.747 -8.53 -7.006 4.696 8.766 28.16 ... 8.782 0.7017 -2.132 -1.981 -1.87\nCoordinates: (3/8)\n  * neuron      (neuron) int64 1 2 3 4 5 6 7 8 9 10\n  * stimulus    (stimulus) int64 1 2 3 4 5 6 7 ... 994 995 996 997 998 999 1000\n    ...          ...\n    std         (neuron) float64 -5.0 2.0 -3.0 5.0 -10.0 ... -2.0 -4.0 -2.0 -8.0xarray.DataArray'neuron responses'stimulus: 1000neuron: 10-1.747 -8.53 -7.006 4.696 8.766 ... 8.782 0.7017 -2.132 -1.981 -1.87array([[ -1.7468685 ,  -8.52988047,  -7.00619911, ..., -12.08407229,\n         -6.11588709,   5.79962078],\n       [  0.5417999 ,   5.76857817, -14.19056082, ...,  -7.57254665,\n         15.63044955, -12.08460524],\n       [-15.14390287,   7.43798007, -11.87857537, ...,  -5.60388679,\n          1.55649921,  -0.7286054 ],\n       ...,\n       [-11.867323  ,   3.04997894,  -8.19304571, ..., -13.63460347,\n          7.49747562,  10.62648332],\n       [-23.71380179,   4.65415007,  -5.80502762, ...,  -7.50271373,\n        -10.13915485,  -8.14599889],\n       [ -7.25947058,   1.02221347,  -8.99930611, ...,  -2.13177182,\n         -1.98124877,  -1.87010738]])Coordinates: (8)neuron(neuron)int641 2 3 4 5 6 7 8 9 10array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10])stimulus(stimulus)int641 2 3 4 5 ... 996 997 998 999 1000array([   1,    2,    3, ...,  998,  999, 1000])color(stimulus)float640.637 0.2698 ... 0.1649 0.38array([6.36961687e-01, 2.69786714e-01, 4.09735239e-02, 1.65276355e-02,\n       8.13270239e-01, 9.12755577e-01, 6.06635776e-01, 7.29496561e-01,\n       5.43624991e-01, 9.35072424e-01, 8.15853554e-01, 2.73850017e-03,\n       8.57404277e-01, 3.35855753e-02, 7.29655446e-01, 1.75655621e-01,\n       8.63178922e-01, 5.41461220e-01, 2.99711891e-01, 4.22687221e-01,\n       2.83196711e-02, 1.24283276e-01, 6.70624415e-01, 6.47189512e-01,\n       6.15385111e-01, 3.83677554e-01, 9.97209936e-01, 9.80835339e-01,\n       6.85541984e-01, 6.50459276e-01, 6.88446731e-01, 3.88921424e-01,\n       1.35096505e-01, 7.21488340e-01, 5.25354322e-01, 3.10241876e-01,\n       4.85835359e-01, 8.89487834e-01, 9.34043516e-01, 3.57795197e-01,\n       5.71529831e-01, 3.21869391e-01, 5.94300030e-01, 3.37911226e-01,\n       3.91619001e-01, 8.90274352e-01, 2.27157594e-01, 6.23187145e-01,\n       8.40153436e-02, 8.32644148e-01, 7.87098307e-01, 2.39369443e-01,\n       8.76484231e-01, 5.85680348e-02, 3.36117061e-01, 1.50279467e-01,\n       4.50339367e-01, 7.96324270e-01, 2.30642209e-01, 5.20213011e-02,\n       4.04551840e-01, 1.98513045e-01, 9.07530456e-02, 5.80332386e-01,\n       2.98696133e-01, 6.71994878e-01, 1.99515444e-01, 9.42113111e-01,\n       3.65110168e-01, 1.05495280e-01, 6.29108152e-01, 9.27154553e-01,\n       4.40377155e-01, 9.54590494e-01, 4.99895814e-01, 4.25228625e-01,\n       6.20213452e-01, 9.95096505e-01, 9.48943675e-01, 4.60045139e-01,\n...\n       3.39811155e-01, 2.21699710e-04, 4.82537622e-01, 6.08000665e-01,\n       9.29904601e-02, 2.42094402e-01, 8.03991821e-01, 8.40281560e-01,\n       3.87733254e-01, 8.14223731e-01, 2.77140253e-01, 7.06108222e-01,\n       5.45456624e-01, 4.40099079e-01, 6.56442276e-01, 1.33906791e-02,\n       1.62443443e-01, 2.93823464e-01, 6.80562610e-01, 7.06235313e-01,\n       6.80760824e-01, 7.67617107e-01, 7.95515610e-02, 1.05888908e-01,\n       8.55351160e-01, 3.56837753e-01, 5.68371290e-01, 5.03502806e-01,\n       6.26662571e-01, 7.69467112e-02, 7.69790226e-01, 1.23402328e-01,\n       6.81374462e-01, 4.02142099e-01, 4.92261636e-01, 6.71693734e-01,\n       3.71002751e-01, 4.60374714e-02, 9.64211552e-01, 5.22676899e-01,\n       7.42144641e-01, 5.31294834e-01, 8.19686903e-01, 5.64616179e-01,\n       1.22756878e-01, 6.41906565e-01, 1.72740669e-01, 8.23654135e-01,\n       6.81061600e-01, 9.39808638e-01, 6.29080790e-01, 2.25163097e-01,\n       5.57137546e-01, 7.71772277e-01, 7.11888298e-01, 3.42296706e-01,\n       6.55351151e-01, 9.35269061e-01, 6.84810043e-01, 3.67301402e-01,\n       9.10758330e-01, 8.27624193e-01, 8.55183760e-01, 1.06841377e-01,\n       2.90828834e-01, 7.90127803e-01, 2.74807496e-01, 7.37059356e-02,\n       6.83266012e-01, 7.99269956e-01, 6.41767808e-01, 3.44843336e-01,\n       5.59773319e-01, 2.15199514e-02, 5.62661656e-01, 8.56801175e-01,\n       7.80532350e-02, 3.83319397e-01, 1.64863718e-01, 3.80007897e-01])size(stimulus)float640.01301 0.8278 ... 0.08158 0.3216array([0.01300767, 0.82776292, 0.49624331, 0.43591807, 0.60179472,\n       0.8500282 , 0.29126072, 0.26751697, 0.04949421, 0.26639909,\n       0.06621185, 0.04155849, 0.55273056, 0.18383489, 0.07425772,\n       0.91671474, 0.14873389, 0.09483079, 0.97067806, 0.66696696,\n       0.72575404, 0.563204  , 0.07038971, 0.84187724, 0.418029  ,\n       0.39246782, 0.13530924, 0.11321989, 0.52224593, 0.56874359,\n       0.51868554, 0.61312468, 0.87764346, 0.50420484, 0.37914768,\n       0.2565727 , 0.30684664, 0.56080705, 0.79537234, 0.44112103,\n       0.04076233, 0.1881545 , 0.09065223, 0.33334341, 0.68437666,\n       0.59071473, 0.66212759, 0.45459513, 0.10978054, 0.29625596,\n       0.51096048, 0.49716497, 0.24366139, 0.82530156, 0.43331334,\n       0.84545613, 0.26549263, 0.94193959, 0.11185735, 0.76918249,\n       0.02018645, 0.23632066, 0.8705533 , 0.350105  , 0.93247949,\n       0.929417  , 0.80019258, 0.39610545, 0.8582685 , 0.45710434,\n       0.1261722 , 0.85195837, 0.8162467 , 0.13556544, 0.86652652,\n       0.51896305, 0.74359076, 0.26817602, 0.21546148, 0.84831281,\n       0.6002138 , 0.14770547, 0.36587009, 0.85903582, 0.46828358,\n       0.3368529 , 0.34095386, 0.8246442 , 0.45429903, 0.9483535 ,\n       0.31220015, 0.75648033, 0.28570549, 0.7678388 , 0.01759798,\n       0.12982098, 0.25925691, 0.87009196, 0.32249838, 0.48352554,\n...\n       0.25857775, 0.09677339, 0.18039325, 0.25448073, 0.83934999,\n       0.22122572, 0.82839782, 0.74329961, 0.97429452, 0.75359014,\n       0.11511305, 0.94004275, 0.84209462, 0.4436286 , 0.43358894,\n       0.03095428, 0.21764154, 0.71488612, 0.1105268 , 0.99174489,\n       0.02168247, 0.99097346, 0.29620757, 0.46086521, 0.54547145,\n       0.28955319, 0.22049757, 0.0505692 , 0.823433  , 0.97230937,\n       0.12555748, 0.85983084, 0.72053945, 0.75022534, 0.38780546,\n       0.05757491, 0.69042312, 0.96212643, 0.65875825, 0.23405231,\n       0.53673526, 0.12167514, 0.71787855, 0.67286419, 0.446794  ,\n       0.00866449, 0.06036607, 0.68197769, 0.69241334, 0.57520933,\n       0.22769122, 0.66313159, 0.1055052 , 0.62768057, 0.57231791,\n       0.35549364, 0.21916604, 0.7245827 , 0.18099196, 0.15735466,\n       0.63276412, 0.66062453, 0.1019691 , 0.26228831, 0.09859603,\n       0.91383146, 0.00838652, 0.35101158, 0.15682101, 0.46699488,\n       0.90683736, 0.70737638, 0.36017375, 0.1866537 , 0.70501213,\n       0.54191587, 0.72028997, 0.04485529, 0.17314062, 0.3198733 ,\n       0.4665367 , 0.56944602, 0.56209399, 0.54291566, 0.56611634,\n       0.41747562, 0.27881285, 0.51812954, 0.12186051, 0.74927137,\n       0.95293572, 0.05409649, 0.78232714, 0.22383289, 0.33641135,\n       0.03353818, 0.9690858 , 0.56209561, 0.08158143, 0.32155563])beta_color(neuron)float6410.0 -4.0 1.0 7.0 ... -4.0 0.0 10.0array([10., -4.,  1.,  7., 10., 10., -9., -4.,  0., 10.])beta_size(neuron)float647.0 1.0 -3.0 6.0 ... -3.0 8.0 6.0array([ 7.,  1., -3.,  6.,  1., -7.,  4., -3.,  8.,  6.])mean(neuron)float640.0 -1.0 -9.0 2.0 ... -9.0 3.0 8.0array([ 0., -1., -9.,  2.,  1.,  7., -1., -9.,  3.,  8.])std(neuron)float64-5.0 2.0 -3.0 ... -4.0 -2.0 -8.0array([ -5.,   2.,  -3.,   5., -10.,  -4.,  -2.,  -4.,  -2.,  -8.])Indexes: (2)neuronPandasIndexPandasIndex(Index([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], dtype='int64', name='neuron'))stimulusPandasIndexPandasIndex(Index([   1,    2,    3,    4,    5,    6,    7,    8,    9,   10,\n       ...\n        991,  992,  993,  994,  995,  996,  997,  998,  999, 1000],\n      dtype='int64', name='stimulus', length=1000))Attributes: (0)\n\n\nAs before, we can visualize each the principal component and plot the eigenspectrum:\n\n\nVisualize principal component scores\nbig_pca = compute_pca(big_data)\n\nview_individual_scatter(\n    big_pca[\"score\"],\n    coord=\"rank\",\n    dim=\"component\",\n    template_func=lambda x: f\"principal component {x}\",\n)\n\n\n\n\n\n\n\n\n\n\n\nView eigenspectrum\nview_eigenspectrum(big_pca)\n\n\n\n\n\n\n\n\n\nWe know by design that this data was generated from 2 latent variables – color and size. However, in real datasets with naturalistic stimuli, we often don’t know what the latent variables are! It’s common to use the eigenspectrum to estimate the latent dimensionality of the data. For instance, inspecting the eigenspectrum of our toy example tells us that the first two dimensions have much higher variance than the rest. We refer to these as the effective dimensions.\nIn general, there are several approaches for estimating dimensionality based on the eigenspectrum:\n\nRank of the covariance matrix\nThe rank of the covariance matrix – equal to the number of nonzero eigenvalues – would be the latent dimensionality in the ideal setting where the data has zero noise. In real data, the rank is typically equal to the ambient dimensionality (which here is the number of neurons we record from), since there is typically some variance along every dimension.\n\n\nSet a threshold of zero on the variance\nprint(f\"rank = {(big_pca.eigenvalue &gt; 0).sum().values}\")\n\n\ndef view_thresholded_eigenspectrum(pca: PCA, *, threshold: int | float) -&gt; Figure:\n    fig = view_eigenspectrum(pca)\n    ax = fig.get_axes()[0]\n\n    ax.axhline(threshold, ls=\"--\", c=\"gray\")\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    ax.fill_between(x=xlim, y1=ylim[-1], y2=threshold, color=\"green\", alpha=0.1)\n    ax.fill_between(x=xlim, y1=ylim[0], y2=threshold, color=\"red\", alpha=0.1)\n\n    return fig\n\n\nview_thresholded_eigenspectrum(big_pca, threshold=0)\n\n\nrank = 10\n\n\n\n\n\n\n\nSetting an arbitrary variance threshold\nThough not typically used today, another approach is to set an arbitrary threshold on the variance (historically recommended as 1 for normalized data); only dimensions with variance above that threshold are considered useful.\n\n\nSet an arbitrary positive threshold on the variance\nview_thresholded_eigenspectrum(big_pca, threshold=150)\n\n\n\n\n\n\n\nSetting an arbitrary cumulative variance threshold\nA very commonly used method is to set a threshold based on the cumulative variance of the data: the number of dimensions required to exceed, say 80\\% of the variance, is taken as the latent dimensionality.\n\n\nSet an arbitrary threshold on the cumulative variance\ndef view_cumulative_eigenspectrum(\n    pca: xr.DataArray, *, threshold: float = 0.8\n) -&gt; Figure:\n    fig, ax = plt.subplots(figsize=(pca.sizes[\"component\"], 5))\n\n    data = pca[\"eigenvalue\"].copy()\n    data[\"eigenvalue\"] = data.cumsum()\n    data = data.rename({\"eigenvalue\": \"cumulative variance\"})\n    data[\"cumulative proportion of variance\"] = (\n        data[\"cumulative variance\"] / pca[\"eigenvalue\"].sum()\n    )\n\n    sns.lineplot(\n        ax=ax,\n        data=data.to_dataframe(),\n        x=\"rank\",\n        y=\"cumulative variance\",\n        marker=\"s\",\n    )\n    ax.set_xticks(pca[\"rank\"].values)\n\n    ax_twin = ax.twinx()\n    ax_twin.set_ylabel(\"cumulative proportion of variance\")\n\n    ax_twin.axhline(threshold, ls=\"--\", c=\"gray\")\n    xlim = ax_twin.get_xlim()\n    ylim = ax_twin.get_ylim()\n\n    ax_twin.fill_between(x=xlim, y1=ylim[-1], y2=threshold, color=\"red\", alpha=0.1)\n    ax_twin.fill_between(x=xlim, y1=ylim[0], y2=threshold, color=\"green\", alpha=0.1)\n\n    sns.despine(ax=ax, offset=20)\n    sns.despine(ax=ax_twin, offset=20, left=True, bottom=True, right=False, top=True)\n    ax_twin.yaxis.set_major_formatter(ticker.PercentFormatter(1))\n\n    fig.tight_layout()\n    plt.close(fig)\n\n    return fig\n\n\nview_cumulative_eigenspectrum(big_pca)\n\n\n\n\n\n\n\nEyeballing the “knee” of the spectrum\nWhen the number of latent dimensions is low, eigenspectra often have a sharp discontinuity (the “knee”), where a small number of dimensions have high-variance and the remainder have much have lower variance. The latent dimensionality is then taken to be the number of dimensions above this threshold determined by eye.\n\n\nPlot the apparent knee of the spectrum\ndef view_eigenspectrum_knee(pca: PCA, *, knee: int) -&gt; Figure:\n    fig = view_eigenspectrum(pca)\n    ax = fig.get_axes()[0]\n    ax.plot(\n        knee,\n        big_pca[\"eigenvalue\"].isel({\"component\": big_pca[\"rank\"] == knee}).values,\n        \"o\",\n        ms=30,\n        mec=\"r\",\n        mfc=\"none\",\n        mew=3,\n    )\n\n    ax.axvline(knee, ls=\"--\", c=\"gray\")\n    xlim = ax.get_xlim()\n    ylim = ax.get_ylim()\n\n    ax.fill_betweenx(y=ylim, x1=xlim[-1], x2=knee, color=\"red\", alpha=0.1)\n    ax.fill_betweenx(y=ylim, x1=xlim[0], x2=knee, color=\"green\", alpha=0.1)\n\n    return fig\n\n\nview_eigenspectrum_knee(big_pca, knee=3)\n\n\n\n\n\nSo far, all of the techniques discussed depend on a paramter that needs to be decided by the researcher. For instance, a cumulative threshold of 80% is an arbitrary choice, and if we decided to choose 90% instead, we would get a different number of dimensions. This is why sometimes a parameter-free estimate is preferred.\n\n\nComputing a summary statistic over the entire spectrum\nA metric such as effective dimensionality summarizes the spectrum using an entropy-like measure, taking into account variances along all the dimensions:\nd_\\text{eff}(\\lambda_1, \\dots \\lambda_n) = \\dfrac{\\left( \\sum_{i=1}^n \\lambda_i \\right)^2}{\\sum_{i=1}^n \\lambda_i^2}\n\n\nCompute the effective dimensionality from the spectrum\ndef compute_effective_dimensionality(eigenspectrum: np.ndarray) -&gt; float:\n    return (np.sum(eigenspectrum) ** 2) / (eigenspectrum**2).sum()\n\n\ndef view_effective_dimensionality_examples(pca: PCA) -&gt; Figure:\n    n_components = pca.sizes[\"component\"]\n\n    spectrum_1 = [float(pca[\"eigenvalue\"].mean().values)] * n_components\n\n    weights = 1 / (1 + np.arange(n_components, dtype=np.float32))\n    weights /= weights.sum()\n\n    spectrum_2 = list(weights * pca[\"eigenvalue\"].sum().values)\n    spectrum_3 = [float(pca[\"eigenvalue\"].sum().values)] + [0] * (n_components - 1)\n\n    data = pd.DataFrame(\n        {\n            \"eigenvalue\": spectrum_1 + spectrum_2 + spectrum_3,\n            \"rank\": list(np.tile(pca[\"rank\"].values, 3)),\n            \"example\": [0] * n_components + [1] * n_components + [2] * n_components,\n        }\n    )\n    # return data\n    g = sns.relplot(\n        kind=\"line\",\n        data=data,\n        col=\"example\",\n        x=\"rank\",\n        y=\"eigenvalue\",\n        marker=\"o\",\n        height=3,\n        aspect=1,\n        facet_kws={\n            \"sharey\": False,\n        },\n    )\n\n    for i_spectrum, ax in enumerate(g.axes.flat):\n        d = compute_effective_dimensionality(\n            data.loc[data[\"example\"] == i_spectrum, \"eigenvalue\"]\n        )\n        ax.set_title(\"$d_{eff}$\" + f\" = {d.round(2)}\")\n        ax.set_xticks([1, 10])\n\n    sns.despine(g.figure, offset=10)\n    g.figure.tight_layout(w_pad=2)\n    plt.close(g.figure)\n    return g.figure\n\n\nprint(\n    \"effective dimensionality =\"\n    f\" {compute_effective_dimensionality(big_pca['eigenvalue']).values.round(2)}\"\n)\n\nview_effective_dimensionality_examples(big_pca)\n\n\neffective dimensionality = 2.55"
  },
  {
    "objectID": "pages/introducing_pca.html#takeaways-and-further-thoughts",
    "href": "pages/introducing_pca.html#takeaways-and-further-thoughts",
    "title": "An introduction to PCA",
    "section": "Takeaways and further thoughts",
    "text": "Takeaways and further thoughts\n\nPCA is a useful tool for studying the neural code and estimating the latent dimensionality of representations.\nHowever, PCA focuses on how much variance is present along each dimension, and not the usefulness of the dimensions.\nHere we used a toy example with idealized data, where the high variance dimensions are meaninful and the low variance ones are noise.\nThis may not always be the case, we could instead have high variance noise dimensions, and low variance meaningful dimensions.\nLater in the tutorial, we will discuss more powerful techniques that can handle such problems. But before that, let’s analyze a real neuroscience dataset."
  },
  {
    "objectID": "pages/introducing_pca.html#additional-notes",
    "href": "pages/introducing_pca.html#additional-notes",
    "title": "An introduction to PCA",
    "section": "Additional notes",
    "text": "Additional notes\n\nPreprocessing the data\nBefore PCA, it’s often recommended to preprocess the data by Z-scoring each of the input features X – ensuring that they have zero mean and unit variance:\nZ = \\dfrac{X - \\mu}{\\sigma}\n\n\n\n\n\n\nWhen and why should we standardize the data?\n\n\n\n\n\nOften, PCA is applied to data where the features are fundamentally different from each other. For example, we might have a dataset where the features of interest are the prices of cars (in dollars) and their masses (in kilograms). Since these two features have different units, the variances of the features are not directly comparable – there’s no obvious way to numerically compare a variance of ($20,000)2 in price and a variance of (1,000 kg)2 in mass. Even if the features being compared are all the same, if they are in different units – say euros, dollars, and cents – the raw variances of the data matrix are meaningless.\nSince PCA implicitly assumes that the variances along each dimension are comparable, we can Z-score each of the features before applying PCA to ensure that they are on a common scale.\nNote, however, that this transformation reduces the information in the system – it is possible that the variances of the features are informative."
  },
  {
    "objectID": "pages/introducing_pca.html#further-reading",
    "href": "pages/introducing_pca.html#further-reading",
    "title": "An introduction to PCA",
    "section": "Further reading",
    "text": "Further reading\n\nThis StackOverflow answer is a great explanation for how singular value decomposition is related to principal component analysis.\nThis tutorial by Giudice (2020) on effective dimensionality provides a great overview of different notions of dimensionality and metrics to quantify it."
  }
]